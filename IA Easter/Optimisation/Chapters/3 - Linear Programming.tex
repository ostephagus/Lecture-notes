\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Formulating a Linear Program}
\begin{definition}{Linear program}
    A \underline{linear program} is an optimisation problem where the objective function and all constraints are linear in $\vec{x}$.
\end{definition}
The most general form is:
\begin{align*}
    \text{Minimise } &\vec{c}^T\vec{x} \\
    \text{Subject to } &\vec{a_i}^T \vec{x} \geq \vec{b_i},~i \in M_1 \\
    &\vec{a_i}^T \vec{x}\leq \vec{b_i},~i \in M_2 \\
    &\vec{a_i}^T \vec{x}= \vec{b_i},~i \in M_3 \\
    &x_j \geq 0,~j \in N_1 \\
    &x_j \leq 0,~j \in N_1 \\
\end{align*}
However, it can be shown that any such set of constraints can be written as: $A\vec{x} \leq \vec{b}$ for some matrix $A$ formed from the constraints above. This is the \underline{general form}.

The \underline{standard form} of a linear program has constraints $A\vec{x} = \vec{b}$, $\vec{x} \geq 0$. Any program in standard form can trivially be expressed in the general form, but also any program in the general form can be written in standard form by expressing each component as $x_j = x_j^+ - x_j^-$.

The general form is easier to prove results about, whereas the standard form is often easier to solve.
\section{Extreme Points}
\subsection{Maxima of Convex Functions}
Consider a problem:
\begin{align*}
    \text{Maximise } &f(\vec{x}) \\
    \text{Subject to } &x \in \Xset
\end{align*}
where $f$ and $\Xset$ are convex.
\begin{definition}{Extreme point}
    Let $\Xset$ be a convex set. A point $\vec{x} \in \Xset$ is an \underline{extreme point} if $\vec{x}$ cannot be written as:
    \begin{equation*}
        \vec{x} = \lambda \vec{y} + (1-\lambda) \vec{z},~\lambda \in (0, 1)
    \end{equation*}
    where $\vec{x}, \vec{y}$ and $\vec{z}$ are distinct.
\end{definition}
\begin{theorem}
    The maximum of a convex function constrained to a convex set is realised at an extreme point.
\end{theorem}
\begin{proof}
    Suppose $\vec{z}$ can be written as a convex combination of points $\vec{x}$ and $\vec{y}$. Then we can show that $f(\vec{z}) \leq \max\{f(\vec{x}), f(\vec{y})\}$.
\end{proof}
\subsection{Algebraic Description of Extreme Points}
In low-dimensional space, we can easily find the extreme points of a polygon by considering vertices. However, for a higher-dimensional space we need an algebraic description of extreme points.

Consider the problem:
\begin{align*}
    \text{Minimise }& \vec{c} \cdot \vec{x} \\
    \text{Subject to }& A\vec{x} = \vec{b} \\
    & \vec{x} \geq 0
\end{align*}
Then $A$ is an $m \times n$ matrix, it has $m$ rows and $n$ columns. Represent a row by $a_1^T, a_2^T, \cdots, a_m^T$ and a column by $A_1, A_2, \cdots, A_n$. We will make 3 assumptions to simplify the problem.
\begin{enumerate}
    \item The rows $a_1^T \cdots a_m^T$ are linearly independent, and so the rank of the matrix is $m$. If this is not the case, we can reduce the number of constraints because some must be redundant.
    \item Every set of $m$ columns is linearly independent. This assumption is not made without loss of generality.
\end{enumerate}

\begin{definition}{Basic solution}
    A point $\vec{x}$ that satisfies $A\vec{x} = \vec{b}$ and has at most $m$ non-zero indices is a \underline{basic solution} to the above problem.
\end{definition}
\begin{definition}{Basic feasible solution}
    A basic solution $\vec{x}$ that satisfies $\vec{x} \geq \vec{0}$ is a \underline{basic feasible solution}.
\end{definition}

We can now define our third assumption:
\begin{enumerate}
    \setcounter{enumi}{2}
    \item Every basic feasible solution is non-degenerate.
\end{enumerate}
Here a \textit{non-degenerate} solution is one that has exactly $m$ non-zero entries.

We can also detail a simple algorithm to find basic feasible solutions.
\begin{enumerate}
    \item Select some indices $B(1), \cdots, B(m) \in \{1, \cdots, m\}$.
    \item For each index $B(i)$, set the $i$th column of a matrix $B$ to be $A_{B(i)}$.
    \item For each index $B(i)$, set this index in a basic solution vector $\vec{x}$ to be:
        \begin{equation*}
            x_{B(i)} = \left[B^{-1} b\right]_i
        \end{equation*}
    \item Set the other entries in $\vec{x}$ to zero.
\end{enumerate}
This in fact finds all basic solutions, and we can check each to find basic \textit{feasible} solutions.

\begin{theorem}[Basic feasible solutions are extreme points of $\Xset(\vec{b})$]
    A point $\vec{x}$ is a basic feasible solution if and only if it is an extreme point of the feasible set:
    \begin{equation*}
        \Xset(\vec{b}) = \subsetselect{x}{Ax = b, x \geq 0}
    \end{equation*}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Assume $\vec{x}$ is a basic feasible solution}
        Assume that $\vec{x}$ can be written as $\vec{x} = \lambda \vec{y} + (1-\lambda) \vec{z}$ for some points $\vec{y}$ and $\vec{z}$ in $\Xset(\vec{b})$ and scalar $\lambda \in [0, 1]$.

        Note that, since $\vec{y}$ and $\vec{z}$ are feasible, their elements must be non-negative. Therefore, if an element of $\vec{x}$ is zero, the corresponding elements of $\vec{y}$ and $\vec{z}$ must be zero. Then the constraint that $\vec{y}$ and $\vec{z}$ are feasible then we can use the above process to fix $\vec{y}$ and $\vec{z}$, which means $\vec{x} = \vec{y} = \vec{z}$.

        Therefore $\vec{x}$ must be an extreme point, and cannot be written as a convex combination of distinct feasible solutions.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Assume $\vec{x}$ is not a basic feasible solution}
        This means $\vec{x}$ must have more than $m$ non-zero entries. Let $i_1, i_2, \cdots, i_r$ be the non-zero indices, where $r > m$. Since the column rank is $m$, the columns $\vec{A_{i_1}}, \cdots, \vec{A_{i_r}}$ are linearly depedent, so we can find a non-zero linear combination:
        \begin{equation*}
            w_{i_1} \vec{A_{i_1}} + \cdots + w_{i_r} \vec{A_{i_r}} = 0
        \end{equation*}
        Then set $\vec{w}$ to be the vector with component $w_{i_k}$ in column $i_k$, zero elsewhere. Observe that, for any $\epsilon > 0$,
        \begin{align*}
            A(\vec{x} + \epsilon \vec{w}) &= A\vec{x} + \epsilon A \vec{w} \\
            &= \vec{b} + \vec{0} = \vec{b}
        \end{align*}
        Then if $\epsilon$ is sufficiently small, $\vec{x} \pm \epsilon \vec{w} \geq 0$, and these are feasible.

        Thus we have found a convex combination:
        \begin{equation*}
            \vec{x} = \frac{1}{2} \left(\vec{x} + \epsilon \vec{w}\right) + \frac{1}{2} \left(\vec{x} - \epsilon \vec{w}\right)
        \end{equation*}
        and so $\vec{x}$ is not an extreme point.
    \end{proofdirection}
\end{proof}
\subsection{Simple Point-Testing Algorithm}
We can now construct an algorithm:
\begin{enumerate}
    \item Try all $\begin{pmatrix} n \\ m \end{pmatrix}$ basis matrices.
    \item List all basic solutions, filter out the basic feasible solutions.
    \item Choose the basic feasible solution that minimises $\vec{c}^T \vec{x}$.
\end{enumerate}
\begin{remarks}
    \item This is not a good algorithm, especially in higer-dimensional space.
    \item We could construct a better algorithm if we could identify a minimum point when we find it, like $\nabla f = \vec{0}$ for convex optimisation.
\end{remarks}
\section{Duality in Linear Programs}
\subsection{Strong Duality}
\begin{theorem}
    If a linear program is feasible and bounded then it satisfies strong duality.
    \label{thmLinearProgStrongDual}
\end{theorem}
\begin{remark}
    This is very similar to theorem~\ref{thmSupportPlane}, so the proof is similar with only slight modification.
\end{remark}
Now consider the standard form:
\begin{align*}
    \text{Minimise } &\vec{c}^T \vec{x} \\
    \text{Subject to } &A\vec{x} = \vec{b} \\
    &\vec{x} \geq \vec{0}
\end{align*}
Then we consider the lagrangian:
\begin{align*}
    L(\vec{x}, \vec{\lambda}) &= \vec{c}^T \vec{x} - \lambda^T (A\vec{x} - \vec{b}) \\
    &= (\vec{c}^T - \vec{\lambda^T} A)\vec{x} + \vec{\lambda}^T \vec{b}
\end{align*}
Then $\Lambda = \subsetselect{\vec{\lambda} \in \R^m}{\vec{c}^T - \vec{\lambda}^T A \geq 0}$. However, we can consider the dual problem:
\begin{align*}
    g(\vec{\lambda}) &= \min_{\vec{x} \geq 0, \vec{\lambda} \in \Lambda} L(\vec{x}, \vec{\lambda}) \\
    &= \vec{\lambda}^T \vec{b} = \vec{b} \cdot \vec{\lambda}
\end{align*}
Therefore we formulate the dual problem:
\begin{align*}
    \text{Maximise } &\vec{b}^T \vec{\lambda} \\
    \text{Subject to } &A^T \vec{\lambda} \leq \vec{c}
\end{align*}
which, we note, is also a linear program.

Therefore given the most general primal problem:
\begin{align*}
    \text{Minimise } &\vec{c}^T\vec{x} \\
    \text{Subject to } &\vec{a_i}^T \vec{x} \geq \vec{b_i},~i \in M_1 \\
    &\vec{a_i}^T \vec{x} \leq \vec{b_i},~i \in M_2 \\
    &\vec{a_i}^T \vec{x} = \vec{b_i},~i \in M_3 \\
    &x_j \geq 0,~j \in N_1 \\
    &x_j \leq 0,~j \in N_2 \\
    &x_j \text{ free},~j \in N_3
\end{align*}
we can formulate the dual problem:
\begin{align*}
    \text{Maximise } &\vec{b}^T\vec{\lambda} \\
    \text{Subject to } &\vec{\lambda_i}^T \geq 0,~i \in M_1 \\
    &\vec{\lambda_i}^T \leq 0,~i \in M_2 \\
    &\vec{\lambda_i}^T \text{ free},~i \in M_3 \\
    &\vec{\lambda}^T \vec{A_j} \leq c_j,~j \in N_1 \\
    &\vec{\lambda}^T \vec{A_j} \geq c_j,~j \in N_2 \\
    &\vec{\lambda}^T \vec{A_j} = c_j,~j \in N_3
\end{align*}
\subsection{Optimality Conditions}
\begin{theorem}
    Let $\vec{x}$ and $\vec{\lambda}$ be feasible points in the primal and dual problems. Then $\vec{x}$ and $\vec{\lambda}$ are optimal if and only if:
    \begin{align*}
        \lambda_i \left(\vec{a_i}^T \vec{x} - b_i\right) &= 0~\forall i \\
        \left(c_i - \vec{\lambda}^T \vec{A_j}\right)x_j &= 0~\forall j
    \end{align*}
    \label{thmLinearOptimality}
\end{theorem}
\begin{remark}
    This is saying that the conditions for optimality are:
    \begin{itemize}
        \item $\vec{x}$ is feasible for the primal problem.
        \item $\vec{\lambda}$ is feasible for the dual problem.
        \item Complementary slackness holds
    \end{itemize}
    and this is special to linear programs: complementary slackness is not usually sufficient, only necessary.
\end{remark}
\begin{proof}
    Define:
    \begin{align*}
        u_i &= \lambda_i \left(\vec{a_i}^T \vec{x} - b_i\right) \\
        v_j &= \left(c_j = \vec{\lambda}^T \vec{A_j}\right)x_j
    \end{align*}
    and note that these are non-negative. Their sums are:
    \begin{align*}
        \sum_i u_i &= \vec{\lambda}^T A \vec{x} - \vec{\lambda}^T \vec{b} \\
        \sum_j v_j &= \vec{c}^T \vec{x} - \vec{\lambda}^T A \vec{x} \\
    \end{align*}
    and summing these gives:
    \begin{equation*}
        \sum_i u_i + \sum_j v_j = \vec{c}^T \vec{x} - \vec{\lambda}^T \vec{b}
    \end{equation*}
    If complementary slackness holds, then $\vec{c}^T \vec{x} = \vec{\lambda}^T \vec{b}$ because all the $u_i$ and $v_j$ are zero. Therefore by weak duality we have that $\vec{x}$ and $\vec{\lambda}$ are optimal.

    Conversely, if $\vec{x}$ and $\vec{\lambda}$ are optimal, then the sum of all $u_i$ and $v_j$ is zero. Since these are non-negative variables, this means they are all zero and therefore complementary slackness holds.
\end{proof}
Consider a basic feasible solution with non-zero indices $B(1), \cdots, B(n)$. We want to find the corresponding $\vec{\lambda}$. Therefore, by theorem~\ref{thmLinearOptimality},
\begin{equation*}
    x_{B(i)} (\vec{c} - A^T \vec{\lambda})_{B(i)} = 0
\end{equation*}
and solving this for each $B(i)$ gives:
\begin{equation*}
    \vec{\lambda} = (B^T)^{-1} \vec{c_B} % TODO: What is this B matrix?
\end{equation*}
and we note that this $\vec{\lambda}$ is feasible:
\begin{align*}
    A^T (B^T)^{-1} \vec{c_B} &\leq \vec{c} \\
    \vec{c_B}^T B^{-1} A \leq \vec{c}^T
\end{align*}
so $\vec{\lambda}$ is feasible. 
\begin{definition}{Reduced cost vector}
    The \underline{reduced cost vector} $\bvec{c}$ is defined by:
    \begin{equation*}
        \bar{c}_j = c_j - \vec{c_B}^T B^{-1} \vec{A_j}
    \end{equation*}
\end{definition}
Then a basic feasible solution $\vec{x}$ is optimal if and only if $\bvec{c} \geq 0$.
\section{Simplex Algorithm}
The Simplex algorithm uses a Simplex Tableau:

\begin{tabular}{c|c|c|c|c}
    $-\vec{c_B}^T \vec{x_B}$ & $\bar{c_1}$ & $\bar{c_2}$ & $\cdots$ & $\bar{c_n}$ \\
    \hline
    $x_{B(1)}$ & $\vdots$ & $\vdots$ & & $\vdots$ \\
    $\vdots$ & $B^{-1} \vec{A_1}$ & $B^{-1} \vec{A_2}$ & $\cdots$ & $B^{-1} \vec{A_n}$ \\
    $x_{B(n)}$ & $\vdots$ & $\vdots$ & & $\vdots$
\end{tabular}

The steps of the Simplex algorithm are as follows:
\begin{enumerate}
    \item Convert the general form problem to a standard form problem using slack variables:
    \begin{enumerate}
        \item Add a positive slack variable for $\leq$, and add as a component of $\vec{x}$
        \item Add a negative slack variable for $\geq$, add it as a component of $\vec{x}$.
    \end{enumerate}
    \item Convert the problem into the vectors/matrices $A$, $\vec{b}$, $B$, $\vec{c}$:
    \begin{enumerate}
        \item $A$ is the matrix of coefficients of variables in the constraints
        \item $\vec{b}$ is the vector that $A\vec{x}$ must be equal to (in the constraints)
        \item $\vec{c}$ is the vector of coefficients of the variables in the objective function
    \end{enumerate}
    \item Start the simplex method with a basic feasible solution setting all initial variables to $0$ and all slack variables equal to the $b_i$ in the corresponding constraint. This only works for $\vec{b} \geq 0$. Then $B$ is the matrix from the last columns, corresponding only to slack variables. For the first step, it will be $I$.
    \item Fill in the Simplex Tableau \label{splxStepRptStart}
    \item Identify the direction to move in:
    \begin{enumerate}
        \item If all entries in the top row are non-negative, $\bvec{c} \geq 0$ and we are done.
        \item If not, select the column corresponding to the smallest negative entry in the top row. If there are more than one, pick any. Let this column be the $j$th
        \item For each value in the $j$th column, divide the element in the leftmost column and same row by it. Choose the row corresponding to the minimal ratio. Let this row be the $i$th.
    \end{enumerate}
    \item Perform row operations:
    \begin{enumerate}
        \item We need the $j$th column to be the identity column.
        \item Therefore, scale the $i$th row and add it to the other rows such that the entry $(i, j)$ is $1$ and all other entries in column $i$ are zero.
    \end{enumerate}
    \item Go back to step~\ref{splxStepRptStart} and continue the algorithm with this new tableau.
\end{enumerate}
\end{document}