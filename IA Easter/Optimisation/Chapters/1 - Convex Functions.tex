\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Introduction to Minimisation} % TODO: Make things vectors.
Consider a generic problem:

Minimise $f(\vec{x})$ such that $\vec{h}(\vec{x}) = \vec{b}$, $\vec{x} \in \Xset$.

We define $f$ as the \underline{objective function} $f : \R^n \mapsto \R$. The constraint $\vec{h}(\vec{x}) = \vec{b}$ is a \underline{functional constraint}. The constraint $\vec{x} \in \Xset$ is a \underline{regional}\\\underline{constraint}.

The set of values that satisfy the constraints on $\vec{x}$ is called the \underline{feasible}\\\underline{set}:
\begin{equation}
    \Xset(\vec{b}) = \subsetselect{\vec{x} \in \R^n}{\vec{h}(\vec{x}) = \vec{b}, \vec{x} \in \Xset}
    \label{eqnFeasibleSet}
\end{equation}
\begin{definition}{Optimal value}
    If $\vec{x}* \in \Xset(b)$ and $\vec{x}*$ minimises the objective function $f$, then $\vec{x}*$ is the \underline{optimal value}
\end{definition}
\section{Defining Convexity}
\subsection{Definitions}
\begin{definition}{Convex set}
    A set $S \subseteq \R^n$ is \underline{convex} if for all $\vec{x}, \vec{y} \in S$ and every $\lambda \in [0, 1]$,
    \begin{equation*}
        \lambda \vec{x} + (1 - \lambda) \vec{y} \in S
    \end{equation*}
\end{definition}
\begin{remark}
    An intuition for this, in $\R^n$, is that a convex set contains all lines between two points within it.
\end{remark}
\begin{definition}{Convex function}
    A function $f : S \mapsto \R$ is a \underline{convex function} if $S$ is convex, and $\forall \vec{x}, \vec{y} \in S$ and $\lambda \in [0, 1]$, we have:
    \begin{equation*}
        f(\lambda \vec{x} + (1-\lambda)\vec{y}) \leq \lambda f(\vec{x}) + (1 - \lambda) f(\vec{y})
    \end{equation*}
\end{definition}
\begin{remarks}
    \item A convex function is a function where any chord between two points $f(\vec{x})$ and $f(\vec{y})$ lies above the graph of the function.
    \item A \underline{concave} function is one that obeys the opposite inequality. If $f$ is convex function, $-f$ is concave.
    \item Convex functions are an ideal first candidate for minimisation, since (as we will see) any local minimum is a global minimum.
\end{remarks}
\subsection{Convexity and Derivatives}
\begin{theorem}[First-order condition for convexity]
    A differentiable function $f : \R^n \mapsto \R$ is convex if and only if for all $\vec{x}, \vec{y} \in \R^n$:
    \begin{equation}
        f(\vec{y}) \geq f(\vec{x}) + \left[\nabla f(\vec{x})\right]^T(\vec{y} - \vec{x})
        \label{eqnFirstOrderCondition}
    \end{equation}
    \label{thmFirstOrderCondition}
\end{theorem}
\begin{remark}
    If $\nabla f(\vec{x}) = 0$ for some $\vec{x}$, $f(\vec{y}) \geq f(\vec{x})$ for all $\vec{y} \in \R^n$, and $\vec{x}$ must be the optimal point $\vec{x}*$.
\end{remark}
\begin{proof}
    We consider the restricted function:
    \begin{equation*}
        f((1-t)\vec{x} + t\vec{y}), t \in [0, 1]
    \end{equation*}
    that is, the function $f$ on the line joining $\vec{x}$ and $\vec{y}$.
    \begin{proofdirection}{$\Rightarrow$}{Assume that $f$ is convex.}
        We know that $f((1-t)\vec{x} + t\vec{y}) \leq (1-t)f(\vec{x}) + tf(\vec{y})$. This means:
        \begin{equation*}
            f(\vec{y}) \geq f(\vec{x}) + \frac{f(\vec{x} + t(\vec{y}-\vec{x})) - f(\vec{x})}{t}
        \end{equation*}
        Then we can consider the limit as $t \to 0$. Only the right-hand side depends on $t$, and is in fact the directional derivative of $f$:
        \begin{equation*}
            f(\vec{y}) \geq f(\vec{x}) + (\vec{y} - \vec{x})^T \nabla f(\vec{x})
        \end{equation*}
        as required.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Assume $f$ satisfies equation~\ref{eqnFirstOrderCondition}.}
        Let $\vec{x_t} = (1 - t)\vec{x} + t\vec{y}$. Then we have:
        \begin{align}
            f(\vec{y}) &\geq f(\vec{x_t}) + \left[\nabla f(\vec{x_t})\right]^T (\vec{y} - \vec{x_t}) \label{eqnFirstOrder1} \\
            f(\vec{x}) &\geq f(\vec{x_t}) + \left[\nabla f(\vec{x_t})\right]^T (\vec{x} - \vec{x_t}) \label{eqnFirstOrder2}
        \end{align}
        Then consider $t$ times equation~\ref{eqnFirstOrder1} and $(1-t)$ times equation~\ref{eqnFirstOrder2}:
        \begin{align*}
            tf(\vec{y}) + (1-t)f(\vec{x}) &\geq f(\vec{x_t}) + \left[\nabla f(\vec{x_t})\right]^T (\vec{y} - \vec{x_t}) \times t \\
            &+ \left[\nabla f(\vec{x_t})\right]^T (\vec{x} - \vec{x_t}) \times (1-t) \\
            &= f(\vec{x_t}) + \left[\nabla f(\vec{x_t})\right]^T t(1-t)(\vec{y} - \vec{x} + \vec{x} - \vec{y}) \\
            &= f(\vec{x_t})
        \end{align*}
        Then expanding out $\vec{x_t}$ gives the required result.
    \end{proofdirection}
\end{proof}
We also define the Hessian matrix:
\begin{equation}
    \left[\nabla^2 f(x)\right]_{ij} = \frac{\partial^2 f(x)}{\partial x_i \partial x_j}
    \label{eqnHessian}
\end{equation}
Note that it is symmetric.
\begin{definition}{Positive semi-definiteness}
    A matrix $A$ is \underline{positive semi-definite} if, for any $x \in \R^n$,
    \begin{equation}
        x^T A x \geq 0
        \label{eqnPositiveSemiDefinite}
    \end{equation}
    or, equivalently, it has all eigenvalues non-negative. If so, we write:
    \begin{equation*}
        A \succeq 0 
    \end{equation*}
\end{definition}
\begin{theorem}[Second-order condition for convexity]
    A twice differentiable function $f : \R^n \mapsto \R$ is convex if and only if, at every point $\vec{x} \in \R^n$,
    \begin{equation}
        \nabla^2 f(\vec{x}) \succeq 0
        \label{eqnSecondOrderCondition}
    \end{equation}
    \label{thmSecondOrderCondition}
\end{theorem}
\begin{proof}[of the if relation]
    Recall the multivariate Taylor series of the function $f$ on the line joining some points $x$ and $y$:
    \begin{equation*}
        f(\vec{y}) = f(\vec{x}) + [\nabla f(\vec{x})]^T (\vec{y} - \vec{x}) + \frac{(\vec{y} - \vec{x})^T \nabla^2 f(z) (\vec{y}-\vec{x})}{2}
    \end{equation*}
    for some point $\vec{z}$ on the line between $\vec{x}$ and $\vec{y}$. But now we have that the second-order term must be non-negative, which gives the first-order condition. This implies convexity by theorem~\ref{thmFirstOrderCondition}.
\end{proof}
\section{Gradient Descent}
Let $f$ be a convex function. Start at some point $\vec{x}$. We then consider a perturbation $\vec{x} - \nabla f(\vec{x} \epsilon)$ for some small $\epsilon > 0$. This new point has value approximately:
\begin{equation*}
    f(\vec{x}) \approx ||\nabla f(\vec{x})||^2 \epsilon
\end{equation*}
Note that this is a decrease in the value of $\vec{f}$ as long as the approximation is valid ($\epsilon$ must be small). Repeating this will decrease the value of $f$ until $\nabla f = 0$, or is close enough to $0$ to be satisfactory, and given that $f$ is convex this will be equal to (or close enough to) the minimum of $f$.

However, we have a few problems to deal with when using this algorithm:
\begin{itemize}
    \item Step size: depending on how \textit{curved} the function is, the step size may need to be modified. For very curved functions, a small step size is required. For a very flat function, a large step size is required.
    \item Descent direction: if the problem is constrained, such as the value $\vec{x}$ must lie in some subset of $\R^n$, the direction of the negative gradient may not be appropriate if it takes the point outside the subset. Instead, we may have to redefine the direction of the perturbation.
    \item Time limit: we may impose some limit on the number of steps the algorithm is allowed to take before we stop it
    \item Stop condition: the condition of small enough $\nabla f$ may give vastly different results for different functions. The results are similar to changing the step size.
\end{itemize}
\subsection{Types of Convexity}
\begin{definition}{$\beta$-smoothness}
    A twice-differentiable function $f : \R^n \mapsto \R$ is \underline{$\beta$-smooth} for $\beta > 0$ if $\nabla^2 f(\vec{x}) \preceq \beta I$ for all $\vec{x} \in \R^n$.
\end{definition}
\begin{definition}{$\alpha$-strong convexity}
    A twice-differentiable function $\vec{f} : \R^n \mapsto \R$ is \underline{$\alpha$-strongly convex} for $\alpha > 0$ if $\nabla^2 f(\vec{x}) \succeq \alpha I$ for all $\vec{x} \in \R^n$.
\end{definition}
We also define $A \preceq B$ if $B - A \succeq 0$, $B - A$ is positive semi-definite. Therefore we have that a function is $\alpha$-strongly convex and $\beta$-smooth if:
\begin{equation*}
    \alpha I \preceq \nabla^2 f(\vec{x}) \preceq \beta I
\end{equation*}
We can then use these notions to get better choices for the parameters in the gradient descent algorithm.
\begin{theorem}
    If $f$ is $\beta$-smooth then:
    \begin{equation}
        f(\vec{y}) \leq f(\vec{x}) + \nabla f(\vec{x})^T (\vec{y} - \vec{x}) + \frac{\beta}{2} ||\vec{x} - \vec{y}||^2
        \label{eqnBetaSmoothBound}
    \end{equation}
    and if $f$ is $\alpha$-strongly convex then:
    \begin{equation}
        f(\vec{y}) \geq f(\vec{x}) + \nabla f(\vec{x})^T (\vec{y} - \vec{x}) + \frac{\alpha}{2} ||\vec{x} - \vec{y}||^2
        \label{eqnAlphaStrongBound}
    \end{equation}
    \label{thmConvexityBounds}
\end{theorem}
\begin{proof}
    Considering the Taylor series of $f$:
    \begin{align*}
        f(\vec{y}) &= f(\vec{x}) + \nabla f(\vec{x})^T (\vec{y} - \vec{x}) \\
        &+ \frac{1}{2} (\vec{y} - \vec{x})^T \nabla^2 f(\vec{z}) (\vec{y} - \vec{x})
    \end{align*}
    Then we recall that if $A \preceq B$ then:
    \begin{equation*}
        \vec{x}^T A \vec{x} \leq \vec{x}^T B \vec{x}~~\forall \vec{x} \in \R^n
    \end{equation*}
    Then using the various bounds for the Hessian and using the above property, we get the required results.
\end{proof}
\begin{corollary}
    For a function $f$ with $\alpha$-strong convexity and $\beta$ smoothness:
    \begin{equation}
        f\left(\vec{x} - \frac{1}{\beta} \nabla f(\vec{x})\right) \leq f(\vec{x}) - \frac{1}{2\beta} ||\nabla f(\vec{x})||^2
        \label{eqnStepSizeBound}
    \end{equation}
    \label{corStepSizeBound}
\end{corollary}
\begin{proof}
    Substitute $\vec{y} = \vec{x} - \frac{1}{\beta} \nabla f(\vec{x})$ into theorem~\ref{thmConvexityBounds}
\end{proof}
\begin{remark}
    This gives us a step size to use in the gradient descent algorithm
\end{remark}
\begin{corollary}
    Let $p*$ be the minimum value of $f(\vec{x})$. Then for any $\vec{x} \in \R^n$:
    \begin{equation}
        p* \geq f(\vec{x}) - \frac{||\nabla f(\vec{x})||^2}{2\alpha}
        \label{eqnStopCriterionBound}
    \end{equation}
    \label{corStopCriterionBound}
\end{corollary}
\begin{remark}
    If $||\nabla f(\vec{x})|| \leq \sqrt{2\alpha \epsilon}$, then we have that $f(\vec{x})$ is within $\epsilon$ of $p*$. This gives us a stopping condition for the gradient descent algorithm.
\end{remark}
\begin{proof}
    By $\alpha$-strong convexity:
    \begin{equation*}
        p* \geq \min_{\vec{y}} f(\vec{x}) + \nabla f(\vec{x})^T (\vec{y} - \vec{x}) + \frac{\alpha}{2} ||\vec{x} - \vec{y}||^2
    \end{equation*}
    Taking the gradient with respect to $\vec{y}$:
    \begin{equation*}
        \nabla f(\vec{x}) + \alpha (\vec{y} - \vec{x}) = 0
    \end{equation*}
    which gives:
    \begin{equation*}
        \vec{y} = \vec{x} - \frac{1}{\alpha} \nabla f(\vec{x})
    \end{equation*}
    Substituting into the RHS gives the required result.
\end{proof}
\subsection{Convergence of Gradient Descent}
\begin{theorem}
    Let $\alpha, \beta > 0$ be real numbers. Let a function $f : \R^n \mapsto \R$ be $\alpha$-strongly convex and $\beta$-smooth. Then a gradient descent algorithm with step size $\frac{1}{\beta}$ satisfies, at iteration $T$:
    \begin{align}
        f(\vec{x_T}) - f(\vec{x}^*) &\leq \left(1 - \frac{\alpha}{\beta}\right)^T (f(\vec{x_0}) - f(\vec{x}^*)) \label{eqnGradDescConvergence} \\
        &\leq e^{-\frac{T\alpha}{\beta}} \left(f(\vec{x_0}) - f(\vec{x}^*)\right) \nonumber
    \end{align}
    where here $\vec{x}^*$ is the optimal point $\vec{x}$, and $\vec{x_0}$ is the point where the gradient descent starts.
    \label{thmGradDescConvergence}
\end{theorem}
\begin{remark}
    This gives a number of step sizes that is sufficient to be within a given error of the minimum value.
\end{remark}
\begin{proof}
    We have that $\vec{x_{t + 1}} = \vec{x_t} - \frac{1}{\beta} \nabla f(\vec{x_t})$. By corollary~\ref{corStepSizeBound}, we have that:
    \begin{align*}
        f(\vec{x_{t+1}}) &\leq f(\vec{x_t}) - \frac{1}{2\beta} ||\nabla f(\vec{x_t})||^2 \\
        f(\vec{x_{t+1}}) - f(\vec{x}^*) &\leq f(\vec{x_t}) - f(\vec{x}^*) - \frac{1}{2\beta} ||\nabla f(\vec{x_t})||^2
    \end{align*}
    Then by corollary~\ref{corStopCriterionBound}:
    \begin{align*}
        f(\vec{x_{t+1}}) - f(\vec{x}^*) &\leq f(\vec{x_t}) - f(\vec{x}^*) = \frac{\alpha}{\beta} (f(\vec{x_t}-f(\vec{x}^*))) \\
        &= \left(1-\frac{\alpha}{\beta}\right) (f(\vec{x_t}) - f(\vec{x}^*))
    \end{align*}
    Then iterating this back to 0 gives the result. We can also use well-known bounds on $(1 - \alpha / \beta)^T$ to get the other result.
\end{proof}
\section{Second-Order Methods}
We consider the Taylor series approximation to second order:
\begin{equation*}
    f(\vec{y}) \approx f(\vec{x}) + \nabla f(\vec{x})^T (\vec{y} - \vec{x}) + \frac{1}{2} (\vec{y} - \vec{x})^T \nabla^2 f(\vec{x}) (\vec{y} - \vec{x})
\end{equation*}
Then this is a quadratic function. Note that it is convex in every direction because the hessian is positive-definite, so the square term is always positive. It is much easier to minimise a quadratic function - we consider only the derivative equal to 0.
\begin{align*}
    \nabla f(\vec{y}) &\approx \nabla f(\vec{x}) + \nabla^2 f(\vec{x}) (\vec{y} - \vec{x}) = 0 \\
    -\nabla f(\vec{x}) &= \nabla^2 f(\vec{x}) (\vec{y} - \vec{x}) \\
    y - x &= -\left(\nabla^2 f(\vec{x})\right)^{-1} \nabla f(\vec{x}) \\
    y &= \vec{x} - \left(\nabla^2 f(\vec{x})\right)^{-1} \nabla f(\vec{x})
\end{align*}
We can use this idea to create an algorithm.
\subsection{Newton's Method}
We define Newton's Method by the above idea:
\begin{equation*}
    \vec{x_{t+1}} = \vec{x_t} - \left(\nabla^2 f(\vec{x})\right)^{-1} \nabla f(\vec{x})
\end{equation*}
from the above formula. We use the 1-dimensional case to illustrate. Then:
\begin{equation*}
    x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}
\end{equation*}
Then we want to find the point $x$ where $f'(x) = 0$. Note that the formula above is equivalent to applying the Newton-Raphson method to $g'(x)$ to find its root.
\begin{definition}{Matrix norm}
    For a matrix $A \in \R^{n \times n}$, the matrix norm $||A||$ is the smallest $a > 0$ such that $||A\vec{z}|| \leq a ||z||$ for all $\vec{z} \in \R^n$. For positive semi-definite matrices, this is the largest eigenvalue.
\end{definition}
\begin{remark}
    We can understand this notion of norm or distance by how much the linear map represented by this matrix stretches the underlying space.
\end{remark}
\begin{theorem}
    Suppose $f$ is a twice-differentiable function that is $\alpha$-strongly convex. Suppose further that its hessian $\nabla^2 f(\vec{x})$ is $l$-Lipschitz, that is, for all $\vec{x}$ and $\vec{y}$,
    \begin{equation*}
        ||\nabla^2 f(\vec{x}) - \nabla^2 f(\vec{y})|| \leq l||\vec{x} - \vec{y}||
    \end{equation*}
    Then Newton's method satisfies:
    \begin{equation*}
        f(\vec{x_t}) - f(\vec{x}^*) \leq \frac{2\alpha^3}{l^2} \left(\frac{l}{2\alpha^2} ||\nabla f(\vec{x_0})||\right)^{2^{k+1}}
    \end{equation*}
\end{theorem}
\begin{remarks}
    \item The $l$-Lipschitz condition replaces the $\beta$-smoothness condition in theorem~\ref{thmGradDescConvergence}.
    \item If the term in the large brackets is less than 1, equivalent to\\$||\nabla f(\vec{x_0})|| < \frac{2\alpha^2}{l}$. This means we need a sufficiently good starting point. There are situations where a poorly-chosen starting point can mean that Newton's method does not converge. Instead, running a few steps of gradient descent can get to this starting point.
    \item The new exponent shows that Newton's method can be understood to be exponentially more powerful than gradient descent.
    \item The proof of this theorem is non-examinable.
\end{remarks}
\subsection{Barrier Method}
We have so far considered an unconstrained problem, where we consider the minimum over all $\R^n$. If we instead want to find the minimum in a particular subset of $\R^n$, the above methods may fail in the case that the actual minimum of $f$ is outside of the subset. For example, consider:

Minimise $f(\vec{x})$ such that:
\begin{equation*}
    \vec{a_i}^T \vec{x} - b_i \leq 0 \text{ for } i \in \{1, \cdots, m\}
\end{equation*}
We then consider a new function:
\begin{equation*}
    \Phi(\vec{x}) = -\sum_{i=1}^m \log\left(-\left(\vec{a_i}^T \vec{x} - \vec{b_i}\right)\right)
\end{equation*}
This function is then small inside the feasible region, and blows up to infinity outside it.

We consider minimising the function $tf(\vec{x}) + \Phi(\vec{x})$. We could start with setting $t$ to be small, and finding a minimum inside the boundary using Newton's method or gradient descent. We then increase $t$ slowly and use Newton's Method each time with the last minimum as start point to find the new minimum, noting that Newton's method will converge very quickly since we are close to the real minimum. More explicitly, this is:
\begin{enumerate}
    \item Find a strictly feasible $\vec{x}$, set $t > 0$ and $s > 1$.
    \item Repeat the following:
    \begin{enumerate}
        \item Compute $\vec{x}^*(t)$ by minimising $tf(\vec{x}) + \Phi(\vec{x})$, using Newton's method with $\vec{x}$ as the starting point.
        \item Update $\vec{x} = \vec{x}^*(t)$, and set $t = st$.
        \item Stop when $t$ is large enough.
    \end{enumerate}
\end{enumerate}
\end{document}