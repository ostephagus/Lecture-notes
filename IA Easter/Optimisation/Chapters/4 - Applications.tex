\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Game Theory}
\subsection{Mathematical Description of Games}
Consider a \textit{game} with 2 players where each player can choose from a number of actions. Let player 1's set of allowed actions be $\{1, 2, \cdots, m\}$ and player 2's set of allowed actions $\{1, 2, \cdots, n\}$.

\begin{definition}{Payoff matrix}
    For each player, that player's \underline{payoff matrix} is an $m \times n$ matrix with entries $A_{ij}$, where $A_{ij}$ is the score attained by player $1$ given that they play action $i$ and player 2 plays action $j$.
\end{definition}
\begin{example}{Rock-paper-scissors}
    Each player (1 and 2) can play one of rock, paper, scissors. Then if rock is assigned $1$, paper is assigned $2$ and scissors is assigned $3$, the payoff matrix for player 1 is:
    \begin{equation*}
        A =
        \begin{pmatrix}
            0 & -1 & 1 \\
            1 & 0 & -1 \\
            -1 & 1 & 0
        \end{pmatrix}
    \end{equation*}
\end{example}
\begin{definition}{Zero-sum game}
    A game is called \underline{zero-sum} if after each round of the game the total payoff is zero.
\end{definition}
In this case, the payoff matrix for player 2 is $-A$.

From player 1's perspective, they assume player 2 will play the action that will gain them the most points. This is $\min_{j \in \{1, \cdots, n\}} a_{ij}$.
Then the \underline{conservative approach} is for player 1 to choose the action that minimises the damage given player 2's optimal choice. This is:
\begin{equation*}
    \max_{i \in \{1, \cdots, m\}} \min_{j \in \{1, \cdots, n\}} a_{ij}
\end{equation*}
\begin{example}
    Consider a matrix $A$:
    \begin{equation*}
        A =
        \begin{pmatrix}
            1 & 2 \\
            3 & 4
        \end{pmatrix}
    \end{equation*}
    then in this case, player 1 thinks they should play $i = 1$, and player 2 thinks they should play $j = 1$, so both converge on the cell $A_{21} = 3$.
\end{example}
\begin{example}
    Now consider a payoff matrix:
    \begin{equation*}
        A =
        \begin{pmatrix}
            4 & 2 \\
            1 & 3
        \end{pmatrix} 
    \end{equation*}
    Then player 1 will choose column $i = 2$, and will then expect player 2 to choose row $j = 1$. However, player 2 will choose row $j = 2$ and expect player 1 to choose column $i = 2$. Unlike the last case, the players do not converge on a single value. We note that in a game where the players take turns, this is fine. In a case such as rock, paper, scissors, where the players choose at the same time, this introduces a problem.
\end{example}
To resolve this, we can introduce an element of probability to the actions each player chooses.
\begin{definition}{Strategy}
    A \underline{strategy} is the probability distribution of actions that a player picks.
\end{definition}
\begin{definition}{Pure strategy}
    A \underline{pure strategy} is a strategy where one action has probability 1.
\end{definition}
\begin{definition}{Mixed strategy}
    A \underline{mixed strategy} is a strategy where no one action has probability 1.
\end{definition}
Let player 1 pick action $i$ with probability $p_i$. Let player 2 pick action $j$ with probability $q_j$.

Then player 1 must choose their strategy based on:
\begin{equation*}
    \max_{\vec{p} \in P_m} \left(\min_{j \in \{1, \cdots, n\}} \sum_{i = 1}^m p_i a_{ij}\right)
\end{equation*}
where $P_m$ is the set of probability distributions on $\{1, \cdots, m\}$.
\subsection{Linear Programming for Strategies}
We can re-write player 1's problem using the vector $\vec{e}$ which contains $m$ entries all $1$.

\begin{align*}
    \text{Maximise } &v \\
    \text{Subject to } &A^T \vec{p} \geq v\vec{e} \\
    &\vec{e}^T \vec{p} = 1 \\
    \vec{p} \geq 0
\end{align*}
Where here $\vec{p}$ is the vector of probabilities, and $v$ is the maximum value for player 1.

We can also re-formulate player 2's problem to find a probability distribution $\vec{q}$, using $\vec{e}$ to be $n$-dimensional:
\begin{align*}
    \text{Minimise } &w \\
    \text{Subject to } &A\vec{q} \leq w\vec{e} \\
    &\vec{e}^T \vec{q} = 1 \\
    &\vec{q} \geq 0
\end{align*}
And we can prove that these two problems are duals.

Consider a Lagrangian for player 2's problem:
\begin{equation*}
    L(w, \vec{q}, \vec{s}, \vec{\lambda_1}, \lambda_2) = w + \vec{\lambda_1}^T \left(A\vec{q} + \vec{s} - w\vec{e}\right) - \lambda_2\left(\vec{e}^T\vec{q} - 1\right)
\end{equation*}
Then $\Lambda$ is the set of $\vec{\lambda}$ such that the minimum of the Lagrangian is bounded, and we formulate the dual problem:
\begin{align*}
    \text{Maximise } &\lambda_2 \\
    \text{Subject to } &\vec{\lambda_1}^T \vec{e} = 1 \\
    &\vec{\lambda_1} \geq 0 \\
    &A^T \vec{\lambda_1} \geq \lambda_2\vec{e}
\end{align*}
Then this is also player 1's problem.

\begin{definition}{Nash equilibrium}
    The pair $(\vec{p}, \vec{q})$ that solves the above primal and dual problems is called the \underline{Nash equilibrium}.
\end{definition}

Note that, by \ref{thmLinearProgStrongDual}, the Nash Equilibrium exists for a well-defined problem (feasible and bounded).

\begin{theorem}
    A strategy $\vec{p}$ is optimal for player 1 if and only if there exists $\vec{q}$ and $v$ such that:
    \begin{enumerate}
        \item The primal is feasible: $A^T\vec{p} \geq v\vec{e}, \vec{e}^T \vec{p} = 1, \vec{p} \geq 0$,
        \item The dual is feasible: $A\vec{q} \leq v\vec{e}, \vec{e}^T\vec{q}, \vec{q} \geq 0$,
        \item Complementary slackness holds: $v = \vec{p}^T A \vec{q}$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    If $(\vec{p}, v)$ and $(\vec{q}, w)$ are optimal, then by complementary slackness:
    \begin{align*}
        (A\vec{q} - w\vec{e})^T \vec{p} &= 0 \\
        (\vec{q}^T A^T\vec{p} - v\vec{e}) = 0
    \end{align*}
    and these are equivalent to $v = w = \vec{p}^T A \vec{q}$.
\end{proof}
\subsection{Finding optimal strategies}
We have some different steps for finding strategies.
\begin{enumerate}
    \item Look for saddle points, in which case a pure strategy is optimal.
    \item Look for \textit{dominating actions}, where each payoff is greater for one action than another no matter the action of the other player. In the payoff matrix, this corresponds to every entry of a row being greater than that of another row. In this case, the action that is dominated can be removed from the space of actions.
    \item After trying the above, formulate and solve the resulting linear program. 
\end{enumerate}
We consider an example for step 2:
\begin{equation*}
    \begin{pmatrix}
        2 & 3 & 4 \\
        3 & 1 & \frac{1}{2} \\
        1 & 3 & 2
    \end{pmatrix}
\end{equation*}
Then we see that for player 2, playing row 1 is always better than playing row 3. Therefore, we can remove the row and reduce the size of the matrix.
%TODO: Continue reviewing lectures.
\section{Transport Problems}
\subsection{Network Flow Problems}
\begin{definition}{Directed graph}
    A \underline{directed graph} $G = (V, E)$ consists of a set of vertices $V$ and a set of edges $E \subseteq V \times V$.
\end{definition}
Consider a graph with $n$ vertices. Let the vector $\vec{b} \in \R^n$ represent the amount of a good produced or consumed at a vertex. If this quantity is positive, the vertex is a \underline{source} of the good. Instead, if the quantity is negative, the vertex is a \underline{sink}. If we assume that all the good that is produced is consumed,
\begin{equation*}
    \sum_{i = 1}^n b_i = 0
\end{equation*}
Consider also an $n \times n$ matrix $C$ such that $C_{ij}$ is the \underline{cost per unit flow} to transport the goods along the edge $(i, j)$.

We also have $n \times n$ matrices $\underbar{M}, \overline{M}$ such that the flow $x_{ij}$ along edge $(i, j)$ is bounded by:
\begin{equation*}
    \underbar{M}_{ij} \leq x_{ij} \leq \overline{M}_{ij}
\end{equation*}
\begin{definition}{Network flow problem}
    The minimum cost flow is a linear program given by:
    \begin{align*}
        \text{Minimise } &\sum_{(i, j) \in E} x_{ij} C_{ij} \\
        \text{Subject to } &b_i + \sum_{j : (j,i) \in E} x_{ji} = \sum_{j : (i, j) \in E} x_{ij} \\
        & \underbar{M}_{ij} \leq x_{ij} \leq \overline{M}_{ij}
    \end{align*}
\end{definition}
\subsection{Defining a Transport Problem}
A transport problem is a special case of a network flow problem where there are no intermediate vertices.

Consider $n$ suppliers and $m$ consumers. Let supplier $i$ produce $s_i$ goods, and consumer $j$ demand $d_i$ goods. Require that these demands are met:
\begin{equation*}
    \sum_{i = 1}^n s_i = \sum_{j = 1}^m d_j
\end{equation*}
Let the cost per unit flow be $c_{ij}$.

\begin{definition}{Tranport problem}
    The \underline{transport problem} is:
    \begin{align*}
        \text{Minimise } &\sum_{(i, j)} x_{ij} \\
        \text{Subject to } &\sum_{j = 1}^m  x_{ij} = s_i~\forall i \in \{1, \cdots, n\} \\
        &\sum_{i = 1}^n x_{ij} = d_j~\forall j \in \{1, \cdots, m\}
    \end{align*}
\end{definition}
\begin{theorem}{Network flow equivalent to transport problem}
    Every minimum cost network flow problem with finite capacities ($\overline{M}$ is finite) or non-negative costs can be re-cast as an equivalent transport problem
    \label{thmFlowTransportEquiv}
\end{theorem}
\begin{remark}
    By this theorem, any algorithm to solve transport problems solves any network flow problem.
\end{remark}
\begin{proof}
    Given $\vec{b}, C, \underbar{M}, \overline{M}$, observe that $\underbar{M}$ can be set to be $0$ by considering $\tilde{x}_{ij} = x_{ij} - \underbar{M}_{ij}$.

    Now the conditions for $\tilde{x}$ are:
    \begin{align*}
        &0 \leq \tilde{x}_{ij} \leq \overline{M}_{ij} - \underbar{M}_{ij} \\
        &\tilde{b}_i + \sum_{i, j} \tilde{x}_{ij} = \sum_{i, j} \tilde{x}_{ij} \\
        &\text{Where } \tilde{b}_i = \sum_{(i, j) \in E} \underbar{M}_{ij} = \sum_{(i, j) \in E} \overline{M}_{ij} + b_i
    \end{align*}
    Note also that we can replace any infinite-capacity edge with one with capacity:
    \begin{equation*}
        \overline{M}_{ij} = \sum_i |b_i|
    \end{equation*}
    which can never be exceeded.

    For every $(i, j) \in E$, construct a supplier $(i, j)$, and for every $i \in V$, construct a consumer. Then we have created a bipartite graph with $|E|$ suppliers and $|V|$ consumers. Now, let the supply from supplier $(i, j)$ be $\overline{M}_{ij}$. Let the cost of transport from this supplier to consumer $i$ be 0, and the cost to transport to $j$ be $c_{ij}$. These are the only possible routes for goods from supplier $(i, j)$.
    
    Let the amount of goods consumed by consumer $i$ be:
    \begin{equation*}
        \sum_{k : (i, k) \in E} \overline{M}_{ik} - b_i
    \end{equation*}
    and of consumer $j$ be:
    \begin{equation*}
        \sum_{k : (k, j) \in E} \overline{M}_{jk} - b_j
    \end{equation*}

    Now suppose we send a flow $x_{ij}$ from producer $(i, j)$ to consumer $j$. Then the flow from this producer to consumer $i$ must be $\overline{M}_{ij} - x_{ij}$. The cost of transport is:
    \begin{equation*}
        \sum_{(i, j) \in E} c_{ij} x_{ij}
    \end{equation*}
    Then the constraints on this transport problem are:
    For consumer $i$:
    \begin{equation*}
        \sum_{k : (i, k) \in E} \overline{M}_{ik} - b_i = \sum_{k : (i, k) \in E} (\overline{M}_{ik} - x_{ik}) + \sum_{k : (k, i) \in E} x_{ki}
    \end{equation*}
    and simplifying this:
    \begin{equation*}
        b_i + \sum_{k : (k, i) \in E} x_{ki} = \sum_{k : (i, k) \in E} x_{ik}
    \end{equation*}
    which we note is the same as the original flow problem. Therefore, the problems have the same feasible set. We have already seen that the cost functions are the same, so the problems are equivalent.
\end{proof}
\subsection{The Transport Algorithm}
We can consider an algorithm that optimises the Simplex algorithm for transport problems. 
\begin{theorem}[Optimality conditions for the transport problem]
    If, for some feasible transport plan $x$, we have a $\vec{\lambda} \in \R^n$ (with $\lambda_i$ for each supplier $i$), and a $\vec{\mu} \in \R^m$ (with $\mu_j$ for each consumer $j$) such that, for all $i$ and $j$:
    \begin{align*}
        c_{ij} &\geq \lambda_i + \mu_j \\
        0 &= (c_{ij} - (\lambda_i + \mu_j)) x_{ij}
    \end{align*}
\end{theorem}
\begin{proof}
    We find the Lagrangian:
    \begin{align*}
        L(x, \vec{\lambda}, \vec{\mu}) &= \sum_{i = 1}^n \sum_{j = 1}^m c_{ij} x_{ij} - \sum_{i = 1}^n \lambda_i \left(\sum_{j = 1}^m x_{ij} - s_i\right)\\
        &- \sum_{j = 1}^m \mu_j \left(\sum_{i = 1}^n x_{ij} - d_i\right) \\
        &= \sum_{i = 1}^n \sum_{j = 1}^m (c_{ij} - \lambda_i - \mu_j) x_{ij} + \sum_{i = 1}^n \lambda_i s_i + \sum_{j = 1}^m \mu_j d_j
    \end{align*}
    and a pair $(\vec{\lambda}, \vec{\mu})$ is dual feasible if and only if $\lambda_i + \mu_i \leq c_{ij}$ for all $i$ and $j$. Thus, the $(\vec{\lambda}, \vec{\mu})$ pair given by the theorem is dual feasible and satisfies complementary slackness, so $x$ must be optimal.
\end{proof}
Similar to Simplex, we perform the algorithm on a tableau that represents the current state of the algorithm. The table has $n$ rows and $m$ columns:

\begin{tabular}{c|c|c|c|c|c}
     & $\mu_1$ & $\mu_2$ & & $\mu_m$ & \\
    \hline
    $\lambda_1$ & \transportcell{$\lambda_1 + \mu_1$}{$x_{11}$}{$c_{11}$} & \transportcell{$\lambda_1 + \mu_2$}{$x_{12}$}{$c_{12}$} & $\cdots$ & \transportcell{$\lambda_1 + \mu_m$}{$x_{1m}$}{$c_{1m}$} & $s_1$ \\
    \hline
    $\lambda_2$ & \transportcell{$\lambda_2 + \mu_1$}{$x_{21}$}{$c_{21}$} & \transportcell{$\lambda_2 + \mu_2$}{$x_{22}$}{$c_{22}$} & $\cdots$ & \transportcell{$\lambda_2 + \mu_m$}{$x_{2m}$}{$c_{2m}$} & $s_2$ \\
    \hline
    $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ \\
    \hline
    $\lambda_n$ & \transportcell{$\lambda_n + \mu_1$}{$x_{n1}$}{$c_{n1}$} & \transportcell{$\lambda_n + \mu_2$}{$x_{n2}$}{$c_{n2}$} & $\cdots$ & \transportcell{$\lambda_n + \mu_m$}{$x_{nm}$}{$c_{nm}$} & $s_n$ \\
    \hline
     & $d_1$ & $d_2$ & $\cdots$ & $d_m$

\end{tabular}

We consider an example to illustrate, with 3 suppliers and 4 consumers. We first need to come up with any feasible solution to initialise the algorithm. The \underline{north-west corner rule} gives as much supply as possible to the first consumer starting with the first producer, then as much as possible to the second starting with the first producer, and so forth. See figure~\ref{figNWCorner} for this specific case.
\begin{theorem}
    Let $x$ be a basic feasible solution obtained by the north-west corner rule. Then the undirected graph corresponding to the edges with non-zero flow is a spanning tree.
\end{theorem}
\begin{figure}[ht]
    \begin{tikzpicture}
        %TODO: Make this graph.
    \end{tikzpicture}
    \caption{Flow diagram of the north-west corner rule}
    \label{figNWCorner}
\end{figure}
Let the cost matrix be:
\begin{equation*}
    c =
    \begin{pmatrix}
        5 & 3 & 4 & 6 \\
        2 & 7 & 4 & 1 \\
        5 & 6 & 2 & 4
    \end{pmatrix}
\end{equation*}
We then need to consider the values of $\vec{\lambda}$ and $\vec{\mu}$. We have 6 complementary slackness equations (one for each non-zero $x_{ij}$), given by $(c_{ij} - (\lambda_i + \mu_j))x_{ij} = 0$. Therefore, fix $\lambda_1 = 0$ WLOG and then the other equations give:
\begin{equation*}
    \vec{\lambda} = \begin{pmatrix}0 \\ 4 \\ 2\end{pmatrix},~~\vec{\mu} = \begin{pmatrix} 5 \\ 3 \\ 0 \\ 2\end{pmatrix}
\end{equation*}
so then the transport tableau is:

\begin{tabular}{c|c|c|c|c|c}
     & 5 & 3 & 0 & 2 & \\
    \hline
    0 & \transportcell{5}{12}{5} & \transportcell{3}{2}{3} & \transportcell{0}{0}{4} & \transportcell{2}{0}{6} & 14 \\
    \hline
    4 & \transportcell{9}{0}{2} & \transportcell{7}{3}{7} & \transportcell{4}{7}{4} & \transportcell{6}{0}{1} & 10 \\
    \hline
    2 & \transportcell{7}{0}{5} & \transportcell{5}{0}{6} & \transportcell{2}{1}{2} & \transportcell{4}{8}{4} & 9 \\
    \hline
     & 12 & 5 & 8 & 8 
\end{tabular}

Now we choose a cell that has $c_{ij} < \lambda_i + \mu_j$. We choose $(i, j) = (2, 1)$. Consider sending a quantity $\theta$ along the corresponding edge. Use the graph to consider the resulting quantities across different edges.

In this case, the smallest quantity is $3 - \theta$. This means $\theta = 3$, and we update the graph and table accordingly.

\begin{tabular}{c|c|c|c|c|c}
     & 5 & 3 & 7 & 9 & \\
    \hline
    0 & \transportcell{5}{9}{5} & \transportcell{3}{5}{3} & \transportcell{7}{0}{4} & \transportcell{9}{0}{6} & 14 \\
    \hline
    -3 & \transportcell{2}{3}{2} & \transportcell{0}{0}{7} & \transportcell{4}{7}{4} & \transportcell{6}{0}{1} & 10 \\
    \hline
    -5 & \transportcell{0}{0}{5} & \transportcell{-2}{0}{6} & \transportcell{2}{1}{2} & \transportcell{4}{8}{4} & 9 \\
    \hline
     & 12 & 5 & 8 & 8 
\end{tabular}

Then we choose any cell where $\lambda_i + \mu_j > c_{ij}$. In this case choose $(2, 4)$. Continue the process.
\section{Max-Flow Min-Cut}
We now consider a new type of transport problem where there is a single source and a single sink. All other vertices are intermediate.
\begin{definition}{Max-Flow Min-Cut problem}
    A \underline{max-flow min-cut problem} is the following optimisation problem:
    \begin{align*}
        \text{Maximise }& \delta \\
        \text{Subject to }& \sum_{j : (1, j) \in E}x_{1j} - \sum_{j : (j, 1) \in E} x_{j1} = \delta \\
        & \sum_{j : (n, j) \in E}x_{nj} - \sum_{j : (j, n) \in E} x_{nj} = -\delta \\
        & \sum_{j : (i, j) \in E}x_{ij} - \sum_{j : (j, i) \in E} x_{ij} = 0,~~i \neq 1, n \\
        & 0 \leq x_{ij} \leq c_{ij} \text{ for all } i, j.
    \end{align*}
\end{definition}
\subsection{Cuts}
\begin{definition}{Graph Cut}
    The \underline{cut} of a graph $G$ is a partition of its vertices $V = \{1, 2, \cdots, n\}$ into two sets $S$ and $V \backslash S$ such that $1 \in S, n \in V \backslash S$.
\end{definition}
\begin{definition}{Cut Capacity}
    The \underline{capacity} of a graph cut is:
    \begin{equation*}
        C(S) = \sum_{(i, j) \in E, i \in S, j \notin S} c_{ij}.
    \end{equation*}
\end{definition}
\begin{theorem}[Max flow is at most min cut]
    Let $x$ be a feasible flow. Then for any cut $(S, V \backslash S)$, $C(S)$ is at least as large as $\delta$.
\label{thmMaxFlowLessMinCut}
\end{theorem}
\begin{proof}
    For any sets $X$ and $Y \subseteq V$, let:
    \begin{equation*}
        f_x(X, Y) = \sum_{(i, j) \in E, i \in X, j \in Y} x_{ij}
    \end{equation*}
    We note that $X$ and $Y$ need not be disjoint.

    We sum up all the flow constraints over all vertices in $S$.
    \begin{align*}
        \delta &= \sum_{i \in S} \left(\sum_{j : (i, j) \in E} s_{ij} - \sum_{j : (j, i) \in E} x_{ij}\right) \\
        &= f_x(S, V) - f_x(V, S) \\
        &= \left(f_x(S, V \backslash S) + f_x(S, S)\right) - \left(f_x(S, S) - f_x(V \backslash S, S)\right) \\
        &= f_x(S, V \backslash S) - f_x(V \backslash S, S) \\
        &\leq C(S) + 0
    \end{align*}
    as required.
\end{proof}
\begin{theorem}[Max-flow min-cut theorem]
    Let $\delta^*$ be the optimal value of the flow from the source to the sink.
    \begin{equation*}
        \delta^* = \min \subsetselect{C(S)}{C \subseteq V, 1 \in S, n \notin S}
    \end{equation*}
    that is, there exists a cut such that the capacity of the cut is the maximal flow.
\label{thmMaxFlowMinCut}
\end{theorem}
\begin{proof}
    A path $v_0, v_1, \cdots, v_k$ is a sequence of vertices in $V$ such that all adjacent vertices are connected by an edge (either forward \textit{or backward}).

    A path from vertex $1$ (the source) to vertex $n$ (the sink) is called an \underline{augmenting path} if every forward edge is not saturated (the flow is less than the capacity, $x_{ij} < c_{ij}$) and every reverse edge has non-zero flow. We can reason that if such an augmenting path exists, the resulting $x$ cannot be optimal.

    Now we can define a cut. Let $S$ be the set of all vertices that can be reached from $1$ by an augmenting path. In the optimal case, where the flow is $\delta^*$, $n \notin S$ be the above. We now claim that this cut has $C(S) = \delta^*$.
    \begin{align*}
        \delta^*&= f_x(S, V\backslash S) - f_x(V \backslash S, S) \text{from theorem~\ref{thmMaxFlowLessMinCut}.}
        &= C(S) + 0
    \end{align*}
    this is because any edge from $S$ to $V \backslash S$ must be at capacity, since this cannot be an augmenting path, and any reverse edges from $V \backslash S$ to $S$ must be empty.
\end{proof}
This proof suggests the basics of an algorithm: we start with a feasible flow, and look for augmenting paths; for any augmenting paths, we push flow along them until there are no more.
\subsection{Ford-Fulkerson Algorithm}
The Ford-Fulkerson Algorithm solves the Max-Flow Min-Cut problem. It is as follows.
\begin{enumerate}
    \item Start with $x = 0$ (or any other feasible flow).
    \item Search for an augmenting path, push as much flow as possible along that path.
    \item Stop when no augmenting paths exist.
\end{enumerate}
We then can use theorem~\ref{thmMaxFlowMinCut} to find an optimal cut by selecting the vertices with augmenting paths from $1$. Then if $\delta = C(S)$, we have the optimal flow $\delta^*$.

We know that, for a graph with only integer capacities, the Ford-Fulkerson algorithm will converge in a finite amount of time. For non-integer capacities, the algorithm may or may not converge.
\end{document}