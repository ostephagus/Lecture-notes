\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
For $\Omega \subseteq \R^n$,
\begin{align*}
    C^k(\Omega) &= \subsetselect{y : \Omega \mapsto \R}{y \text{ is } k \text{ times differentiable}} \\
    C^\infty(\Omega) &= \subsetselect{y : \Omega \mapsto \R}{y \text{ is smooth}}
\end{align*}
Let $\funcset$ be a set of functions, such as $C^\infty(\Omega)$. Recall that a \underline{functional} is a map:
\begin{align*}
    F : \funcset &\mapsto \R \\
    y &\mapsto F[y]
\end{align*}
More generally, a functional can map several functions to a number.
\section{Variation}
\subsection{Single argument}
An important case is when $\Omega = [\alpha, \beta]$, $\funcset = C^\infty$, and the functional is defined:
\begin{equation*}
    F[y] = \int_\alpha^\beta f(y(x), y'(x), x) dx
\end{equation*}
Then we consider what extremises this function. Consider varying $y$ to $y + \delta y \in \funcset$.
\begin{align*}
    \delta F[y] &= F[y + \delta y] - F[y] = \int_\alpha^\beta \left[f(y + \delta y, y' + \delta y', x) - f(y, y', x)\right]dx \\
    &= \int_\alpha^\beta \left(\delta y \frac{\partial f}{\partial y} + \delta y' \frac{\partial f}{\partial y'}\right)dx + \cdots \\
    &= \int_\alpha^\beta \delta y(x) \left(\frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'}\right)dx + \left[\delta y \frac{\partial f}{\partial y'}\right]_\alpha^\beta + \cdots
\end{align*}
Then we can assume that the definition of $\funcset$ includes boundary conditions that ensure that the last term is zero. For example, 
\begin{itemize}
    \item $y(\alpha) = \alpha, y(\beta) = \beta$ \\
    \item $\frac{\partial f}{\partial y'} = 0$ at $x \in \{\alpha, \beta\}$.
\end{itemize}
Therefore:
\begin{equation*}
    \delta F[y] = \int_\alpha^\beta \delta y(x) \frac{\delta F[y]}{\delta y(x)} dx + \cdots
\end{equation*}
where we define the \underline{functional derivative}:
\begin{equation*}
    \frac{\delta F[y]}{\delta y(x)} \equiv \frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'}
\end{equation*}
Then $\delta F[y]$ vanishes to first order for arbitrary $\delta y(x)$ if and only if $y(x)$ satisfies the \underline{Euler-Lagrange equation}:
\begin{equation}
    \frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'} = 0
    \label{eqnEulerLagrange}
\end{equation}
Then this is a second-order differential equation for $y$, and solving gives the extremum of the functional $F$.
\subsection{Multivariate generalisation}
Then we can generalise this. Consider $\vec{y}(x) = (y_1(x), \cdots, y_n(x)), x \in [\alpha, \beta]$. Therefore now $\vec{y} : [\alpha, \beta] \mapsto \R^n$. Now define:
\begin{equation*}
    F[\vec{y}] = \int_\alpha^\beta f(\vec{y}, \vec{y}', x) dx
\end{equation*}
Then, with suitable boundary conditions as seen above, we have $n$ Euler-Lagrange equations:
\begin{equation}
    \frac{\partial f}{\partial y_i} - \frac{d}{dx} \frac{\partial f}{\partial y'_i} = 0
    \label{eqnMultivariateEulerLagrange}
\end{equation}
\section{Geodesics for the Euclidean Plane}
What is the shortest curve between two points $A$, $B$ of Euclidean space? We define the length to be a functional:
\begin{equation*}
    L[y] = \int_C \sqrt{dx^2 + dy^2} = \int_C dl
\end{equation*}
for any curve $C$ defined by a function $y(x)$.
Then we have that $L[y] = \int_\alpha^\beta \sqrt{1 + y^{\prime 2}}dx$. The Euler-Lagrange equation becomes:
\begin{equation*}
    -\frac{d}{dx} \frac{\partial f}{\partial y'}
\end{equation*}
Integrating once with respect to $x$:
\begin{equation*}
    \frac{\partial f}{\partial y'} = c
\end{equation*}
Which, after solving, gives that $y'$ is fixed and $y = mx + c$. We fix these constants by the boundary condition that the line must go through the points.

However, using $y$ as a function of $x$ excludes some curves. Instead we could have that the curve is parameterised by $t \in [0, 1]$, $y$ and $x$ are functions of $t$. Ensure that $t = 0$ at $A$ and $t = 1$ at $B$.

\begin{equation*}
    L[\vec{x}] = \int_0^1 \sqrt{\dot{x}^2 + \dot{y}^2} dt
\end{equation*}
Then the Euler-Lagrange equation gives:
\begin{equation*}
    -\frac{d}{dx} \frac{\partial f}{\partial \dot{\vec{x}}} = 0
\end{equation*}
This then gives:
\begin{align*}
    \frac{\dot{x}}{\sqrt{\dot{x}^2 + \dot{y}^2}} &= c \\
    \frac{\dot{y}}{\sqrt{\dot{x}^2 + \dot{y}^2}} &= s \\
\end{align*}
Then we have that $c^2 + s^2 = 1$, so it is convenient to let $c = cos(\theta), s = \sin(\theta)$.
This is as far as we can get with a general parameter $t$. We can then instead use arc length $l$ as the parameter:
\begin{equation*}
    \frac{dl}{dt} = \sqrt{\dot{x}^2 + \dot{y}^2}
\end{equation*}
Then results in the equations:
\begin{equation*}
    x = x_A + l \cos{\theta},~~y = y_A + l\sin{\theta}
\end{equation*}
Note that $\theta$ is fixed by ensuring the curve passes through $B$. We could not use $l$ as a parameter initially because in deriving the Euler-Lagrange equation we assume that the limits of integration are fixed, which is not the case for an arc length parameter.
\section{First Integrals and Fermat's Principle}
\subsection{First Integrals}
In general, if $\frac{\partial f}{\partial y} = 0$, then equation~\ref{eqnEulerLagrange} can be immediately integrated to give that:
\begin{equation}
    \frac{\partial f}{\partial y'} = \text{ const}
    \label{eqnFirstIntegral}
\end{equation}
This is a first-order differential equation, which is easier to solve than the Euler-Lagrange equation (which is second-order)

Another case where there is a first integral is when $f$ has no $x$ dependence:
\begin{proposition}
    If $\frac{\partial f}{\partial x} = 0$. Then:
    \begin{equation}
        f - y' \frac{\partial f}{\partial y'} = 0
        \label{eqnFirstIntegral2}
    \end{equation}
    for any solution $y$ of the Euler-Lagrange equation
    \label{propFirstIntegral}
\end{proposition}
\begin{remark}
    Note that there is still implicit dependence on $x$ in this case through $y(x)$. However, the explicit dependence is absent.
\end{remark}
\begin{proof}
    \begin{align*}
        \frac{df}{dx} &= \frac{d}{dx} f(y(x), y'(x), x) \\
        &= \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y}y' + \frac{\partial f}{\partial y'} y'' \\
        &= \frac{\partial f}{\partial x} + y'\left[\frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'}\right] + \frac{d}{dx} \left(y' \frac{\partial f}{\partial y'}\right) \\
        &= \frac{\partial f}{\partial x} + \frac{d}{dx} \left(y' \frac{\partial f}{\partial y'}\right) \text{ by Euler-Lagrange}
    \end{align*}
    Then if $\frac{df}{dx} = 0$, we have the required form after integrating once.
\end{proof}
\subsection{Fermat's Principle}
Consider light propogating in a medium. Let $c(\vec{x})$ be the speed of light in a medium. Let $c$ be the speed of light in a vacuum. The refractive index is:
\begin{equation*}
    n(\vec{x}) = \frac{c}{c(\vec{x})}
\end{equation*}
Then the time taken for light to travel along a path $C$ from $A$ to $B$ is:
\begin{equation*}
    T[C] = \int_C \frac{dl}{c(\vec{x})}
\end{equation*}
and it is convention to define the \textit{optical path length}
\begin{equation*}
    P[C] = cT[C] = \int_c n(\vec{x}) dl
\end{equation*}
Then \underline{Fermat's Principle} is that light travels from $A$ to $B$ along a path that (locally) minimises $P$ (or equivalently $T$).
\begin{example}
    Find the path of a light ray in the $x, z$ plane through a medium with refractive index $n(z) = \sqrt{a - bz}$.
    Consider a path parameterised by $x$. Let $x \in [\alpha, \beta]$.
    \begin{align}
        P[z] = \int_\alpha^\beta n(z) \sqrt{1 + z'} dx = \int_\alpha^\beta f(z, z')
    \end{align}
    Note that there is no explicit $x$ dependence, so we can use the first integral in equation~\ref{eqnFirstIntegral2}:
    \begin{align*}
        k &= f - z'\frac{\partial f}{\partial z'} \\
        &= \frac{n(z)}{\sqrt{1 + z^{\prime 2}}} \\
        &= \sqrt{\frac{a - bz}{1 + z^{\prime 2}}}
    \end{align*}
    Then our differential equation looks like:
    \begin{equation*}
        z' = \pm \sqrt{\frac{b}{k^2}(z_0 - z)} \\
    \end{equation*}
    Which, after integrating, gives:
    \begin{equation*}
        z = z_0 - \frac{b}{4k^2}(x - x_0)^2
    \end{equation*}
    where $z_0 = \frac{a - k^2}{b}$.
\end{example}
\begin{example}[Brachistochrone curve]
    Consider a bead that slides on a frictionless wire in a vertical plane. The problem is to find the shape that will minimise the time taken for the bead to fall from rest at $A$ to a lower point $B$.

    Choose axes such that $A$ is at the origin. Consider conservation of energy:
    \begin{equation*}
        \frac{1}{2} mv^2 + mgy = 0
    \end{equation*}
    We can rearrange to determine the speed at any point:
    \begin{equation*}
        |v| = \sqrt{2gy}
    \end{equation*}
    Then we consider the time taken for the bead to travel along a path $C$:
    \begin{align*}
        T[C] &= \int_C \frac{dl}{v} = \frac{1}{\sqrt{2g}} \int_C \frac{dx^2 + dy^2}{\sqrt{-y}} \\
        &\propto \int_0^{x_B} \frac{1 + y^{\prime 2}}{\sqrt{-y}} \\
        &= \int_0^{x_B} f(y, y') dx
    \end{align*}
    where $f = \sqrt{\frac{1 + y^{\prime 2}}{-y}}$. Again there is no explicit $x$ dependence, so we have a first-order equation:
    \begin{align*}
        \text{const } &= f - y' \frac{\partial f}{\partial y'} = \frac{1}{\sqrt{(-y)(1 + y^{\prime 2})}} \\
        \implies 2c &= (-y)(1 + y^{\prime 2})
    \end{align*}
    Then we can solve tihs by substituting $y = -c(1 - \cos{\theta})$ and obtain (eventually):
    \begin{equation*}
        x = c(\theta - \sin{\theta})
    \end{equation*}
    which defines an (inverted) cycloid, the fastest path for a bead to fall along to get to $B$. $c$ is fixed by ensuring that the curve passes through $B$.
\end{example}
\section{Constraints}
We can also extremise a functional subject to some constriaints. Consider the problem to extremise $F[y]$ subject to a functional constraint $P[y] = c$. We introduce a Lagrange multiplier:
\begin{equation*}
    \Phi_\lambda[y] = F[y] - \lambda (P[y] - c)
\end{equation*}
Then extremising this with respect to $y$ means that:
\begin{equation}
    \frac{\delta F}{\delta y(x)} - \lambda \frac{\delta P}{\delta y(x)} = 0
    \label{eqnEulerLagrangeConstraint}
\end{equation}
and we need to extremise with respect to $\lambda$ which recovers the constraint.

\begin{example}
    Determine the $y(x)$ that minimises:
    \begin{equation*}
        \int_0^1 \frac{1}{2} y^{\prime 2} dx 
    \end{equation*}
    subject to a constraint:
    \begin{equation*}
        \int_0^1 y dx = 1
    \end{equation*}
    over the class of smooth functions on $[0, 1]$ with boundary condition $y(0) = y(1) = 0$.

    Therefore, we formulate $\Phi$:
    \begin{align*}
        \Phi_\lambda[y] &= \int_0^1 \frac{1}{2} y^{\prime 2} dx - \lambda \int_0^1 y dx + \lambda \\
        &= \int_0^1 \left(\frac{1}{2} y^{\prime 2} - \lambda y\right)dx + \lambda
    \end{align*}
    setting $f$ to be the bracketed term in the integral.

    Then equation~\ref{eqnEulerLagrangeConstraint} gives:
    \begin{equation*}
        -\lambda - y'' = 0
    \end{equation*}
    and solving this gives:
    \begin{equation*}
        y = -\frac{1}{2} \lambda x^2 + Ax + B
    \end{equation*}
    Then substituting in the boundary conditions and constraint gives:
    \begin{equation*}
        y = 6x(1 - x)
    \end{equation*}
\end{example}
\subsection{Sturm-Liouville Problem}
Let the set of functions be:
\begin{equation*}
    \funcset = \subsetselect{y : [\alpha, \beta] \mapsto \R}{y \text{ is twice continuously differentiable}, y(\alpha) = y(\beta) = 0}
\end{equation*}
Then let:
\begin{align*}
    F[y] &= \int_\alpha^\beta [\rho(x) y'(x)^2 + \sigma(x) y(x)^2] dx \\
    G[y] &= \int_\alpha^\beta w(x) y(x)^2 dx
\end{align*}
where $\rho(x), w(x) > 0$ on $[\alpha, \beta]$.

The problem is then to minimise $F[y]$ in $\funcset$ subject to $G[y] = 1$. First extremise:
\begin{equation*}
    \Phi_\lambda[y] = F[y] - \lambda\left(G[y] - 1\right)
\end{equation*}
\begin{equation}
    \frac{\delta F[y]}{\delta y(x)} - \lambda \frac{\delta G[y]}{\delta y(x)} = 0
    \label{eqnSLProbEulerLagrange}
\end{equation}
Evaluating these functional derivatives:
\begin{equation*}
    \frac{\delta F[y]}{\delta y(x)} = 2\sigma y - \frac{d}{dx} (2 \rho y') = 2Ly
\end{equation*}
where $L$ is an operator, $L = -\frac{d}{dx} (\rho \frac{d}{dx}) + \sigma$.
\begin{equation*}
    \frac{\delta G[y]}{\delta y(x)} = 2wy
\end{equation*}
and therefore equation~\ref{eqnSLProbEulerLagrange} becomes:
\begin{equation}
    Ly = \lambda w y
    \label{eqnSLProbEFuncs}
\end{equation}
Then equation~\ref{eqnSLProbEFuncs} with boundary conditions from earlier is an \underline{eigenvalue problem}, only for discrete values of $\lambda$ does there exist a non-trivial solution (eigenfunction) $y(x)$. This is a \textit{Sturm-Liouville problem}, which will be discussed further in the course IB Methods. $w(x)$ is a \underline{weight function}. If $y(x)$ solves the equation then so does any multiple of it. The constraint $G[y] = 1$ fixes a normalisation of $y$.

Integrating by parts:
\begin{align*}
    F[y] &= \int_\alpha^\beta y Ly dx + \left[\rho y y'\right]_\alpha^\beta \\
    &= \int_\alpha^\beta \lambda w y^2 dx \text{ by equation~\ref{eqnSLProbEFuncs}} \\
    &= \lambda G[y] = \lambda
\end{align*}
so the solution is the smallest eigenvalue $\lambda_0$.

As we have done before, we can consider the ratio of the functionals:
\begin{equation*}
    \Lambda[y] = \frac{F[y]}{G[y]}
\end{equation*}
and look for the unconstrained extremum of $\Lambda$. The functional derivative is:
\begin{equation*}
    \frac{\delta \Lambda}{\delta y(x)} = \frac{1}{G} \left(\frac{\delta F}{\delta y(x)} - \frac{F}{G} \frac{\delta G}{\delta y(x)}\right) = 0
\end{equation*}
And we can derive:
\begin{equation*}
    0 = \frac{2}{G} \left(Ly - \lambda w y\right)
\end{equation*}
which is the same as we had before.
\subsection{Geodesics and function constraints}
Consider a surface $S$ in Euclidean space with defining equation $g(\vec{x}) = 0$. Then let $A, B \in S$. We want to find the shortest curve on $S$ between $A$ and $B$.

Let $\vec{x}(t)$ be a curve on $S$ between $A$ and $B$. Let $t = 0$ give the point $A$ and $t = 1$ give the point $B$.

We want to extremise the length functional:
\begin{equation*}
    L[\vec{x}] = \int_0^1 \sqrt{\dot{\vec{x}}^2} dt
\end{equation*}
subject to the constraint that $g(\vec{x}(t)) = 0$. We again use a Lagrange multiplier method, but using this function constraint rather than the functional constraint. Define $\lambda(t)$ to be this lagrange multiplier. Then:
\begin{equation*}
    \Phi[\vec{x}, \lambda] = L[\vec{x}] - \int_0^1 \lambda(t) g(\vec{x}(t)) dt
\end{equation*}
Extremising with respect to $\lambda$ recovers the constraint, and extremising with respect to $x_i$ gives:
\begin{equation*}
    0 = \frac{\delta \Phi}{\delta x_i(t)} = -\lambda \left[\nabla g(\vec{x}(t))\right]_i -\frac{d}{dx} \left(\frac{x_i}{\sqrt{\dot{\vec{x}}^2}}\right)
\end{equation*}
Then we can eliminate $\lambda$. Eventually,
\begin{equation*}
    \lambda(t) = \frac{\dot{x}_i \dot{x}_j \frac{\partial^2 g}{\partial x_i \partial x_j}}{(\nabla g)^2 \sqrt{\dot{\vec{x}}^2}}
\end{equation*}
and then we arrive at:
\begin{equation*}
    \frac{1}{\sqrt{\dot{\vec{x}}^2}} \frac{d}{dt}\left(\frac{\dot{x}_i}{\sqrt{\dot{\vec{x}}^2}}\right) + \frac{\dot{x}_j \dot{x}_k \frac{\partial^2 g}{\partial x_j \partial x_k}}{(\nabla g)^2 \dot{\vec{x}}^2} [\nabla g]_i
\end{equation*}
If then we parameterise with arc length rather than $t$, and reintroduce the Hessian matrix for $g$:
\begin{equation*}
    \frac{d^2 x_i}{dl^2} + \frac{1}{(\nabla g)^2}\frac{dx_j}{dl} \frac{dx_k}{dl} H_{jk} [\nabla g]_i
\end{equation*}
\end{document}