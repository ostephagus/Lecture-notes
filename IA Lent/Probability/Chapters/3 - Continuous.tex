\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Defining Continuous Probability}
Previously we defined the probability space $\left(\Omega, \sigalg, P\right)$, where $X$ is a random variable $X : \Omega \mapsto \R$ with a notion of order: $\forall x \in \R \{X \leq x\} = \subsetselect{\omega}{X(\omega) \leq x} \in \sigalg$.
\begin{definition}{Probability distribution function}
    The \underline{probability distribution function} is defined to be:
    \begin{align*}
        F : \R &\mapsto [0, 1] \\
        x &\mapsto P(X \leq x)
    \end{align*}
\end{definition}
\begin{propositions}{
        Suppose that $F$ is a probability distribution function as above defined.
        \label{propsPDFProps}
    }
    \item If $x \leq y$, then $F(x) \leq F(y)$. \label{propPDFIncreasing}
    \item For any real numbers $a < b$, $P(a < X \leq b) = F(b) - F(a)$. \label{propPDFSubtract}
    \item $F$ is right-continuous, and left limits always exist. \label{propPDFContinuity}
    \item $\lim_{y \to x^-} F(y) = P(X < x)$.\label{propPDFStrictLessThan}
    \item $\lim_{x \to \infty} F(x) = 1$ and $\lim_{x \to -\infty} F(x) = 0$. \label{propPDFLimits}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Simply note that $\{X \leq x\} \subseteq \{X \leq y\}$.
        \item \begin{align*}
            P(A < X \leq B) &= P(X \leq b, X > a) \\
            &= P(X \leq b) - P(X \leq b, X \leq a) \\
            &= P(X \leq b) - P(X \leq a) \\
            &= F(b) - F(a)
        \end{align*}
        \item Let $x_n$ be a decreasing sequence converging to $x$ as $n \to \infty$.
            Define $A_n = \{x < X \leq x_n\}$. Note that $A_n$ is a decreasing sequence ($A_{n + 1} \subseteq A_n$).\par
            Note that for a decreasing sequence,
            \begin{equation*}
                P(A_n) \to P\left(\bigcap_{n \in \N} A_n\right)
            \end{equation*}
            and in this case the infinite intersection is the empty set, so $P(A_n) \to 0$.\par
            Therefore $F(x_n) - F(x) \to 0$, and so $F$ is right-continuous.\par
            To show that left limits exist, we can bound them from above (since $F$ is increasing) by their limit point:
            \begin{equation*}
                \lim_{y \to x^-} F(y) \leq F(x)
            \end{equation*}
        \item $\lim_{y \to x^-} F(y) = \lim_{n \to \infty} F(x - \frac{1}{n})$ (we can choose any increasing sequence converging to $x$).\par
            Define also $B_n = \{X \leq x - \frac{1}{n}\}$. $B_n$ is increasing, and so $P(B_n)$ tends to the probability of the union. The probability of the union is exactly equal to $\{X < x\}$.
        \item Proof by the previous case.
    \end{enumerate}
\end{proof}
\begin{definition}{Continuous random variable}
    A random variable $X$ is \underline{continuous} if its probability distribution function is continuous. That is,
    \begin{equation*}
        \lim_{y \to x^-} F(y) = \lim_{y \to x^+} F(x)
    \end{equation*}
\end{definition}
Immediately this tells us that $P(X < x) = P(X \leq x)$, or $P(X = x) = 0$.\par
$F$ may not always be differentiable. However, in this course, we will assume the differentiability of $F$ on the whole of $\R$.
\begin{definition}{Probability density function}
    Given that, for a random variable $X$, the probability distribution function $F$ is differentiable everywhere, the \underline{probability density function} $f$ is defined to be the derivative $F'$.
\end{definition}
%TODO: Are these properties that can be derived or defining properties?
\begin{propositions}{
        Consider a random variable $X$, with probability distribution function $F$ and probability density function $f$.
        \label{propsPdensFuncProps}
    }
    \item $f(x) \geq 0$. \label{propPdensFuncNonNegative}
    \item $\int_{-\infty}^\infty f(x) dx = 1$ \label{propPdensFuncIntegralOne}
    \item $F(x) = \int_{-\infty}^x f(y) dy$. \label{propPdensFuncIntegratePDF}
\end{propositions}
More generally, for any subset of the real numbers $A$,
\begin{equation*}
    P(X \in A) = \int_A f(x) dx
\end{equation*}
\section{Simple Continuous Distributions}
\subsection{Uniform Distribution}
\begin{definition}{Uniform distribution}
    Given a real interval $[a, b]$, the \underline{uniform distribution} on that interval is $U([a, b])$. Its probability density function is: 
    \begin{equation*}
        f(x) =
        \begin{cases}
            \frac{1}{b-a} & x \in [a, b] \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    The probability $P(X \leq x)$ is:
    \begin{equation*}
        P(X \leq x) = \int_{-\infty}^x f(y) dy = \frac{x - a}{b - a}
    \end{equation*}
\end{definition}
\subsection{Exponential Distribution}
\begin{definition}{Exponential distribution}
    Let $\lambda > 0$ be a real number. Then the \underline{exponential distribution} is $\text{Exp}(\lambda)$, with probability density function:
    \begin{equation*}
        f(x) =
        \begin{cases}
            \lambda e^{-\lambda x} & x > 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{proposition}
    Let $T \sim Exp(\lambda)$, let $T_n = \lfloor nT \rfloor$.\par
    Then $T_n$ is a geometric distribution, and
    \begin{equation*}
        \frac{T_n}{n} \to T \text{ as } n \to \infty
    \end{equation*}
    \label{propExpLimitOfGeom}
\end{proposition}
\begin{proof}
    For any $k \in \N$,
    \begin{align*}
        P(T_n \geq k) &= P(\lfloor nT \rfloor \geq k) \\
        &= P(nt \geq k) \\
        &= P(T \geq \frac{k}{n}) \\
        &= e^{-\frac{\lambda k}{n}} \\
        &= \left(e^{-\frac{\lambda}{n}}\right)^k
    \end{align*}
    Note that this is a geometric random variable with success parameter $p_n = 1 - e^{-\frac{\lambda}{n}}$, as required, and this tends to $\frac{\lambda}{n}$ when $n$ is large.\par
    Therefore $\frac{T_n}{n} \to T$ as $n \to \infty$.
\end{proof}
\begin{proposition}[Memoryless property]
    If $T$ is an exponential random variable with parameter $\lambda$,
    \begin{equation*}
        P(T \geq t + s | T \geq s) = P(T \geq t)
    \end{equation*}
    \label{propExpMemoryless}
\end{proposition}
\begin{proof}
    \begin{align*}
        P(T \geq t + s | T \geq s) &= \frac{P(T \geq t + s, T \geq s)}{P(T \geq s)} \\
        &= \frac{P(T \geq t + s)}{P(T \geq s)} \\
        &= \frac{e^{-\lambda(t + s)}}{e^{-\lambda s}} \\
        &= e^{-\lambda t}
    \end{align*}
\end{proof}
\begin{theorem}[Memoryless property unique to exponentials]
    Let $T$ be a positive random variable with a density $g$, not identically equal to $0$ or $\infty$. Then $T$ has the memoryless property only if $T$ is an exponential random variable.
    \label{thmExpMemorylessUnique}
\end{theorem}
\begin{proof}
    $T$ has the memoryless property, so $g(t + s) = g(t) g(s)$.\par
    If $t > 0$ and $m \in \N$,
    \begin{equation*}
        g(mt) = \left(g(t)\right)^m
    \end{equation*}
    Then setting $m = 1$ gives $g(m) = g(1)^m$.\par
    Now set $\lambda = -\log(P(T \geq 1))$, so $P(T \geq 1) = e^{-\lambda}$.\par
    Therefore, for any $m \in \N$, $g(m) = e^{-\lambda m}$. Let also $n \in \N$:
    \begin{equation*}
        g(\frac{m}{n}) = e^{-\lambda \frac{m}{n}}
    \end{equation*}
    Therefore we have the property that $g(r) = r^{-\lambda r}$ for any $r \in \Q_+$.\par
    Let $t$ be a positive real number. Then let $s$ and $r \in \Q_+$ such that $s < t < r$ and $|s - r| \leq \epsilon$. Since $g$ is a decreasing function,
    \begin{equation*}
        g(r) \leq g(t) \leq g(s) \implies e^{-\lambda r} \leq g(t) \leq e^{-\lambda s}
    \end{equation*}
    Then taking the limit as $\epsilon$ goes to zero gives the required result for positive real values also.
\end{proof}
\section{Expectation and Variance}
\subsection{Expectation}
\begin{definition}{Continuous expectation}
    The \underline{expectation} of a non-negative continuous random variable $X$, with a probability density function $f$, is:
    \begin{equation*}
        E[X] = \int_0^\infty xf(x) dx
    \end{equation*}
\end{definition}
\begin{remarks}
    \item We can define the expectation for random variables that take negative values in the same way as in the discrete case, using $X_+$ and $X_-$. This gives:
        \begin{equation*}
            E[X] = \int_{-\infty}^\infty xf(x) dx
        \end{equation*}
    \item Expectation is still a linear function, i.e.
        \begin{equation*}
            E\left[\sum_{i = 1}^n a_i X_i\right] = \sum_{i = 1}^n a_i E[X_i]
        \end{equation*}
\end{remarks}
\begin{lemma}
    Given $X \geq 0$, the expectation is:
    \begin{equation*}
        E[X] = \int_0^\infty P(X \geq x)
    \end{equation*}
\end{lemma}
\begin{proof}
    \begin{align*}
        E[X] &= \int_0^\infty x f(x) dx \\
        &= \int_0^\infty \left(\int_0^x 1 dy\right)f(x) dx \\
        &= \int_0^\infty \left(\int_0^y f(x) dx\right) dy \\
        &= \int_0^\infty P(X \geq y) dy
    \end{align*}
\end{proof}
\subsection{Variance}
\begin{definition}{Variance}
    Given a continuous random variable $X$, the \underline{variance} is:
    \begin{equation*}
        \Var(X) = E\left[(X - E[X])^2\right] = E\left[X^2\right] - \left(E[X]\right)^2
    \end{equation*}
\end{definition}
\begin{remark}
    Note that this definition needs no modification from the discrete case.
\end{remark}
\begin{example}[Expectation and variance of uniform distribution]
    Let $X \sim U([a, b])$. Then the expectation is:
    \begin{align*}
        E[X] &= \int_{-\infty}^\infty xf(x) dx \\
        &= \int_a^b \frac{x}{b-a} dx \\
        &= \frac{b^2-a^2}{2(b-a)} \\
        &= \frac{a + b}{2}
    \end{align*}
    That is, the expectation of a uniform distribution is its midpoint, as we would expect.\par
    The variance of a uniform distribution is $\frac{1}{12}(b - a)^2$.
\end{example}
\begin{example}[Expectation and variance of exponential distribution]
    Let $X \sim Exp(\lambda)$. Then its expectation is:
    \begin{align*}
        E[X] &= \int_0^\infty xf(x) \\
        &= \int_0^\infty x \lambda e^{-\lambda x} \\
        &= \frac{1}{\lambda}
    \end{align*}
    As we would expect, given the result of proposition~\ref{propExpLimitOfGeom}.\par
    Note also that its variance is $\frac{1}{\lambda^2}$.
\end{example}
\section{The Normal Distribution}
\begin{definition}{Normal distribution}
    Given real numbers $\mu$ and $\sigma$, with $\sigma > 0$, define the \underline{normal distribution} with parameters $\mu$ and $\sigma$, $N(\mu, \sigma^2)$. Let the probability density function be:
    \begin{equation*}
        f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
    \end{equation*}
\end{definition}
\begin{proposition}
    $f(x)$, as above defined, is a valid density function.
\end{proposition}
\begin{proof}
    We need to show that $I = \int_{-\infty}^\infty f(x) dx = 1$.
    First change variables to $u = \frac{x - \mu}{\sigma}$, so $dx = \sigma du$:
    \begin{align*}
        \int_{-\infty}^\infty \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} &= \int_{-\infty}^\infty \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{u^2}{2}} \sigma du \\
        &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}} du
    \end{align*}
    Then consider $I^2$ and use polar coordinates:
    \begin{align*}
        I^2 &= \frac{1}{2\pi} \int_{x = -\infty}^\infty \int_{y = -\infty}^\infty e^{-\frac{x^2}{2}} e^{-\frac{y^2}{2}} dx dy \\
        &= \frac{1}{2\pi} \int_{x = -\infty}^\infty \int_{y = -\infty}^\infty e^{-\frac{x^2 + y^2}{2}} dx dy \\
        &= \frac{1}{2\pi} \int_{r = 0}^\infty \int_{\phi = 0}^{2\pi} e^{-\frac{r^2}{2}} r dr d\phi \\
        &= \frac{1}{2\pi} \int_{r = 0}^\infty \int_{\phi = 0}^{2\pi} e^{-\frac{r^2}{2}} r dr d\phi \\
        &= \int_{r = 0}^\infty \frac{d}{dx}\left(-e^{-\frac{r^2}{2}}\right) dr \\
        &= 1
    \end{align*}
    Therefore, since $f(x)$ is a positive function, we must have the positive root:
    \begin{equation*}
        I = 1
    \end{equation*}
\end{proof}
\begin{propositions}{
        Let $X \sim N(\mu, \sigma^2)$.
        \label{propsNormalMeanVar}
    }
    \item $E[X] = \mu$ \label{propNormalMean}
    \item $\Var(X) = \sigma^2$ \label{propNormalVar}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Expectation given by:
            \begin{align*}
                E[X] &= \int_{-\infty}^\infty xf(x) dx \\
                &= \int_{-\infty}^\infty x \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx \\
                &= \int_{-\infty}^\infty \frac{x - \mu}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx + \int_{-\infty}^\infty \frac{\mu}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx \\
                &= 0 + \mu
            \end{align*}
        \item Variance given by:
            \begin{align*}
                \Var(X) &= E[(X - \mu)^2] \\
                &= \int_{-\infty}^\infty \frac{(x - \mu)^2}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx
            \end{align*}
            Change variables to $u = \frac{x - \mu}{\sigma}$:
            \begin{align*}
                \Var(X) &= \int_{-\infty}^\infty \frac{\sigma^2u^2}{\sqrt{2\pi}} e^{-\frac{u}{2}}du \\
                &= \sigma^2 \int_{-\infty}^\infty \frac{u^2}{\sqrt{2\pi}} e^{-\frac{u}{2}} du
            \end{align*}
            Which is equal to $\sigma^2$ (integration by parts).
    \end{enumerate}
\end{proof}
\subsection{Transforming a Distribution}
\begin{theorem}
    Let $X$ have density $f$. Let $g$ be a continuous function which is strictly monotone and has inverse $g^{-1}$ which is differentiable.\par
    Then $g(X)$ has a density given by:
    \begin{equation*}
        f(g^{-1}(x)) \left|\frac{d(g^{-1})}{dx}\right|
    \end{equation*}
\end{theorem}
\begin{proof}
    \begin{case}{$g$ is strictly increasing.}
        \begin{align*}
            P(g(X) \geq x) &= P(X \leq g^{-1}(x)) \\
            &= F(g^{-1}(x))
        \end{align*}
        Then denote by $f_{g(X)}$ the density function of $g(X)$.
        \begin{align*}
            f_{g(X)} &= \frac{d}{dx} P(g(X) \leq x) \\
            &= f(g^{-1}(x)) \frac{d(g^{-1}(x))}{dx}
        \end{align*}
        Note also that since $g$ is increasing, the derivative of its inverse (which is also increasing) is positive.
    \end{case}
    \begin{case}{$g$ is strictly decreasing.}
        Proceed as above, but note:
        \begin{equation*}
            P(g(X) \geq x) = -f(g^{-1}(x)) \frac{d(g^{-1}(x))}{dx}
        \end{equation*}
        and that its derivative is negative, but since we use absolute value:
        \begin{equation*}
            f_{g(X)} = f(g^{-1}(x)) \left|\frac{d(g^{-1})}{dx}\right|
        \end{equation*}
    \end{case}
\end{proof}
\begin{example}[Transforming the normal distribution]
    Suppose $X \sim N(\mu, \sigma^2)$. Let $a, b$ be real numbers and $a \neq 0$. Define $g(x) = ax + b$, and $g(X) = Y$. Then we consider the density function of $Y$.\par
    First note that $g^{-1} = \frac{x - b}{a}$, and that its derivative is $\frac{1}{a}$. Then $f_{Y}$ is:
    \begin{align*}
        f_Y(y) &= f_X (g^{-1}(y)) \left|\frac{d(g^{-1}(y))}{dy}\right| \\
        &= \frac{1}{\sigma\sqrt{2\pi}} \exp{\left(-\frac{\left(\frac{y - b}{a} - \mu\right)^2}{2\sigma^2}\right)} \frac{1}{a}
        &= \frac{1}{a\sigma\sqrt{2\pi}} \exp{\left(-\frac{y - (a\mu + b)}{2(a\sigma)^2}\right)}
    \end{align*}
    Which is a distribution $N(a\mu + b, (a\sigma)^2)$.\par
    We can therefore find any normal distribution by transforming the standard normal: suppose that $X \sim N(\mu, \sigma^2)$. Then $\frac{X - \mu}{\sigma}$ is a standard normal random variable.
\end{example}
\subsection{The Standard Normal}
\begin{definition}{Standard normal distribution}
    If $X \sim N(0, 1)$, $X$ has the \underline{standard normal distribution}. Its probability density function is:
    \begin{equation*}
        \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
    \end{equation*}
\end{definition}
\begin{example}[More on the standard normal]
    Define:
    \begin{align*}
        \Phi(x) &= \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} du = P(N(0, 1) \leq x) \\
        \phi(x) &= \Phi'(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
    \end{align*}
    Note now that $\phi$ is even: $\phi(x) = \phi(-x)$.
    %TODO: Continue
\end{example}
The ability to transform any normal distribution to a standard normal is very useful, only one table of probabilities is needed (that of the standard normal) to be able to calculate any normal distribution probability.
\subsection{The Median of a Distribution}
\begin{definition}{Median}
    Let $X$ be a continuous random variable. Then the \underline{median} of $X$, $m$, is the number that satisfies:
    \begin{equation*}
        P(X \leq m) = P(X \geq m)
    \end{equation*}
\end{definition}
\begin{example}[Median of $X$]
    We can transform any normal distribution to the standard normal, so consider only the standard normal. As above calculated, $\Phi(0) = \frac{1}{2}$, and so the median is 0. Transforming back to the general normal distribution, this gives that the median is $\mu$.
\end{example}
\section{Multivariate Density Functions}
\begin{definition}{Multivariate density function}
    Let $X$ be a continuous random variable. Let $(X_i)_{i=1}^n$ be continuous random variables. Then $X$ has \underline{density function} $f$ if:
    \begin{equation*}
        P(X_1 \leq x_1, \cdots, X_n \leq x_n) = \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n} f(y_1, \cdots, y_n) dy_1 \cdots dy_n
    \end{equation*}
\end{definition}
Note that we can further generalise this: for a set $B \subseteq R^n$,
\begin{equation*}
    P((X_1, \cdots, X_n) \in B) = \int_B f(y_1, \cdots, y_n) dy_1 \cdots dy_n
\end{equation*}
\subsection{Independence of Random Variables}
\begin{definition}{Independence}
    If $X_1, \cdots, X_n$ are random variables, we say that they are \underline{independent} if for any $x_1, \cdots, x_n$:
    \begin{equation*}
        P(X_1 \leq x_1, \cdots, X_n \leq x_n) = \prod_{i=1}^{n} P(X_i \leq x_i)
    \end{equation*}
\end{definition}
\begin{theorem}
    Let $X = (X_1, \cdots, X_n)$ have density $f$.\par
    Suppose that $X_i$ are independent with densities $f_i$, $i = 1, \cdots, n$. Then:
    \begin{equation}
        f(x_1, \cdots, x_n) = \prod_i f_i(x_i)
        \label{eqnDensityFuncProduct}
    \end{equation}
    Conversely, suppose that $f$ factorises as in equation~\ref{eqnDensityFuncProduct} for some non-negative functions $f_i$. Then $X_i$ are independent and have densities proportional to the $f_i$.
    \label{thmDensityFactorising}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Suppose that $X_i$ are independent with densities $f_i$}
        \begin{align*}
            P(X_1 \leq x_1, &\cdots, X_n \leq x_n) = \prod_{i = 1}^n P(X_i \leq x_i) \\
            &= \prod_{i = 1}^n \int_{-\infty}^{x_i} f_i(y_i) dy_i \\
            &= \int_{-\infty}^{x_1} \int_{-\infty}^{x_2} \cdots \int_{-\infty}^{x_n} f_1(y_1) \cdots f_1(y_n) dy_1 dy_2 \cdots dy_n
        \end{align*}
        Which, by differentiation $n$ times with respect to $y_i$ for each $i$, gives the result.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $f$ factorises as above.}
        Fix $i$ and defined $B_j = \R \forall j \neq i$. Let $B_i \subseteq \R$.
        \begin{align*}
            P(X_i \in B_i) &= P(X_i \in B_i, X_j \in B_j~\forall j \neq i) \\
            &= \int_{B_i} \int_{B_1} \cdots_{\text{excl. } i} \int_{B_n} f_1(x_1) \cdots f_n(x_n) dx_1 \cdots dx_n \\
            &= \int_{B_i} f_i(x_i) dx_i \prod_{j \neq i} \int_{-\infty}^\infty f_j(x_j) dx_j
        \end{align*}
        However, we know that $f$ is a density. Therefore,
        \begin{equation*}
            \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f_1(x_1) \cdots f_n(x_n) dx_1 \cdots dx_n = 1
        \end{equation*}
        Therefore, we have that:
        \begin{equation*}
            \implies \int_{-\infty}^\infty f_i(x_i) dx_i \left(\prod_{j \neq i} \int_{-\infty}^\infty f_j(x_j) dx_j\right) = 1
        \end{equation*}
        So we can get the required probability:
        \begin{equation*}
            P(X_i \in B_i) = \frac{\int_{B_i} f_i(x_i) dx_i}{\int_{-\infty}^\infty f_i(x_i) dx_i}
        \end{equation*}
        And therefore the density of $X_i$ is:
        \begin{equation*}
            \frac{f_i}{\int_{-\infty}^\infty f_i(x) dx}
        \end{equation*}
        Then consider the probability with this new density:
        \begin{align*}
            P(X_1 \leq x_1)&\cdots P(X_n \leq x_n) = \prod_{i=1}^{n} \frac{\int_{-\infty}^{x_i} f_i(y_i) dy_i}{\int_{-\infty}^\infty f_i(x_i) dx_i} \\
            &= \frac{\int_{y_1 = -\infty}^{x_1} \cdots \int_{y_n = -\infty}^{x_n} f_1(y_1) \cdots f_n(y_n) dy_1 \cdots dy_n}{\int_{y_1 = -\infty}^{\infty} \cdots \int_{y_n = -\infty}^{\infty} f_1(y_1) \cdots f_n(y_n) dy_1 \cdots dy_n} \\
            &= \frac{\int_{y_1 = -\infty}^{x_1} \cdots \int_{y_n = -\infty}^{x_n} f(y_1, \cdots, y_n) dy_1 \cdots dy_n}{\int_{y_1 = -\infty}^{\infty} \cdots \int_{y_n = -\infty}^{\infty} f(y_1, \cdots y_n) dy_1 \cdots dy_n} \\
            &=\frac{P(X_1 \leq x_1, \cdots, X_n \leq x_n)}{1}
        \end{align*}
    \end{proofdirection}
\end{proof}
Therefore, suppose that $X = (X_1, \cdots, X_n)$ has density $f$.
\begin{align*}
    P(X_1 &\leq x) = P(X_1 \leq x, X_2 \leq \infty, \cdots, X_n \leq \infty) \\
    &= \int_{x_1 = -\infty}^x \int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x_1, x_2, \cdots, x_n) dx_1 \cdots dx_n \\
    &= \int_{x_1 = -\infty}^x \left(\int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x_1, \cdots, x_n) dx_2 \cdots dx_n\right)dx_1 \\
\end{align*}
Therefore the density of $X_1$ is:
\begin{equation*}
    f_{X_1}(x) = \int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x, x_2, \cdots, x_n) dx_2 \cdots dx_n
\end{equation*}
by differentiating. This is the \underline{marginal density} of $X_1$.
\subsection{Summing Random Variables}
Consider random variables $X$ and $Y$ with densities $f_X$ and $f_Y$, which are independent. We want to consider $f_{X + Y}$.\par
In the discrete case, we had the convolution:
\begin{equation*}
    P(X + Y = z) = \sum_x P(X = x)P(Y = z - x)
\end{equation*}
Therefore in the continuous case, we can do a similar thing:
\begin{proposition}[Convolutions in the continuous case]
    For random variables $X$ and $Y$ as above defined,
    \begin{equation*}
        f_{X + Y}(z) = \int_{-\infty}^\infty f_X(x) f_Y(z - x) dx
    \end{equation*}
    \label{propSumContRVs}
\end{proposition}
\begin{proof}
    Let the joint density of $X$ and $Y$ be $f_{X, Y}$.
    \begin{align*}
        P(X + Y \leq z) &= \int_{\{x + y \leq z\}} f_{X, Y} (x, y) dx dy \\
        &= \int_{\{x + y \leq z\}} f_X(x) f_Y(y) dx dy \\
        &= \int_{x = -\infty}^\infty \int_{y = -\infty}^{z - x} f_X(x) F_Y(y) dx dy \\
        &= \int_{-\infty}^\infty f_X(x) dx \int_{-\infty}^{z - x} f_Y(y) dy \\
        &= \int_{-\infty}^\infty f_X(x) dx \int_{-\infty}^z f_Y(y - x) dy \\
        &= \int_{-\infty}^z \left(\int_{-\infty}^\infty f_X(x) f_Y(y-x)dx\right)dy
    \end{align*}
    And therefore we have the required result.
\end{proof}
This is important enough that we use a special notation:
\begin{definition}{Convolution}
    Given two independent densities $f$ and $g$, the \underline{convolution} is defined to be:
    \begin{equation}
        f \star g(x) = \int_{-\infty}^\infty f(x - y) g(y) dy
        \label{eqnContConvolution}
    \end{equation}
\end{definition}
\subsection{Conditional Density and Expectation}
\begin{definition}{Conditional density}
    Let $X$ and $Y$ be random variables with joint density $f_{X, Y}$ and marginal densities $f_X$ and $f_Y$. Then the \underline{conditional density} of $X$ given $Y = y$ is:
    \begin{equation*}
        F_{X | Y}(x | y) = \frac{f_{X, Y}(x, y)}{F_Y(y)}
    \end{equation*}
\end{definition}
This is as would be expected given the continuous case.\par
We have also the Law of Total Probability:
\begin{equation*}
    f_X(x) = \int_{-\infty}^\infty f_{X | Y}(x | y) f_Y(y) dy
\end{equation*}
\begin{definition}{Conditional expectation}
    Given $X$ and $Y$ random variables, with densities as above defined, the \underline{conditional expectation} of $X$ given $Y = y$ is:
    \begin{equation*}
        g(y) = \int_{-\infty}^\infty xf_{X | Y} (x | y) dx
    \end{equation*}
    Then applying this function to the random variable $Y$ gives:
    \begin{equation*}
        E[X | Y] = g(Y)
    \end{equation*}
    Which, as in the discrete case, is a random variable depending on $Y$.
\end{definition}
\begin{theorem}
    Let $X$ be a random variable with values in $D \subseteq \R^d$ with density $f$. Let $g$ be a bijection from $D$ to $g(D)$ with a continuous derivative on $D$ and $\det{(g'(x) \neq 0)}~\forall x \in D$.\par
    Then the random variable $Y = g(X)$ has density given by:
    \begin{equation*}
        f_Y(y) = f_X(x) |J|
    \end{equation*}
    where $x = g^{-1}(y)$ and $J$ is the Jacobian of the transform $g$.
\end{theorem}
\end{document}