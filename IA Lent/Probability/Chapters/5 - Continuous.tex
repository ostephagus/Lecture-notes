\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Defining Continuous Probability}
Previously we defined the probability space $\left(\Omega, \sigalg, P\right)$, where $X$ is a random variable $X : \Omega \mapsto \R$, and stipulated that we can define a notion of ordering:
\begin{equation*}
    \forall x \in \R,~\{X \leq x\} = \subsetselect{\omega}{X(\omega) \leq x} \in \sigalg
\end{equation*}
\begin{definition}{Probability distribution function}
    The \underline{probability distribution function} is defined to be:
    \begin{align*}
        F : \R &\mapsto [0, 1] \\
        x &\mapsto P(X \leq x)
    \end{align*}
\end{definition}
\begin{propositions}{
        Suppose that $F$ is a probability distribution function as above defined.
        \label{propsPDFProps}
    }
    \item If $x \leq y$, then $F(x) \leq F(y)$. \label{propPDFIncreasing}
    \item For any real numbers $a < b$, $P(a < X \leq b) = F(b) - F(a)$. \label{propPDFSubtract}
    \item $F$ is right-continuous, and left limits always exist. \label{propPDFContinuity}
    \item $\lim_{y \to x^-} F(y) = P(X < x)$.\label{propPDFStrictLessThan}
    \item $\lim_{x \to \infty} F(x) = 1$ and $\lim_{x \to -\infty} F(x) = 0$. \label{propPDFLimits}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Simply note that $\{X \leq x\} \subseteq \{X \leq y\}$.
        \item \begin{align*}
            P(A < X \leq B) &= P(X \leq b, X > a) \\
            &= P(X \leq b) - P(X \leq b, X \leq a) \\
            &= P(X \leq b) - P(X \leq a) \\
            &= F(b) - F(a)
        \end{align*}
        \item Let $x_n$ be a decreasing sequence converging to $x$ as $n \to \infty$.
            Define $A_n = \{x < X \leq x_n\}$. Note that $A_n$ is a decreasing sequence ($A_{n + 1} \subseteq A_n$).\par
            Note that for a decreasing sequence,
            \begin{equation*}
                P(A_n) \to P\left(\bigcap_{n \in \N} A_n\right)
            \end{equation*}
            and in this case the infinite intersection is the empty set, so $P(A_n) \to 0$.\par
            Therefore $F(x_n) - F(x) \to 0$, and so $F$ is right-continuous.\par
            To show that left limits exist, we can bound them from above (since $F$ is increasing) by their limit point:
            \begin{equation*}
                \lim_{y \to x^-} F(y) \leq F(x)
            \end{equation*}
        \item $\lim_{y \to x^-} F(y) = \lim_{n \to \infty} F(x - \frac{1}{n})$ (we can choose any increasing sequence converging to $x$).\par
            Define also $B_n = \{X \leq x - \frac{1}{n}\}$. $B_n$ is increasing, and so $P(B_n)$ tends to the probability of the union. The probability of the union is exactly equal to $\{X < x\}$.
        \item Proof by the previous case.
    \end{enumerate}
\end{proof}
\begin{definition}{Continuous random variable}
    A random variable $X$ is \underline{continuous} if its probability distribution function is continuous. That is,
    \begin{equation*}
        \lim_{y \to x^-} F(y) = \lim_{y \to x^+} F(x)
    \end{equation*}
\end{definition}
Immediately this tells us that $P(X < x) = P(X \leq x)$, or $P(X = x) = 0$.\par
$F$ may not always be differentiable. However, in this course, we will assume the differentiability of $F$ on the whole of $\R$.
\begin{definition}{Probability density function}
    Given that, for a random variable $X$, the probability distribution function $F$ is differentiable everywhere, the \underline{probability density function} $f$ is defined to be the derivative $F'$.
\end{definition}
\begin{propositions}{
        Consider a random variable $X$, with probability distribution function $F$ and probability density function $f$.
        \label{propsPdensFuncProps}
    }
    \item $f(x) \geq 0$. \label{propPdensFuncNonNegative}
    \item $\int_{-\infty}^{\infty} f(x) dx = 1$ \label{propPdensFuncIntegralOne}
    \item $F(x) = \int_{-\infty}^{x} f(y) dy$. \label{propPdensFuncIntegratePDF}
\end{propositions}
More generally, for any subset of the real numbers $A$,
\begin{equation*}
    P(X \in A) = \int_A f(x) dx
\end{equation*}
\section{Simple Continuous Distributions}
\subsection{Uniform Distribution}
\begin{definition}{Uniform distribution}
    Given a real interval $[a, b]$, the \underline{uniform distribution} on that interval is $U([a, b])$. Its probability density function is: 
    \begin{equation*}
        f(x) =
        \begin{cases}
            \frac{1}{b-a} & x \in [a, b] \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    The probability $P(X \leq x)$ is:
    \begin{equation*}
        P(X \leq x) = \int_{-\infty}^x f(y) dy = \frac{x - a}{b - a}
    \end{equation*}
\end{definition}
\subsection{Exponential Distribution}
\begin{definition}{Exponential distribution}
    Let $\lambda > 0$ be a real number. Then the \underline{exponential distribution} is $\text{Exp}(\lambda)$, with probability density function:
    \begin{equation*}
        f(x) =
        \begin{cases}
            \lambda e^{-\lambda x} & x > 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{proposition}
    Let $T \sim Exp(\lambda)$, let $T_n = \lfloor nT \rfloor$.\par
    Then $T_n$ is a geometric distribution, and
    \begin{equation*}
        \frac{T_n}{n} \to T \text{ as } n \to \infty
    \end{equation*}
    \label{propExpLimitOfGeom}
\end{proposition}
\begin{proof}
    For any $k \in \N$,
    \begin{align*}
        P(T_n \geq k) &= P(\lfloor nT \rfloor \geq k) \\
        &= P(nt \geq k) \\
        &= P(T \geq \frac{k}{n}) \\
        &= e^{-\frac{\lambda k}{n}} \\
        &= \left(e^{-\frac{\lambda}{n}}\right)^k
    \end{align*}
    Note that this is a geometric random variable with success parameter $p_n = 1 - e^{-\frac{\lambda}{n}}$, as required, and this tends to $\frac{\lambda}{n}$ when $n$ is large.\par
    Therefore $\frac{T_n}{n} \to T$ as $n \to \infty$.
\end{proof}
\begin{proposition}[Memoryless property]
    If $T$ si an exponential random variable with parameter $\lambda$,
    \begin{equation*}
        P(T \geq t + s | T \geq s) = P(T \geq t)
    \end{equation*}
\label{propMemoryless}
\end{proposition}
\begin{proof}
    \begin{align*}
        P(T \geq t + s | T \geq s) &= \frac{P(T \geq t + s, T \geq s)}{P(T \geq s)} \\
        &= \frac{P(T \geq t + s)}{P(T \geq s)} \\
        &= \frac{e^{-\lambda(t + s)}}{e^{-\lambda s}} \\
        &= e^{-\lambda t}
    \end{align*}
\end{proof}
\begin{theorem}[Memoryless property unique to exponentials]
    Let $T$ be a positive random variable with a density not identically equal to $0$ or $\infty$. Then $T$ has the memoryless property only if $T$ is an exponential random variable.
    \label{thmExpMemorylessUnique}
\end{theorem}
\begin{proof}
    Set $g(t) = P(T \geq t)$. $T$ has the memoryless property, so $g(t + s) = g(t) g(s)$.\par
    If $t > 0$ and $m \in \N$,
    \begin{equation*}
        g(mt) = \left(g(t)\right)^m
    \end{equation*}
    Then setting $t = 1$ gives $g(m) = g(1)^m$.\par
    Now set $\lambda = -\log(P(T \geq 1))$, so $P(T \geq 1) = e^{-\lambda}$.\par
    Therefore, for any $m \in \N$, $g(m) = e^{-\lambda m}$. Let also $n \in \N$:
    \begin{equation*}
        g(\frac{m}{n})^n = g(m) = e^{-\lambda m}
    \end{equation*}
    And therefore $g(\frac{m}{n}) = e^{-\lambda \frac{m}{n}}$.

    Therefore we have the property that $g(r) = e^{-\lambda r}$ for any $r \in \Q_+$.\par
    Let $t$ be a positive real number. Then let $r,s \in \Q_+$ such that $s < t < r$ and $|s - r| \leq \epsilon$. Since $g$ is a decreasing function,
    \begin{equation*}
        g(r) \leq g(t) \leq g(s) \implies e^{-\lambda r} \leq g(t) \leq e^{-\lambda s}
    \end{equation*}
    Then taking the limit as $\epsilon$ goes to zero gives the required result for positive real values also.
\end{proof}
\section{Expectation and Variance}
\subsection{Expectation}
\begin{definition}{Continuous expectation}
    The \underline{expectation} of a non-negative continuous random variable $X$, with a probability density function $f$, is:
    \begin{equation*}
        E[X] = \int_0^\infty xf(x) dx
    \end{equation*}
\end{definition}
\begin{remarks}
    \item We can define the expectation for random variables that take negative values in the same way as in the discrete case, using $X_+$ and $X_-$. This gives:
        \begin{equation*}
            E[X] = \int_{-\infty}^\infty xf(x) dx
        \end{equation*}
    \item Expectation is still a linear function, i.e.
        \begin{equation*}
            E\left[\sum_{i = 1}^n a_i X_i\right] = \sum_{i = 1}^n a_i E[X_i]
        \end{equation*}
\end{remarks}
\begin{lemma}
    Given $X \geq 0$, the expectation is:
    \begin{equation*}
        E[X] = \int_0^\infty P(X \geq x)
    \end{equation*}
\end{lemma}
\begin{proof}
    \begin{align*}
        E[X] &= \int_0^\infty x f(x) dx \\
        &= \int_0^\infty \left(\int_0^x 1 dy\right)f(x) dx \\
        &= \int_0^\infty \left(\int_0^y f(x) dx\right) dy \\ %TODO: Check.
        &= \int_0^\infty P(X \geq y) dy
    \end{align*}
\end{proof}
\subsection{Variance}
\begin{definition}{Variance}
    Given a continuous random variable $X$, the \underline{variance} is:
    \begin{equation*}
        \Var(X) = E\left[(X - E[X])^2\right] = E\left[X^2\right] - \left(E[X]\right)^2
    \end{equation*}
\end{definition}
\begin{remark}
    Note that this definition needs no modification from the discrete case.
\end{remark}
\begin{example}[Expectation and variance of uniform distribution]
    Let $X \sim U([a, b])$. Then the expectation is:
    \begin{align*}
        E[X] &= \int_{-\infty}^\infty xf(x) dx \\
        &= \int_a^b \frac{x}{b-a} dx \\
        &= \frac{b^2-a^2}{2(b-a)} \\
        &= \frac{a + b}{2}
    \end{align*}
    That is, the expectation of a uniform distribution is its midpoint, as we would expect.\par
    The variance of a uniform distribution is $\frac{1}{12}(b - a)^2$.
\end{example}
\begin{example}[Expectation and variance of exponential distribution]
    Let $X \sim \text{Exp}(\lambda)$. Then its expectation is:
    \begin{align*}
        E[X] &= \int_0^\infty xf(x) \\
        &= \int_0^\infty x \lambda e^{-\lambda x} \\
        &= \frac{1}{\lambda}
    \end{align*}
    As we would expect, given the result of proposition~\ref{propExpLimitOfGeom}.\par
    Note also that its variance is $\frac{1}{\lambda^2}$.
\end{example}
\section{The Normal Distribution}
\begin{definition}{Normal distribution}
    Given real numbers $\mu$ and $\sigma$, with $\sigma > 0$, define the \underline{normal distribution} with parameters $\mu$ and $\sigma$, $N(\mu, \sigma^2)$. Let the probability density function be:
    \begin{equation*}
        f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
    \end{equation*}
\end{definition}
\begin{proposition}
    $f(x)$, as above defined, is a valid density function.
\end{proposition}
\begin{proof}
    We need to show that $I = \int_{-\infty}^\infty f(x) dx = 1$.
    First change variables to $u = \frac{x - \mu}{\sigma}$, so $dx = \sigma du$:
    \begin{align*}
        \int_{-\infty}^\infty \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} &= \int_{-\infty}^\infty \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{u^2}{2}} \sigma du \\
        &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}} du
    \end{align*}
    Then consider $I^2$ and use polar coordinates:
    \begin{align*}
        I^2 &= \frac{1}{2\pi} \int_{x = -\infty}^\infty \int_{y = -\infty}^\infty e^{-\frac{x^2}{2}} e^{-\frac{y^2}{2}} dx dy \\
        &= \frac{1}{2\pi} \int_{x = -\infty}^\infty \int_{y = -\infty}^\infty e^{-\frac{x^2 + y^2}{2}} dx dy \\
        &= \frac{1}{2\pi} \int_{r = 0}^\infty \int_{\phi = 0}^{2\pi} e^{-\frac{r^2}{2}} r dr d\phi \\
        &= \frac{1}{2\pi} \int_{r = 0}^\infty \int_{\phi = 0}^{2\pi} e^{-\frac{r^2}{2}} r dr d\phi \\
        &= \int_{r = 0}^\infty \frac{d}{dx}\left(-e^{-\frac{r^2}{2}}\right) dr \\
        &= 1
    \end{align*}
    Therefore, since $f(x)$ is a positive function, we must have the positive root:
    \begin{equation*}
        I = 1
    \end{equation*}
\end{proof}
\begin{propositions}{
        Let $X \sim N(\mu, \sigma^2)$.
        \label{propsNormalMeanVar}
    }
    \item $E[X] = \mu$ \label{propNormalMean}
    \item $\Var(X) = \sigma^2$ \label{propNormalVar}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Expectation given by:
            \begin{align*}
                E[X] &= \int_{-\infty}^\infty xf(x) dx \\
                &= \int_{-\infty}^\infty x \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx \\
                &= \int_{-\infty}^\infty \frac{x - \mu}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx + \int_{-\infty}^\infty \frac{\mu}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx \\
                &= 0 + \mu
            \end{align*}
        \item Variance given by:
            \begin{align*}
                \Var(X) &= E[(X - \mu)^2] \\
                &= \int_{-\infty}^\infty \frac{(x - \mu)^2}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} dx
            \end{align*}
            Change variables to $u = \frac{x - \mu}{\sigma}$:
            \begin{align*}
                \Var(X) &= \int_{-\infty}^\infty \frac{\sigma^2u^2}{\sqrt{2\pi}} e^{-\frac{u}{2}}du \\
                &= \sigma^2 \int_{-\infty}^\infty \frac{u^2}{\sqrt{2\pi}} e^{-\frac{u}{2}} du
            \end{align*}
            Which is equal to $\sigma^2$ (integration by parts).
    \end{enumerate}
\end{proof}
\subsection{Transforming a Distribution}
\begin{theorem}
    Let $X$ have density $f$. Let $g$ be a continuous function which is strictly monotone and has inverse $g^{-1}$ which is differentiable.\par
    Then $g(X)$ has a density given by:
    \begin{equation*}
        f(g^{-1}(x)) \left|\frac{d(g^{-1}(x))}{dx}\right|
    \end{equation*}
\end{theorem}
\begin{proof}
    \begin{case}{$g$ is strictly increasing.}
        \begin{align*}
            P(g(X) \geq x) &= P(X \leq g^{-1}(x)) \\
            &= F(g^{-1}(x))
        \end{align*} %TODO: Check
        Then denote by $f_{g(X)}$ the density function of $g(X)$.
        \begin{align*}
            f_{g(X)} &= \frac{d}{dx} P(g(X) \leq x) \\
            &= f(g^{-1}(x)) \frac{d(g^{-1}(x))}{dx}
        \end{align*}
        Note also that since $g$ is increasing, the derivative of its inverse (which is also increasing) is positive.
    \end{case}
    \begin{case}{$g$ is strictly decreasing.}
        Proceed as above, but note:
        \begin{equation*}
            P(g(X) \geq x) = -f(g^{-1}(x)) \frac{d(g^{-1}(x))}{dx}
        \end{equation*}
        and that its derivative is negative, but since we use absolute value:
        \begin{equation*}
            f_{g(X)} = f(g^{-1}(x)) \left|\frac{d(g^{-1})}{dx}\right|
        \end{equation*}
    \end{case}
\end{proof}
\begin{example}[Transforming the normal distribution]
    Suppose $X \sim N(\mu, \sigma^2)$. Let $a, b$ be real numbers and $a \neq 0$. Define $g(x) = ax + b$, and $g(X) = Y$. Then we consider the density function of $Y$.\par
    First note that $g^{-1}(x) = \frac{x - b}{a}$, and that its derivative is $\frac{1}{a}$. Then $f_{Y}$ is:
    \begin{align*}
        f_Y(y) &= f_X (g^{-1}(y)) \left|\frac{d(g^{-1}(y))}{dy}\right| \\
        &= \frac{1}{\sigma\sqrt{2\pi}} \exp{\left(-\frac{\left(\frac{y - b}{a} - \mu\right)^2}{2\sigma^2}\right)} \frac{1}{a} \\
        &= \frac{1}{a\sigma\sqrt{2\pi}} \exp{\left(-\frac{y - (a\mu + b)}{2(a\sigma)^2}\right)}
    \end{align*}
    Which is a distribution $N(a\mu + b, (a\sigma)^2)$.\par
    We can therefore find any normal distribution by transforming the standard normal: suppose that $X \sim N(\mu, \sigma^2)$. Then $\frac{X - \mu}{\sigma}$ is a standard normal random variable.
\end{example}
\subsection{The Standard Normal}
\begin{definition}{Standard normal distribution}
    If $X \sim N(0, 1)$, $X$ has the \underline{standard normal distribution}. Its probability density function is:
    \begin{equation*}
        \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
    \end{equation*}
\end{definition}
\begin{example}[More on the standard normal]
    Define:
    \begin{align*}
        \Phi(x) &= \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}} du = P(N(0, 1) \leq x) \\
        \phi(x) &= \Phi'(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
    \end{align*}
    Note now that $\phi$ is even: $\phi(x) = \phi(-x)$.
    %TODO: Continue
\end{example}
The ability to transform any normal distribution to a standard normal is very useful, only one table of probabilities is needed (that of the standard normal) to be able to calculate any normal distribution probability.
\subsection{The Median of a Distribution}
\begin{definition}{Median}
    Let $X$ be a continuous random variable. Then the \underline{median} of $X$, $m$, is the number that satisfies:
    \begin{equation*}
        P(X \leq m) = P(X \geq m)
    \end{equation*}
\end{definition}
\begin{example}[Median of $X$]
    We can transform any normal distribution to the standard normal, so consider only the standard normal. As above calculated, $\Phi(0) = \frac{1}{2}$, and so the median is 0. Transforming back to the general normal distribution, this gives that the median is $\mu$.
\end{example}
\section{Multivariate Density Functions}
\begin{definition}{Multivariate density function}
    Let $X$ be a continuous random variable. Let $(X_i)_{i=1}^n$ be continuous random variables. Then $X$ has \underline{density function} $f$ if:
    \begin{equation*}
        P(X_1 \leq x_1, \cdots, X_n \leq x_n) = \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_n} f(y_1, \cdots, y_n) dy_1 \cdots dy_n
    \end{equation*}
\end{definition}
Note that we can further generalise this: for a set $B \subseteq R^n$,
\begin{equation*}
    P((X_1, \cdots, X_n) \in B) = \int_B f(y_1, \cdots, y_n) dy_1 \cdots dy_n
\end{equation*}
\subsection{Independence of Random Variables}
\begin{definition}{Independence}
    If $X_1, \cdots, X_n$ are random variables, we say that they are \underline{independent} if for any $x_1, \cdots, x_n$:
    \begin{equation*}
        P(X_1 \leq x_1, \cdots, X_n \leq x_n) = \prod_{i=1}^{n} P(X_i \leq x_i)
    \end{equation*}
\end{definition}
\begin{theorem}
    Let $X = (X_1, \cdots, X_n)$ have density $f$.\par
    Suppose that $X_i$ are independent with densities $f_i$, $i = 1, \cdots, n$. Then:
    \begin{equation}
        f(x_1, \cdots, x_n) = \prod_i f_i(x_i)
        \label{eqnDensityFuncProduct}
    \end{equation}
    Conversely, suppose that $f$ factorises as in equation~\ref{eqnDensityFuncProduct} for some non-negative functions $f_i$. Then $X_i$ are independent and have densities proportional to the $f_i$.
    \label{thmDensityFactorising}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Suppose that $X_i$ are independent with densities $f_i$}
        \begin{align*}
            P(X_1 \leq x_1, &\cdots, X_n \leq x_n) = \prod_{i = 1}^n P(X_i \leq x_i) \\
            &= \prod_{i = 1}^n \int_{-\infty}^{x_i} f_i(y_i) dy_i \\
            &= \int_{-\infty}^{x_1} \int_{-\infty}^{x_2} \cdots \int_{-\infty}^{x_n} f_1(y_1) \cdots f_1(y_n) dy_1 dy_2 \cdots dy_n
        \end{align*}
        Which, by differentiation $n$ times with respect to $y_i$ for each $i$, gives the result.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $f$ factorises as above.}
        Fix $i$ and defined $B_j = \R \forall j \neq i$. Let $B_i \subseteq \R$.
        \begin{align*}
            P(X_i \in B_i) &= P(X_i \in B_i, X_j \in B_j~\forall j \neq i) \\
            &= \int_{B_i} \int_{B_1} \cdots_{\text{excl. } i} \int_{B_n} f_1(x_1) \cdots f_n(x_n) dx_1 \cdots dx_n \\
            &= \int_{B_i} f_i(x_i) dx_i \prod_{j \neq i} \int_{-\infty}^\infty f_j(x_j) dx_j
        \end{align*}
        However, we know that $f$ is a density. Therefore,
        \begin{equation*}
            \int_{-\infty}^\infty \cdots \int_{-\infty}^\infty f_1(x_1) \cdots f_n(x_n) dx_1 \cdots dx_n = 1
        \end{equation*}
        Therefore, we have that:
        \begin{equation*}
            \implies \int_{-\infty}^\infty f_i(x_i) dx_i \left(\prod_{j \neq i} \int_{-\infty}^\infty f_j(x_j) dx_j\right) = 1
        \end{equation*}
        So we can get the required probability:
        \begin{equation*}
            P(X_i \in B_i) = \frac{\int_{B_i} f_i(x_i) dx_i}{\int_{-\infty}^\infty f_i(x_i) dx_i}
        \end{equation*}
        And therefore the density of $X_i$ is:
        \begin{equation*}
            \frac{f_i}{\int_{-\infty}^\infty f_i(x) dx}
        \end{equation*}
        Then consider the probability with this new density:
        \begin{align*}
            P(X_1 \leq x_1)&\cdots P(X_n \leq x_n) = \prod_{i=1}^{n} \frac{\int_{-\infty}^{x_i} f_i(y_i) dy_i}{\int_{-\infty}^\infty f_i(x_i) dx_i} \\
            &= \frac{\int_{y_1 = -\infty}^{x_1} \cdots \int_{y_n = -\infty}^{x_n} f_1(y_1) \cdots f_n(y_n) dy_1 \cdots dy_n}{\int_{y_1 = -\infty}^{\infty} \cdots \int_{y_n = -\infty}^{\infty} f_1(y_1) \cdots f_n(y_n) dy_1 \cdots dy_n} \\
            &= \frac{\int_{y_1 = -\infty}^{x_1} \cdots \int_{y_n = -\infty}^{x_n} f(y_1, \cdots, y_n) dy_1 \cdots dy_n}{\int_{y_1 = -\infty}^{\infty} \cdots \int_{y_n = -\infty}^{\infty} f(y_1, \cdots y_n) dy_1 \cdots dy_n} \\
            &=\frac{P(X_1 \leq x_1, \cdots, X_n \leq x_n)}{1}
        \end{align*}
    \end{proofdirection}
\end{proof}
Therefore, suppose that $X = (X_1, \cdots, X_n)$ has density $f$.
\begin{align*}
    P(X_1 &\leq x) = P(X_1 \leq x, X_2 \leq \infty, \cdots, X_n \leq \infty) \\
    &= \int_{x_1 = -\infty}^x \int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x_1, x_2, \cdots, x_n) dx_1 \cdots dx_n \\
    &= \int_{x_1 = -\infty}^x \left(\int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x_1, \cdots, x_n) dx_2 \cdots dx_n\right)dx_1 \\
\end{align*}
Therefore the density of $X_1$ is:
\begin{equation*}
    f_{X_1}(x) = \int_{x_2 = -\infty}^\infty \cdots \int_{x_n = -\infty}^\infty f(x, x_2, \cdots, x_n) dx_2 \cdots dx_n
\end{equation*}
by differentiating. This is the \underline{marginal density} of $X_1$.
\subsection{Summing Random Variables}
Consider random variables $X$ and $Y$ with densities $f_X$ and $f_Y$, which are independent. We want to consider $f_{X + Y}$.\par
In the discrete case, we had the convolution:
\begin{equation*}
    P(X + Y = z) = \sum_x P(X = x)P(Y = z - x)
\end{equation*}
Therefore in the continuous case, we can do a similar thing:
\begin{proposition}[Convolutions in the continuous case]
    For random variables $X$ and $Y$ as above defined,
    \begin{equation*}
        f_{X + Y}(z) = \int_{-\infty}^\infty f_X(x) f_Y(z - x) dx
    \end{equation*}
    \label{propSumContRVs}
\end{proposition}
\begin{proof}
    Let the joint density of $X$ and $Y$ be $f_{X, Y}$.
    \begin{align*}
        P(X + Y \leq z) &= \int_{\{x + y \leq z\}} f_{X, Y} (x, y) dx dy \\
        &= \int_{\{x + y \leq z\}} f_X(x) f_Y(y) dx dy \\
        &= \int_{x = -\infty}^\infty \int_{y = -\infty}^{z - x} f_X(x) F_Y(y) dx dy \\
        &= \int_{-\infty}^\infty f_X(x) dx \int_{-\infty}^{z - x} f_Y(y) dy \\
        &= \int_{-\infty}^\infty f_X(x) dx \int_{-\infty}^z f_Y(y - x) dy \\
        &= \int_{-\infty}^z \left(\int_{-\infty}^\infty f_X(x) f_Y(y-x)dx\right)dy
    \end{align*}
    And therefore we have the required result.
\end{proof}
This is important enough that we use a special notation:
\begin{definition}{Convolution}
    Given two independent densities $f$ and $g$, the \underline{convolution} is defined to be:
    \begin{equation}
        f \star g(x) = \int_{-\infty}^\infty f(x - y) g(y) dy
        \label{eqnContConvolution}
    \end{equation}
\end{definition}
\subsection{Conditional Density and Expectation}
\begin{definition}{Conditional density}
    Let $X$ and $Y$ be random variables with joint density $f_{X, Y}$ and marginal densities $f_X$ and $f_Y$. Then the \underline{conditional density} of $X$ given $Y = y$ is:
    \begin{equation*}
        F_{X | Y}(x | y) = \frac{f_{X, Y}(x, y)}{F_Y(y)}
    \end{equation*}
\end{definition}
This is as would be expected given the continuous case.\par
We have also the Law of Total Probability:
\begin{equation*}
    f_X(x) = \int_{-\infty}^\infty f_{X | Y}(x | y) f_Y(y) dy
\end{equation*}
\begin{definition}{Conditional expectation}
    Given $X$ and $Y$ random variables, with densities as above defined, the \underline{conditional expectation} of $X$ given $Y = y$ is:
    \begin{equation*}
        g(y) = \int_{-\infty}^\infty xf_{X | Y} (x | y) dx
    \end{equation*}
    Then applying this function to the random variable $Y$ gives:
    \begin{equation*}
        E[X | Y] = g(Y)
    \end{equation*}
    Which, as in the discrete case, is a random variable depending on $Y$.
\end{definition}
\begin{theorem}
    Let $X$ be a random variable with values in $D \subseteq \R^d$ with density $f$. Let $g$ be a bijection from $D$ to $g(D)$ with a continuous derivative on $D$ and $\det{(g'(x) \neq 0)}~\forall x \in D$.\par
    Then the random variable $Y = g(X)$ has density given by:
    \begin{equation*}
        f_Y(y) = f_X(x) |J|
    \end{equation*}
    where $x = g^{-1}(y)$ and $J$ is the Jacobian of the transform $g$.
    \label{thmTransformRV}
\end{theorem}
This theorem will be used without proof.
\begin{example}
    Let $X$ and $Y$ be independent random variables with standard normal distribution.\par
    Consider now a change of variables from $(X, Y)$ to $(R, \Theta)$:
    \begin{align*}
        f_{R, \Theta}(r, \theta) &= f_{X, Y}(r \cos(\theta), r\sin(\theta)) |J| \\
        &= f_{X, Y}(r \cos(\theta), r\sin(\theta)) r \\
        &= rf_X(r\cos(\theta)) f_Y(r \cos(\theta)) \\
        &= r\frac{1}{\sqrt{2\pi}} e^{-\frac{r^2 \cos^2(\theta)}{2}} \frac{1}{\sqrt{2\pi}} e^{-\frac{r^2 \sin^2(\theta)}{2}} \\
        &= \frac{1}{2\pi} r e^{-\frac{r^2}{2}}
    \end{align*}
    Which, notably, is independent of $\theta$.\par
    We have that:
    \begin{equation*}
        f_{R, \Theta} = \left(\frac{1}{2\pi}\right) \times \left(re^{-\frac{r^2}{2}}\right)
    \end{equation*}
    So the density function factorises into a function of $r$, the second bracket, and a ``function'' of $\theta$ (first bracket, which is independent of $\theta$).

    Therefore, by theorem~\ref{thmDensityFactorising}, we can extract the individual densities:
    \begin{align*}
        f_R(r) &= re^{-\frac{r^2}{2}} \\
        F_\Theta(\theta) &= \frac{1}{2\pi}
    \end{align*}
\end{example}
\subsection{Order Statistics}
Suppose $X_1, \cdots, X_n$ are identically distributed independent random variables. Suppose they have combined distribution function $F$ and combined density $f$.

We consider putting them in increasing order:
\begin{equation*}
    X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}
\end{equation*}
And set $Y_i = X_{(i)}$. Here $Y_i$ are called the order statistics of the random sample. We want to calculate the density of the $Y_i$:
\begin{align*}
    P(Y_1 \leq x) &= P(\min_{i \in \{1, \cdots, n\}} X_i \leq x) \\
    &= 1 - P(\min_{i \in \{1, \cdots, n\}} X_i > x) \\ 
    &= 1 - P(X_1 > x)^n \text{ since all must be greater than } x \\
    &= 1 - (1 - F(X))^n
\end{align*}
So the density is:
\begin{equation*}
    f_{Y_1}(x) = n(1 - F(x))^{n-1} f(x)
\end{equation*}
by differentiation.

We can also consider $Y_n$:
\begin{align*}
    P(Y_n \leq x) &= P(\max_{i \in \{1, \cdots, n\}} X_i \leq x) \\
    &= P(X_1 \leq x)^n \\
    &= (F(x))^n
\end{align*}
So by differentiation:
\begin{equation*}
    f_{Y_n}(x) = n(F(x))^{n-1} f(x)
\end{equation*}
Then we consider the combined density of all the $Y_i$.
\begin{align*}
    &P(Y_1 \leq x_1, \cdots, Y_n \leq x_n) \\
    &= n! P(X_1 \leq x_1, \cdots, X_n \leq x_n, X_1 < X_2 < \cdots < X_n) \\
    &= n! \int \cdots \int I_{u_1 \leq x_1, \cdots, u_n \leq x_n, u_1 \leq \cdots \leq u_n} \times \\
    & f(u_1) f(u_2) \cdots f(u_n) du_1 \cdots du_n \\
    &= n! \int_{u_1 = -\infty}^{x_1} \int_{u_2 = u_1}^{x_2} \cdots \int_{u_n = u_{n - 1}}^{x_n} f(u_1) \cdots f(u_n) du_1 \cdots du_n
\end{align*}
So by differentiating with respect to every $x_i$:
\begin{equation*}
    f_{Y_1, \cdots, Y_n}(x_1, \cdots, x_n) =
    \begin{cases}
        n! f(x_1) f(x_2) \cdots f(x_n) & \text{if } x_i \text{ are in order} \\
        0 & \text{otherwise}
    \end{cases}
\end{equation*}
\begin{example}
    Let $X \sim \text{Exp}(\lambda)$, and let $Y \sim \text{Exp}(\mu)$. Let them be independent.

    Let $Z = \min\{X, Y\}$.
    \begin{align*}
        P(Z \leq z) = P(\min\{X, Y\} \leq z) \\
        &= 1 - P(\min\{X, Y\} > z) \\
        &= 1 - e^{-\lambda z} e^{-\mu z} \\
        &= 1 - e^{-(\lambda + \mu) z}
    \end{align*}
    So $Z \sim \text{Exp}(\lambda + \mu)$. Applying this logic inductively, if $X_i$ are independent exponential random variables with parameters $\lambda_i$,
    \begin{equation*}
        \min\{X_1, \cdots, X_n\} \sim \text{Exp}\left(\sum_{i = 1}^n \lambda_i\right)
    \end{equation*}
\end{example}
\begin{example}
    Let $X_i, i \in \{1, \cdots, n\}$ be independent and identically distributed exponential random variables with parameter $\lambda$. Let $Y_i$ be their order statistics. Set $Z_1 = Y_1$, $Z_i = Y_i - Y_{i-1}, i \geq 2$. Then consider the joint density of the $Z_i$.

    Define the vectors $\vec{Z}$ and $\vec{Y}$ which are related by the matrix $A$:
    \begin{equation*}
        \begin{pmatrix}Z_1 \\ \vdots \\ Z_n\end{pmatrix} = A \begin{pmatrix}Y_1 \\ \vdots \\ Y_n\end{pmatrix}
    \end{equation*}
    where $A$ is:
    \begin{equation*}
        A = 
        \begin{pmatrix}
            1&0&0&\cdots&0 \\
            -1&1&0&\cdots&0 \\
            0&-1&1& & \vdots \\
            \vdots& &\ddots&\ddots&0 \\
            0 &\cdots&0&-1&1
        \end{pmatrix}
    \end{equation*}
    and has determinant 1.
    \begin{align*}
        f_{Z_1, \cdots, Z_n}(z_1, \cdots, z_n) &= f_{Y_1, \cdots, Y_n}(y_1, \cdots, y_n) |J| \\
        &= n! f(y_1) \cdots f(y_n) \\
        &= n! \lambda e^{-\lambda y_1} \cdots \lambda e^{-\lambda y_n} \\
        &= \prod_{i = 1}^n (n - i + 1) \lambda e^{-\lambda(n - i + 1)z_i}
    \end{align*}
    So $Z_i$ are independent with $Z_i \sim \text{Exp}(\lambda(n - i + 1))$.
\end{example}
\section{Moment Generating Functions}
\begin{definition}{Moment generating function}
    Let $X$ be a random variable with density $f$. Then the \\\underline{moment generating function} is defined to be:
    \begin{equation*}
        m(\theta) = E[e^{\theta X}] = \int_{-\infty}^\infty e^{\theta x} f(x) dx
    \end{equation*}
    whenever this integral is finite. $m(0) = 1$.
\end{definition}
The following theorems will be used without proof:
\begin{theorem}
    The moment generating function uniquely determines the distribution of a random variable provided it is defined for an open interval of values of $\theta$.
    \label{thmMGFDetermines}
\end{theorem}
\begin{theorem}
    Suppose the moment generating function is defined for an open interval of values of $\theta$. Then we can get the moments of $\theta$:
    \begin{equation*}
        E[X^r] = m^{(r)}(0)
    \end{equation*}
    that is, the $r$th derivative evaluated at 0.
\end{theorem}
\subsection{Gamma Distribution}
\begin{definition}{Gamma distribution}
    A random variable $X$ has the \underline{gamma distribution} with parameter $n \in \N$ if its density function is:
    \begin{equation*}
        f(x) = \frac{e^{-\lambda x} \lambda^n x^{n-1}}{(n-1)!}, x > 0
    \end{equation*}
\end{definition}
\begin{proposition}
    The gamma function, as above defined, has a valid density.
    \label{propGammaDensityValid}
\end{proposition}
\begin{proof}
    Define:
    \begin{align*}
        I_n &= \int_0^\infty f(x) dx \\
        &= \int_0^\infty \frac{\left(\lambda e^{-\lambda x}\right)\lambda^{x-1} x^{n-1}}{(n-1)!} \\
        &= \int_0^\infty \frac{e^{-\lambda x}\lambda^{n-1} x^{n-2}}{(n-2)!} \text{ by integration by parts} \\
        &= I_{n-1} = \cdots = I_1 \\
        &= \int_0^\infty \lambda e^{-\lambda x} dx \\
        &= 1
    \end{align*}
\end{proof}
If $X$ has density $f$, $X \sim \Gamma(n, \lambda)$, we can find the moment generating function:
\begin{align*}
    m(\theta) &= E[e^{\theta X}] \\
    &= \int_0^\infty e^{\theta x} e^{-\lambda x} \frac{\lambda^n x^{n-1}}{(n-1)!} \\
    &= \int_0^\infty e^{-(\lambda - \theta)x} \frac{(\lambda - \theta)^n x^{n-1}}{(n-1)!} dx \frac{\lambda^n}{(\lambda - \theta)^n} \\
    &= \left(\frac{\lambda}{\lambda - \theta}\right)^n
\end{align*}
\begin{proposition}
    If $X_1, \cdots, X_n$ are independent, then:
    \begin{equation*}
        m(\theta) = E[e^{\theta (X_1 + \cdots + X_n)}] = \prod_{i=1}^n E[e^{\theta X_i}]
    \end{equation*}
    \label{propMGFFactorise}
\end{proposition}
Then we can consider two independent gamma distributions: \\$X \sim \Gamma(n, \lambda)$ and $Y \sim \Gamma(m, \lambda)$. Then the moment generating function of $X+Y$ is:
\begin{align*}
    E[e^{\theta X}] E[e^{\theta Y}] &= \left(\frac{\lambda}{\lambda - \theta}\right)^n \left(\frac{\lambda}{\lambda - \theta}\right)^m \\
    &= \left(\frac{\lambda}{\lambda - \theta}\right)^{n+m}
\end{align*}
And therefore $X + Y \sim \Gamma(n+m, \lambda)$.

Then we can consider a sum of identically distributed independent exponential random variables. Let $X_i \sim \text{Exp}(\lambda), i \in \{1, \cdots, n\}$. Note also that $\Gamma(1, \lambda)$ is the same as the exponential distribution. Therefore applying the above result for two gamma distributions inductively:
\begin{equation*}
    \sum_{i = 1}^n X_i \sim \Gamma(n, \lambda)
\end{equation*}
We can also define: $\Gamma(\alpha, \lambda)$ for any $\alpha \in \R^+$. The only problem so far with our definition is that we have a factorial in the denominator. We replace the $(n-1)!$ with:
\begin{equation*}
    \Gamma(\alpha) = \int_0^\infty e^{-x} x^{\alpha - 1}
\end{equation*}
Then the density function $\Gamma(\alpha, \lambda)$ is:
\begin{equation*}
    f(x) = \frac{e^{-\lambda x}\lambda^\alpha x^{\alpha - 1}}{\Gamma(\alpha)}
\end{equation*}
\subsection{The Normal Distribution}
If $X \sim N(\mu, \sigma^2)$, then $X$ has density:
\begin{equation*}
    f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\end{equation*}
Then $m(\theta) = E[e^{\theta X}]$ is given by:
\begin{equation*}
    \int_{-\infty}^\infty e^{\theta x} \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) dx
\end{equation*}
Then consider the exponent:
\begin{align*}
    &\frac{2\sigma^2\theta x}{2\sigma^2} - \frac{x^2}{2\sigma^2} + \frac{2x\mu}{2\sigma^2} - \frac{\mu^2}{2\sigma^2} \\
    &= \frac{1}{2\sigma^2}\left(-x^2 + 2x(\theta \sigma^2 + \mu) - \mu^2 + (\theta \sigma^2 + \mu)^2 - (\theta \sigma^2 + \mu)^2\right) \\
    &= \frac{1}{2\sigma^2}\left(-(x - (\mu + \theta \sigma^2))^2 - \mu^2 + \theta^2 \sigma^4 + 2\theta \sigma^2 \mu + \mu^2\right) \\
    &= -\frac{1}{2\sigma^2}(x - (\mu + \theta \sigma^2))^2 + \frac{\theta^2 \sigma^2}{2} + \theta \mu
\end{align*}
Then substituting this back into the original formula:
\begin{align*}
    m(\theta) &= \int_{-\infty}^\infty \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x - (\mu + \theta \sigma^2))^2}{2\sigma^2}\right)dx \\
    &\times \exp\left(\frac{\theta^2 \sigma^2}{2} + \theta \mu\right) \\
    &= \exp\left(\frac{\theta^2 \sigma^2}{2} + \theta \mu\right) \text{ by recognising normal dist.}
\end{align*}
Now we have a moment generating function, we can consider the sum of two independent normal distributions. Consider $X \sim N(\mu, \sigma^2)$ and $Y \sim N(\nu, \tau^2)$.
\begin{align*}
    E[e^{\theta(X + Y)}] &= E[e^{\theta X}] E[e^{\theta Y}] \\
    &= \exp\left(\theta \mu + \frac{\theta^2 \sigma^2}{2}\right) \exp\left(\theta \nu + \frac{\theta^2 \tau^2}{2}\right) \\
    &= \exp\left(\theta (\mu + \nu) + \frac{\theta^2 (\sigma^2 + \tau^2)}{2}\right)
\end{align*}
So $X + Y \sim N(\mu + \nu, \sigma^2 + \tau^2)$.
\subsection{Cauchy Distribution}
Define the \underline{Cauchy Distribution} with density:
\begin{equation*}
    f(x) = \frac{1}{\pi (1 + x^2)}
\end{equation*}
Then the moment generating function is given by:
\begin{equation*}
    m(\theta) = \int_{-\infty}^\infty \frac{e^{\theta x}}{\pi(1 + x^2)}dx
\end{equation*}
However, this is infinite for all non-zero $\theta$. Furthermore, $X, 2X, \cdots$ all have the same moment generating function, but they cannot have the same distribution. Therefore, this provides a counterexample to theorem~\ref{thmMGFDetermines} in the case where $m(\theta)$ is not finite on an open interval of values of $\theta$.
\subsection{Multivariate Moment Generating Functions} %TODO: Figure out what needs to be vector bold.
\begin{definition}{Multivariate moment generating function}
    Let $X = (X_1, X_2, \cdots, X_n) \in \R^n$ be a random variable. Then the \underline{moment generating function} of $X$ is defined to be:
    \begin{align*}
        m(\theta) &= E[e^{\theta^T X}] = E[e^{\sum_{i=1}^n \theta_i X_i}] \\
        \theta &= (\theta_1, \cdots, \theta_n)^T
    \end{align*}
\end{definition}
\begin{theorem}
    If the moment generating function is finite for an open set of values of $\theta$, then it uniquely determines the distribution.

    In this case,
    \begin{equation*}
        \frac{\partial^r m}{\partial \theta_i^r} \vert_{\theta = 0} = E[X_i^r]
    \end{equation*}
    and also:
    \begin{equation*}
        \frac{\partial^{r + s}m}{\partial x_i^r \partial x_j^s} \vert_{\theta = 0} = E[X_i^r X_j^s]
    \end{equation*}
    \label{thmMGFMultiVarDetermines}
\end{theorem}
\begin{proposition}
    The moment generating function factorises:
    \begin{equation*}
        m(\theta) = \prod_{i = 1}^n E[e^{\theta_i X_i}]
    \end{equation*}
    if and only if the $X_i$ are independent.
    \label{propMGFMultiVarFactorise}
\end{proposition}
\subsection{Multidimensional Normal Random Variables}
Recall that a random variable in $\R$, $X$, is called Gaussian or Normal in $\R$ if it can be written as $X = \mu + \sigma Z$, where $Z \sim N(0, 1)$.
\begin{definition}{Gaussian Vector}
    Let $\vec{X} = (X_1, \cdots, X_n)^T$ with values in $R^n$. $\vec{X}$ is a \underline{Gaussian Vector} if for all $\vec{u} \in \R^n$:
    \begin{equation*}
        \vec{u}^T \vec{X} = \sum_{i=1}^n u_i X_i \text{ is a Gaussian random variable in } \R.
    \end{equation*}
\end{definition}
\begin{proposition}
    Let $A$ be an $m \times n$ matrix and $\vec{b} \in \R^m$. Then let $\vec{X} = (X_1, \cdots, X_n)^T$ be a Gaussian vector. Then $A\vec{X} + \vec{b}$ is a Gaussian vector in $R^m$.
    \label{propGaussVecLinearMap}
\end{proposition}
\begin{proof}
    Let $\vec{u} = (u_1, \cdots, u_m)^T$. Then:
    \begin{equation*}
        \vec{u}^T (AX + \vec{b}) = (\vec{u}^T A)\vec{X} + \vec{u}^T \vec{b}
    \end{equation*}
    Then set $\vec{v} = A^T \vec{u}$.
    \begin{equation*}
        \vec{u}^T(A\vec{X} + \vec{b}) = \vec{v}^T \vec{X} + \sum_{i = 1}^m u_i b_i
    \end{equation*}
    So, since $\vec{X}$ is Gaussian, so is $\vec{v}^T \vec{X}$ and so $A\vec{X} + \vec{b}$ must also be Gaussian.
\end{proof}
Let $\vec{X} = (X_1, \cdots X_n)$ in $\R^n$ be Gaussian. Set $\vec{\mu} = E[\vec{X}]$, which is a vector defined such that $\mu_i = E[X_i]$. Define also the variance of the vector to be $E[(\vec{X} - \vec{\mu})(\vec{X} - \vec{\mu})^T]$. This gives an $n \times n$ matrix, where the entry $(Var(X))_{i, j} = \Cov(X_i, X_j)$. Note that the elements on the diagonal are therefore $\Var(X_i)$, and that this matrix is symmetric.
\begin{equation*}
    \begin{pmatrix}
        \Var(X_1) & \Cov(X_1, X_2) & \Cov(X_1, X_3) & \cdots & \Cov(X_1, X_n) \\
        \Cov(X_1, X_2) & \Var(X_2) & \Cov(X_2, X_3) & \cdots & \Cov(X_2, X_n) \\
        \Cov(X_1, X_3) & \Cov(X_2, X_3) & \Var(X_3) & & \vdots \\
        \vdots & \vdots & & \ddots & \Cov(X_{n-1}, X_n) \\
        \Cov(X_1, X_n) & \Cov(X_2, X_n) & \cdots & & \Var(X_n) \\
    \end{pmatrix}
\end{equation*}
\begin{align*}
    \Var(\vec{u}^T \vec{X}) &= \Var\left(\sum_{i=1}^n u_i X_i\right) \\
    &= \sum_{i, j=1}^n u_i \Cov(X_i, X_j) x_j \\
    &= \vec{u}^T V \vec{u}
\end{align*}
\begin{proposition}
    $V = \Var(X)$ is a non-negative definite matrix. That is, for any vector $\vec{u}$, $\vec{u}^T V \vec{u} \geq 0$.
    \label{propVarNonNegDefinite}
\end{proposition}
\begin{proof}
    Simply note that variance is always non-negative, so:
    \begin{align*}
        \vec{u}^T V \vec{u} = \Var(\vec{u}^T \vec{X}) \geq 0
    \end{align*}
\end{proof}
We can then consider the Moment Generating Function for $\vec{X}$.

\begin{align*}
    m(\vec{\lambda}) &= E[e^{\vec{\lambda}^T \vec{X}}] \\
    \vec{\lambda}^T \vec{X} &\sim N(\vec{\lambda}^T \vec{\mu}, \vec{\lambda}^T V \vec{\lambda}) \\
    \therefore m(\vec{\lambda}) &= \text{exp}\left({\vec{\lambda}^T \vec{\mu} + \frac{\vec{\lambda^T} V \vec{\lambda}}{2}}\right)
\end{align*}
Therefore, since the moment generating function uniquely characterises the distribution, we have that the Gaussian vector must be parameterised fully by its mean vector, $\vec{\mu}$, and its variance matrix $V$.

So far, we have not seen an example of a Gaussian vector. We would like to construct a Gaussian vector that is similar to the standard normal, and then we would like to transform it (as we do with the standard normal) to general Gaussian vectors.

\begin{proposition}
    Let $Z_1, \cdots, Z_n$ be independent standard normal random variables. Then $\vec{Z} = (Z_1, \cdots, Z_n)^T$ is a Gaussian vector.
    \label{propStdVectorIsGaussian}
\end{proposition}
\begin{proof}
    For any constant vector $\vec{u}$, the moment generating function of $\vec{u}^T \vec{Z}$ is:
    \begin{align*}
        m(\vec{\lambda}) &= E\left[e^{\vec{\lambda} \vec{u}^T \vec{Z}}\right] \\
        &= E\left[\text{exp}\left(\vec{\lambda} \sum_{i = 1}^n u_i Z_i\right)\right] \\
        &= \prod_{i=1}^{n} E[e^{\vec{\lambda} u_i Z_i}] \\
        &= \prod_{i = 1}^n \text{exp}\left({\frac{|\vec{\lambda}|^2 u_i^2}{2}}\right) \\
        &= \text{exp}\left({\frac{|\vec{\lambda}|^2 |\vec{u}|^2}{2}}\right)
    \end{align*}
    And therefore $\vec{u}^T \vec{Z} \sim N(0, |\vec{u}|^2)$.

    Then the variance of $\vec{Z}$ is the identity matrix (since the variables $Z_i$ are independent, and all have variance 1), and so $\vec{Z} \sim N(\vec{0}, I)$ where $I$ is the $n \times n$ identity matrix.
\end{proof}
Now we have shown that a vector of standard normal random variables is a Gaussian vector. We would like to construct a Gaussian vector with mean $\vec{\mu}$ and variance matrix $V$.

First, we need to define the square root of a matrix to be able to consider the standard deviation matrix rather than the variance matrix.
\begin{definition}{Square root of a Matrix}
    Given a non-negative definite real symmetric matrix $V = U^T D U$ where $U$ is an orthogonal matrix and $D$ is a diagonal matrix with all positive elements.

    Then the \underline{square root} of $V$ is:
    \begin{equation*}
        \sqrt{V} = U^T \sqrt{D} U
    \end{equation*}
    Where $\sqrt{D}$ is the matrix formed by taking the positive square root of each element of $D$.
\end{definition}
\begin{theorem}
    Let $\vec{\mu} \in \R^n$ and let $V$ be a non-negative definite matrix.

    Let $\vec{Z}$ be a vector of independent standard normal random variables.

    Then the vector $\vec{X} = \vec{\mu} + \sqrt{V} \vec{Z}$ is a Gaussian vector with mean $\vec{\mu}$ and variance $V$.
    \label{thmGeneralGaussVec}
\end{theorem}
\begin{theorem}
    We model the proof on the case of 1 dimension, where we defined $X = \mu + \sigma Z$. Therefore, we define $\sigma = \sqrt{V}$ and consider $\vec{X} = \vec{\mu} + \sigma \vec{Z}$.

    $\vec{X}$ must be a Gaussian random variable since it is a linear transformation of the Gaussian vector $\vec{Z}$, which is a Gaussian vector by proposition~\ref{propGaussVecLinearMap}.

    The mean is simply $\vec{\mu}$, since the mean of $\vec{Z}$ is $\vec{0}$. The variance is:
    \begin{align*}
        \Var(\vec{X}) &= E[(\vec{X} - \vec{\mu})(\vec{X} - \vec{\mu})^T] \\
        &= E[\sigma \vec{Z} \vec{Z}^T \sigma^T] \\
        &= \sigma E[\vec{Z} \vec{Z}^T] \sigma^T \\
        &= \sigma I \sigma^T \\
        &= \sigma \sigma = V
    \end{align*}
\end{theorem}
Then we consider the density of $\vec{X}$. We have that in the one-dimensional case,
\begin{equation*}
    f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \text{exp}\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\end{equation*}
\begin{theorem}
    For an n-dimensional Gaussian vector with mean $\vec{\mu}$ and variance matrix $V$, the density function is:
    \begin{equation*}
        f_{\vec{X}}(\vec{x}) = \frac{1}{\sqrt{(2\pi)^n} \det{V}} \text{exp} \left(-\frac{(\vec{x} - \vec{\mu})^T V^{-1} (\vec{x} - \vec{\mu})}{2}\right)
    \end{equation*}
\end{theorem}
\begin{proof}
    \begin{case}{$V$ is positive definite (no zero eigenvalues)}
        We can write $\vec{X} = \vec{\mu} + \sigma \vec{Z}$, where $\vec{Z}$ was as above defined. Then consider a change of variables $f_{\vec{X}}(\vec{x}) \mapsto f_{\vec{Z}}(\vec{z})$. From theorem~\ref{thmTransformRV}, this is defined by:
        \begin{equation*}
            f_{\vec{X}}(\vec{x}) = F_{\vec{Z}}(\vec{z}) |J|
        \end{equation*}
        The Jacobian is $\det{\sigma^{-1}}$, and this is $\frac{1}{\sqrt{\det{V}}}$.

        Note also that $\vec{z} = \sigma^{-1} (\vec{x} - \vec{\mu})$.
        \begin{align*}
            f_{\vec{X}}(\vec{x}) &= \prod_{i=1}^{n} \exp\left({-\frac{z_i^2}{2}}{\sqrt{2\pi}} \frac{1}{\sqrt{\det{V}}}\right) \\
            &= \frac{1}{\sqrt{(2\pi)^n \det{V}}} \exp\left({-\frac{|\vec{z}|^2}{2}}\right) \\
            &= \frac{1}{\sqrt{(2\pi)^n \det{V}}} \exp\left({-\frac{\vec{z}^T \vec{z}}{2}}\right) \\
            &= \frac{1}{\sqrt{(2\pi)^n \det{V}}} \exp\left({-\frac{(\vec{x} - \vec{\mu})^T (\sigma^{-1})^T \sigma^{-1} (\vec{x} - \vec{\mu})}{2}}\right) \\
            &= \frac{1}{\sqrt{(2\pi)^n \det{V}}} \exp\left({-\frac{(\vec{x} - \vec{\mu})^T (\sigma \sigma)^{-1} (\vec{x} - \vec{\mu})}{2}}\right) \\
            &= \frac{1}{\sqrt{(2\pi)^n \det{V}}} \exp\left({-\frac{(\vec{x} - \vec{\mu})^T V^{-1} (\vec{x} - \vec{\mu})}{2}}\right)
        \end{align*}
    \end{case}
    \begin{case}{$V$ has some zero eigenvalues}
        By an orthogonal change of basis, we can assume that:
        \begin{equation*}
            V =
            \begin{pmatrix}
                U & 0 \\
                0 & 0
            \end{pmatrix}
        \end{equation*}
        Where $U$ is positive definite, and an $m \times m$ matrix.

        Write also that $\vec{\mu} = \begin{pmatrix}\vec{\lambda} \\ \vec{\nu}\end{pmatrix}$ where $\vec{\lambda} \in \R^m, \vec{\nu} \in \R^{n-m}$

        Then we can now write $\vec{X} = \begin{pmatrix} \vec{Y} \\ \vec{\nu}\end{pmatrix}$ where $Y \sim N(\vec{\lambda}, U)$ and $\vec{Y}$ has density:
        \begin{equation*}
            F_{\vec{Y}}(\vec{y}) = \frac{1}{\sqrt{(2\pi)^n} \det{U}} \exp\left(-\frac{(\vec{y} -\lambda)^T U^{-1} (\vec{y} - \vec{\lambda})}{2}\right)
        \end{equation*}
        by case 1.
    \end{case}
\end{proof}

\begin{proposition}
    If $X_1, X_2, \cdots, X_n$ are independent, then $V$ is a diagonal matrix.
    \label{propDiagonalVariance}
\end{proposition}
\begin{proof}
    Simply note that the non-diagonal elements of $V$ are covariances, which are zero if the variables are independent.
\end{proof}
\begin{lemma}
    For a Gaussian vector $\vec{X}$, with positive-definite variance matrix $V$, if $V$ is diagonal then the $X_i$ are independent.
    \label{lemGaussVecIndepIfZeroCov}
\end{lemma}
\begin{proof}[by factorising the density function]
    Let $V = \text{diag}(\lambda_1, \lambda_2, \cdots, \lambda_n)$, and note that these are all positive. Then the density function is:
    \begin{align*}
        f_{\vec{X}}(\vec{x}) &= \frac{1}{\sqrt{(2\pi)^n} \det{V}} \text{exp} \left(-\frac{(\vec{x} - \vec{\mu})^T V^{-1} (\vec{x} - \vec{\mu})}{2}\right) \\
        f_{\vec{X}}(\vec{x}) &= \frac{1}{\sqrt{(2\pi)^n} \det{V}} \text{exp} \left(-\frac{1}{2} \sum_{i = 1}^n \frac{(x_i - \mu_i)^2}{2\lambda_i}\right) \\
        \prod_{i = 1}^n f_{\vec{X}}(\vec{x}) &= \frac{1}{\sqrt{(2\pi)^n} \det{V}} \text{exp} \left(-\frac{(x_i - \mu_i)^2}{4\lambda_i}\right)
    \end{align*}
    So the density function factorises, and we have that the individual elements are normal random variables, $X_i \sim N(\mu_i, \lambda_i)$ as expected.
\end{proof}
\begin{proof}[by moment generating functions]
    We find the moment generating function of $\vec{X}$:
    \begin{align*}
        m(\vec{\theta}) &= E[\exp(\vec{\lambda}^T \vec{X})] \\
        \vec{\theta}^T\vec{X} &\sim N(\vec{\theta}^T \mu \vec{\theta}^T V \vec{\theta}) \\
        m(\vec{\theta}) &= \exp\left(\vec{\theta}^T \mu + \frac{\vec{\theta}^T V \vec{\theta^T}}{2}\right) \\
        &= \exp\left(\sum_i \theta_i \mu_i + \sum_i \frac{\theta_i^2 \lambda_i}{2}\right)
    \end{align*}
    So $m(\vec{\theta})$ is a product of the required individual random variables.
\end{proof}
\begin{remark}
    Note that this is not true for any vector of random variables. In general, a diagonal variance matrix does not imply independence, as we have seen in the 1-dimensional case with $\Cov(X, Y) = 0 \nRightarrow X \perp Y$.
\end{remark}
\begin{definition}{Correlation}
    For two random variables $X$ and $Y$, the \underline{correlation}, $\Corr(X, Y)$, is defined to be:
    \begin{equation*}
        \Corr(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var(X)\Var(Y)}}
    \end{equation*}
\end{definition}
\begin{example}[Bivariate Gaussian]
    Let $\vec{X} = (X_1, X_2)^T$ be a Gaussian vector in $\R^2$. Define $\mu_k = E[X_k]$, and $\sigma_k^2 = \Var(X_k)$

    Let $\rho = \Corr(X_1, X_2)$. Then we want to show that $\rho \in [-1, 1]$. We can prove this by the Cauchy-Schwarz inequality:
    \begin{align*}
        \Corr(X_1, X_2) &= \frac{E[(X_1 - \mu_1)(X_2 - \mu_2)]}{\sqrt{E[(X_1 - \mu_1)^2]E[(X_2 - \mu_2)^2]}} \\
        |\Corr(X_1, X_2)| &\leq \frac{\sqrt{E[(X_1 - \mu_1)^2] E[(X_2 - \mu_2)^2]}}{\sqrt{E[(X_1 - \mu_1)^2] E[(X_2 - \mu_2)^2]}} \\
        &= 1
    \end{align*}
    Then the variance matrix is:
    \begin{equation*}
        V =
        \begin{pmatrix}
            \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
            \rho \sigma_1 \sigma_2 & \sigma_2^2
        \end{pmatrix}
    \end{equation*}
    Then we shall show that for any $\sigma_1, \sigma_2 > 0$ and any $\rho \in [-1, 1]$ then the matrix $V$ as above defined is a non-negative definite matrix. It is sufficient to show that for any $\vec{u} \in \R^2$, $\vec{u}^T V \vec{u}$. Then:
    \begin{align*}
        \vec{u}^T V \vec{u} &= (1 - \rho)\left(\sigma_1^2 u_1^2 + \sigma_2^2 u_2^2\right) + \rho(\sigma_1 u_1 + \sigma_2 u_2)^2 \\
        &= (1 + \rho)(\sigma_1^2 u_1^2 + \sigma_2^2 u_2^2) - \rho(\sigma_1 u_1 - \sigma_2 u_2)^2
    \end{align*}
    We use the correct expression for positive or negative $\rho$. If $\rho \in [-1, 0]$, we use the second equality (which is certainly non-negative), and if $\rho \in (0, 1]$ we use the first equality, which is certainly non-negative.

    Note that if $\rho = 0$, then $V$ becomes a diagonal matrix.

    Now consider $E[X_2 | X_1]$. Consider $a \in \R$. Write $X_2 = (X_2 - a X_1) + aX_1$
    \begin{align*}
        \Cov(X_2 - aX_1, X_1) &= cov(X_2, X_1) - a\Var(X_1) \\
        &= \rho \sigma_1 \sigma_2 - a\sigma_1^2
    \end{align*}
    So now choose $\rho \sigma_1 \sigma_2 - a\sigma_1^2 = 0$. That is, $a = \frac{\rho \sigma_2}{\sigma_1}$.

    Now let $Y = X_2 - aX_1$. Then we have shown that $\Cov(Y, X_1) = 0$ for this choice of $a$. Then we consider the vector $(X_1, Y)^T$.
    We can write $(X_1, Y)$ as a linear map:
    \begin{equation*}
        \begin{pmatrix}X_1 \\ Y\end{pmatrix} = \begin{pmatrix}1 & 0 \\ -a & 1\end{pmatrix}\begin{pmatrix}X_1 \\ X_2\end{pmatrix}
    \end{equation*}
    and therefore this is a Gaussian vector with elementwise covariance 0. By lemma~\ref{lemGaussVecIndepIfZeroCov}, $X_1$ and $Y$ are independent.

    Then now we can take the expectation:
    \begin{align*}
        E[X_2 | X_1] &= E[X_2 - aX_1 | X_1] + E[aX_1 | X_1] \\
        &= E[X_2 - aX_1] + aX_1 \text{ by independence}
    \end{align*}
    And so we have shown:
    \begin{equation}
        E[X_2 | X_1] = E[X_2] + a(X_1 - E[X_1])
        \label{eqnCondExpecGaussVars}
    \end{equation}
\end{example}
\section{Simulating Random Variables}
\subsection{General Distributions}
Suppose $X$ is a random variable with probability distribution function $F$. We know how to simulate a uniform distribution, $U([0, 1])$. Therefore we want to find some transformation to simulate the random variable $X$.

Let $U \sim U([0, 1])$. Assume that $F$ is a bijection. Therefore take $X = F^{-1}(U)$. Therefore:
\begin{align*}
    P(X \leq x) &= P(F^{-1}(U) \leq x) \\
    &= P(U \leq F(x)) \\
    &= F(x) \text{ since } U \text{ is uniform.}
\end{align*}
However, if $F$ is not a bijection we need a \underline{generalised inverse}. Define:
\begin{equation*}
    G(u) = \inf\subsetselect{x \in \R}{u \leq F(x)}
\end{equation*}
Then $G : (0, 1) \mapsto \R$. Now we need to ensure that $G(u) \leq x \Leftrightarrow u \leq F(x)$.

The forward direction is true by definition of the infimum.

Now consider the reverse direction. If $u \leq F(x)$, suppose $G(u) > x$. This implies $u > F(x)$, reaching a contradiction since $F$ is increasing. Therefore, define $X = G(U)$. As before, we have the required:
\begin{equation*}
    P(X \leq x) = P(U \leq F(x)) = F(x)
\end{equation*}
\subsection{Box-Muller Transform}
We now want to generate $X, Y$ independent standard normal random variables.

Earlier, we saw that transforming $X, Y$ to polar coordinates, $R$ has density $re^{-\frac{r^2}{2}}$ and $\theta \sim U([0, 2\pi])$. We then want to invert this transformation.

Let $U$ and $V$ be independent random variables with distribution $U([0, 1])$. Define $\theta = 2\pi U$, which is the required $\theta$. If then we set $R = \sqrt{-2\log(V)}$ then:
\begin{align*}
    P(R \geq r) &= P(\sqrt{-2\log(V)} \geq r) \\
    &= P(V \leq e^\frac{-r^2}{2})
\end{align*}
And therefore differentiating gives the required density for $R$. Now setting $X = R\cos(\theta)$ and $Y = R\sin(\theta)$ gives independent standard normal random variables.
\subsection{Rejection Sampling}
Suppose we have a random variable $X$ with density $f(x) = \frac{I_{X \in A}}{|A|}$ where $A$ is a subset of the unit cube in $n$ dimensions: $A \subseteq [0, 1]^d$.

Let $(\vec{U_n})$ be independent and identically distributed $d$-dimensional uniform random variables. Define:
\begin{equation*}
    \left(U_{k, n}~|~k \in \{1, \cdots, d\}, n\in\N\right) \text{ to be } U([0, 1])
\end{equation*}
And therefore we have independent and identically distributed uniform vectors $\vec{U_n}$. Define:
\begin{equation*}
    N = \min\subsetselect{n}{U_n \in A} \text{ and } X = U_N
\end{equation*}
Let $X \in [0, 1]^d$.
\begin{align*}
    P(X \in B) &= P(U_n \in B) \\
    &= \sum_{n = 1}^\infty P(U_N \in B, N = n) \\
    &= \sum_{n = 1}^\infty P(U_n \in B, U_n \in A, U_1 \notin A, \cdots, U_{n-1} \notin A) \\
    &= \sum_{n = 1}^\infty P(U_n \in A \cap B) \left(P(U_1 \notin A)\right)^{n-1} \\
    &= \sum_{n = 1}^\infty |A \cap B| \left(1 - |A|\right)^{n-1} \\
    &= \frac{|A \cap B|}{|A|}
\end{align*}
\end{document}