\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
We consider the triple $(\Omega, \sigalg, P)$ to be a probability space. Let $\Omega$ be the set of events $\omega_1, \omega_2, \cdots$, and let $\sigalg$ be all subsets.\par
If we know $P(\omega_i)$ for all $i$, then any subset $A$ in $\sigalg$ has probability:
\begin{equation}
    P(A) = P\left(\bigcup_{\omega \in A} \{\omega\}\right) = \sum_{\omega \in A}P(\{\omega\})
    \label{eqnFiniteCountAdd}
\end{equation}
Then we write $P_i = P(\{\omega_i\})$, and call $(p_i)_{i \in \N}$ a discrete probability distribution. It must satisfy:
\begin{itemize}
    \item $p_i \geq 0~\forall i$,
    \item $\sum_{i \in \N} p_i = 1$.
\end{itemize}
\section{Important Discrete Distributions}
\begin{definition}{Bernoulli distribution}
    The \underline{Bernoulli distribution} with parameter $p$ models two outcomes: $\Omega = \{0, 1\}$. It is given by $p_1 = p$, so $p_0 = 1-p$.
\end{definition}
The Bernoulli distribution is used to model a single trial with two outcomes, such as a coin toss (where, if the coin is unbiased, $p = 0.5$).
\begin{definition}{Binomial distribution}
    The \underline{binomial distribution} with parameters $n$ and $p$ models the outcomes of $n$ trials of an event with two outcomes. $\Omega = \{1, \cdots, n\}$, and the probability of each event is:
    \begin{equation*}
        p_i = \choose{n}{i}p^i(1-p)^{n-i}
    \end{equation*}
    This can be derived from multiple independent Bernoulli-distributed events.
\end{definition}
\begin{example}[Multiple coin tosses]
    For a biased coin with probability of heads $p$, the Binomial distribution models the outcomes of $n$ independent coin tosses.
\end{example}
\begin{definition}{Multinomial distribution}
    The \underline{multinomial distribution} with parameters $n, k, (p_i)_{i=1}^k$ models the outcome of $n$ independent trials of an event with $k$ outcomes, each with probability $p_i$. $\Omega = \subsetselect{(n_1, n_2, \cdots, n_k)}{\sum_{i=1}^k n_i = n}$.\par
    Therefore the probability of any such tuple, $p_{n_1, \cdots, n_k}$ is:
    \begin{equation*}
        p_{n_1, \cdots, n_k} = \choose{n}{n_1, \cdots, n_k} p_1^{n_1} \cdots p_k^{n_k}
    \end{equation*}
\end{definition}
Note that the binomial distribution is the special case $n = 2$, with probabilities $p_2 = p, p_1 = 1-p$.
\begin{example}[Throwing balls into boxes]
    Given $n$ balls, each thrown independently into exactly one of $k$ boxes, the multinomial distribution models the probability of any final configuration of balls in boxes.
\end{example}
\begin{definition}{Geometric distribution}
    The \underline{geometric distribution} with parameter $p$ models repeating a trial with 2 outcomes until an outcome appears. $\Omega$ is the set of numbers of trials required, and so $\Omega = \N$. The probability that $i$ trials are needed is:
    \begin{equation*}
        p_i = (1-p)^{i-1}p
    \end{equation*}
    \label{defGeometricDist}
\end{definition}
The geometric distribution can model, for example, tossing a coin $k$ times until getting a head.
\begin{warning}
    There exist 2 similar ways of defining the geometric distribution. Either definition~\ref{defGeometricDist}, or $p_i = (1-p)^i$, which is the number of unsuccessful trials before a successful one. In the second case, $\Omega = \N_0$.
\end{warning}
\begin{definition}{Poisson distribution}
    The \underline{Poisson distribution} with parameter $\lambda$ models the number of occurrences of an event that occurs at an average rate $\lambda$ over an interval. Then the probability of $i$ occurrences is:
    \begin{equation*}
        p_i = e^{-\lambda}\frac{\lambda^i}{i!}
    \end{equation*}
\end{definition}
The Poisson distribution is the limit of the binomial distribution, and we can derive $p_i$ as such. Consider the following example:
\begin{example}[Customers arriving at a shop]
    Suppose customers arrive at a shop on an interval. Rescale time such that the interval is $[0, 1]$. Let $N \in \N$ to discretise $[0, 1]$:
        \begin{equation*}
            \left[\frac{i-1}{N}, \frac{i}{N}\right], i = 1, \cdots, N
        \end{equation*}
        Then let the probability a customer arrives in a sub-interval be $p$. Let arrivals in different intervals be independent. Then the probability that $k$ customers arrive in $[0, 1]$ is:
        \begin{equation*}
            \choose{N}{k}p^k(1-p)^k
        \end{equation*}
        Now let us take $p = \frac{\lambda}{N}$, where $\lambda$ is the rate of arrival.\par
        Then the probability that $k$ customers arrive becomes:
        \begin{align*}
            &\choose{N}{k} \left(\frac{\lambda}{N}\right)^k \left(1-\frac{\lambda}{N}\right)^{N-k} \\
            &= \frac{N!}{k!(N-k)!} \frac{\lambda^k}{N^k}\left(1-\frac{\lambda}{N}\right)^{N-k} \\
            &= \frac{\lambda^k}{k!} \frac{N(N-1)\cdots(N-k+1)}{N^k}\left(1-\frac{\lambda}{N}\right)^{N-k}
        \end{align*}
        And as $N \to \infty$, this tends to:
        \begin{equation*}
            e^{-\lambda} \frac{\lambda^k}{k!}
        \end{equation*}
        Which is the definition of the Poisson distribution.
\end{example}
\section{Random Variables}
Consider a probability space $(\Omega, \sigalg, P)$.
\begin{definition}{Random variable}
    A \underline{random variable} is a function $X: \Omega \mapsto \R$ with the property that for all $x \in \R$, $\subsetselect{\omega \in \Omega}{X(\omega) \leq x} \in \sigalg$.
\end{definition}
Intuitively, we think of a random variable as assigning a number to each event in $\Omega$. For example, if $X$ represents dice rolls, we assign the numbers 1 to 6 for each outcome in $\Omega$.\par
We also write $\{X \in A\}$ for the set:
\begin{equation*}
    \subsetselect{\omega \in \Omega}{X(\omega) \in A}
\end{equation*}
That is, the set of outcomes corresponding to the elements of $A$, after mapping using $X$. Then the defining property of a random variable becomes:
\begin{equation*}
    \{X \leq x\} \in \sigalg
\end{equation*}
Now we can intuit what this property means: it requires the subset of outcomes given by $\{X \leq x\}$ be a valid event, for all $x$. Using this, we can consider an ordering of events.\par
We also define the \underline{indicator function} for a set $A$:
\begin{align*}
    1_A : \Omega &\mapsto \{0, 1\} \\
    \omega &\mapsto
    \begin{cases}
        1 & \omega \in A \\
        0 & \omega \in A^C
    \end{cases}
\end{align*}
\begin{definition}{Probability distribution function}
    The \underline{probability distribution function} of $X$ is defined to be:
    \begin{align*}
        F_X : \R &\mapsto [0, 1] \\
        x &\mapsto P(X \leq x)
    \end{align*}
\end{definition}
Here we see the power of the random variable. The outcomes in $\Omega$ have no explicit ordering, but using the random variable to assign numbers to each outcome allows us to enjoy all the properties of real numbers.
\begin{definition}{random variables in $\R^n$}
    $(X_1, X_2, \cdots, X_n)$ is called a \underline{random variable in $\R^n$} if
    \begin{equation*}
        (X_1, \cdots, X_n) : \Omega \mapsto \R
    \end{equation*}
    is a function, and for all $x_1, \cdots, x_n$ then:
    \begin{equation*}
        \{X_1 < x_1, \cdots, X_n < x_n\} \in \sigalg
    \end{equation*}
\end{definition}
This is equivalent to saying that $X_1, \cdots, X_n$ are all random variables.
\subsection{Discrete Random Variables}
\begin{definition}{Discrete random variables}
    A random variable $X$ is a \underline{discrete random variable} if its range is finite, or a countable subset of $\R$.
\end{definition}
\begin{definition}{Probability mass function}
    Suppose $X$ takes values in the countable set $S$. For every $x \in S$, we write $p_x$ for $P(X = x)$\par
    We call $(p_x)_{x \in S}$ the \underline{probability mass function} or distribution of $X$. This completely determines the random variable.
\end{definition}
If, for example, the distribution of $X$ is the Bernoulli distribution then we say $X$ is a Bernoulli random variable, or $X$ has the Bernoulli distribution. Same goes for Geometric, Poisson, etc. For this, we write $X \sim B(n, p)$ which means $X$ has binomial distribution.
\begin{definition}{Independence of random variables}
    Recall that if $A$ and $B$ are events, then $P(A \cap B) = P(A) \times P(B)$ is the condition for independence. We can apply the same notion to random variables:\par
    Let $X_1, \cdots, X_n$ be discrete random variables with ranges $S_1, \cdots, S_n$. Then these are \underline{independent} if 
    \begin{equation*}
        P(X_1 = x_1, \cdots, X_n = x_n) = P(X_1 = x_1) \cdots P(X_n = x_n)
    \end{equation*}
    for any $x_i \in S_i, i = 1, \cdots, n$.
\end{definition}
\begin{example}[Tossing a biased coin]
    Consider tossing a coin, with probability of heads $p$, independently.\par
    $\Omega = \{0, 1\}^N$. That is, each $\omega \in \Omega$ is a string of 0 or 1.\par
    Define a random variable $X_k$ for each outcome, such that $X_k \in \{0, 1\}$ represents the outcome of the $k$th coin toss.\par
    Then $P(X_k = 1) = p, P(X_k = 0) = 1-p$, and so $X_k$ is a Bernoulli random variable.\par
    We can also consider the total number of heads.\par
    Let $S_N : \Omega \mapsto \{1, \cdots, N\}$ and:
    \begin{equation*}
        S_N(\omega) = \sum_{k=1}^N X_k(\omega_k)
    \end{equation*}
    So then $S_N$ is a Binomial random variable:
    \begin{equation*}
        P(S_N = k) = \choose{n}{k} p^k (1-p)^{N-k}
    \end{equation*}
\end{example}
\subsection{Expectation}
The power of random variables gives us the ability to define an expected value, which may not be in the range of the random variable, but gives an important result nonetheless.
\begin{definition}{Expectation}
    Let $X$ be a random variable with non-negative range.\par
    Then the \underline{expectation} of $X$ is:
    \begin{equation}
        E[X] = \sum_{\omega \in \Omega} X(\omega) P(\{\omega\})
        \label{eqnExpectationDiscrete}
    \end{equation}
\end{definition}
Note that if we define $\Omega_X$ to be the range of $X$, then equation~\ref{eqnExpectationDiscrete} can be written as:
\begin{equation*}
    E[X] = \sum_{x \in \Omega_X} x P(X = x)
\end{equation*}
So the expectation can be considered a weighted average of outcomes by probability.
\begin{example}[Expectation of the binomial distribution]
    Let $X \sim B(n, p)$.
    \begin{align*}
    E[X] &= \sum_{k=0}^N k P(X = k) \\
    &= \sum_{k=0}^N \frac{k \cdot N!}{k! (N-k)!} p^k (1-p)^{N-k} \\
    &= \sum_{k=1}^N \frac{N (N-1)!}{(k-1)!(N-k)!} p^k (1-p)^{N-k} \\
    &= Np \sum_{k=1}^N \frac{(N-1)!}{(k-1)!(N-k)!} p^{k-1} (1-p)^{N-k} \\
    &= Np \sum_{k=1}^N \choose{N-1}{k-1} p^{k-1} (1-p)^{N-k} \\
    &= Np \sum_{k=0}^{n-1} \choose{N-1}{k} p^k (1-p)^{N-k-1} \\
    &= Np
    \end{align*}
\end{example}
So far we have not defined $E[X]$ if $X$ takes negative values.\par
Let $X$ be any discrete random variable. Let $X_+ = \max{\{X, 0\}}$ and $X_- = \max{\{-X, 0\}}$.\par
Now we have two non-negative variables. Then the expectation of $X$ is given by:
\begin{equation}
    E[X] = E[X_+] - E[X_-]
    \label{eqnExpectationNegativeDiscrete}
\end{equation}
Note that it is possible for equation~\ref{eqnExpectationDiscrete} not to converge (if it is a sum to $\infty$). In this case we say that the expectation is not defined. If, however, we do get a valid (finite) expectation, we call $X$ integrable.
\begin{propositions}[Properties of expectation]{
        Let $X$ be a discrete random variable.
        \label{propsExpectationProps}
    }
    \item If $X \geq 0$, then $E[X] \geq 0$ \label{propExpecNonNegativity}
    \item If $X \geq 0$, and $E[X] = 0$, then $P(X = 0) = 1$. \label{propExpecZero}
    \item If $c$ is a real number, $E[cX] = cE[X]$ and $E[c + X] = c + E[X]$. \label{propExpecSumProd}
    \item Let $X_1, X_2, \cdots, X_n$ be integrable random variables. Let $c_1, \cdots, c_n$ be real numbers. Then expectation obeys linear combination:
        \begin{equation*}
            E\left[\sum_{i=1}^n c_i X_i\right] = \sum_{i=1}^n c_i E[X_i]
        \end{equation*}
        \label{propExpecLinearCombo}
\end{propositions}
\begin{lemma}
    Suppose $X_1, X_2, \cdots$ are non-negative random variables. Then:
    \begin{equation*}
        E\left[\sum_{n=1}^\infty X_n\right] = \sum_{n=1}^\infty E[X_n]
    \end{equation*}
    if the sums are convergent.
\end{lemma}
\begin{proof}
    \begin{align*}
        E\left[\sum_{n=1}^\infty X_n\right] &= \sum_{\omega \in \Omega} \sum_{n=1}^\infty X_n(\omega) P(\{\omega\}) \\
        &= \sum_{n=1}^\infty \sum_{\omega \in \Omega} X_n(\omega) P(\{\omega\}) \\
        &= \sum_{n=1}^\infty E[X_n]
    \end{align*}
\end{proof}
\begin{proposition}
    Suppose that $X$ is a random variable, and let $\Omega$ be countable. Let $g$ be a function $\R \mapsto \R$.
    \begin{equation*}
        E[g(X)] = \sum_{x \in \Omega_X} g(x) P(X = x)
    \end{equation*}
    \label{propExpecFunc}
\end{proposition}
\begin{proof}
    Let $Y = g(X)$. Then its expectation is:
    \begin{equation*}
        E[Y] = \sum_{y \in \Sigma_Y} y P(Y = y)
    \end{equation*}
    Note that the set $\{Y = y\}$ is:
    \begin{align*}
        \subsetselect{\omega \in \Omega}{Y(\omega) = y} &= \subsetselect{\omega \in \Omega}{g(X(\omega)) = y} \\
        &= \{X \in g^{-1}(\{y\})\} \\
        &= \bigcup_{x \in g^{-1}(\{y\})} \{X = x\}
    \end{align*}
    So then we can evaluate $E[Y]$:
    \begin{align*}
        E[Y] &= \sum_{y \in \Omega_Y} y P(X \in g^{-1}(\{y\})) \\
        &= \sum_{y \in \Omega_Y} y \sum_{x \in g^{-1}(\{y\})} P(X = x) \\
        &= \sum_{y \in \Omega_Y} \sum_{x \in g^{-1}(\{y\})} g(x) P(X = x) \\
        &= \sum_{x \in \Omega_X} g(x) P(X = x)
    \end{align*}
\end{proof}
\subsection{Indicator Functions}
We can define indicator functions in a similar (but not identical) way to IA Numbers and Sets.
\begin{definition}{Indicator function}
    For an event $A \in \sigalg$, the \underline{indicator function} for $A$ is defined to be:
    \begin{equation*}
        I_A(\omega) =
        \begin{cases}
            1 & \omega \in A \\
            0 & \omega \notin A
        \end{cases}
    \end{equation*}
\end{definition}
Note that, as we simply use $X$ instead of $X(\omega)$, we often use $I_A$ instead of $I_A(\omega)$.\par
A very useful fact for probability is that the expectation of the indicator is the probability:
\begin{equation}
    E\left[I_A\right] = P(A)
    \label{eqnIndicatorExpec}
\end{equation}
The indicator function has a number of properties. For $A, B \in \sigalg$:
\begin{itemize}
    \item $I_A(\omega) = 1 - I_{A^C}(\omega)$.
    \item $I_{A \cap B}(\omega) = I_A(\omega) I_B(\omega)$
    \item $I_{A \cup B}(\omega) = 1 - (1 - I_A(\omega))(1 - I_B(\omega))$.
\end{itemize}
More generally, for a countable set $A_i \in \sigalg$,
\begin{equation}
    I_{A_1 \cup A_2, \cdots}(\omega) = 1 - \prod_{i \in \N} (1 - I_{A_i}(\omega))
    \label{eqnUnionIndicator}
\end{equation}
Then equation~\ref{eqnUnionIndicator} can be expanded, and when expectations are taken we get the inclusion-exclusion formula.
\begin{proposition}
    Let $X$ be a random variable taking values in the set $\N_0$.
    \begin{equation*}
        E[X] = \sum_{k=1}^\infty P(X \geq k) = \sum_{k = 0}^\infty P(X > k)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        X(\omega) &= \sum_{k=1}^\infty I_{X(\omega) \geq k} \\
        &= \sum_{k=0}^\infty I_{X(\omega) > k}
    \end{align*}
    Then taking the expectation:
    \begin{equation*}
        E[X] = \sum_{k=1}^\infty P(X \geq k) = \sum_{k=0}^\infty P(X > k)
    \end{equation*}
\end{proof}
\subsection{Variance}
\begin{definition}{Moment of a Random Variable}
    For a random variable $X$, and a natural number $r$, the \underline{$r$th moment of $X$} is $E[X^r]$.
\end{definition}
\begin{definition}{Variance}
    The \underline{variance} of a random variable $X$ is defined:
    \begin{equation*}
        \Var{(X)} = E\left[(X - E[X])^2\right]
    \end{equation*}
\end{definition}
Variance is a measure of the spread of $X$. A low variance means $X$ is very concentrated around a single point.\par
The standard deviation is defined as the square root of the variance.\par
We have some properties. Consider $X$ a random variable and $c$ a real number:
\begin{itemize}
    \item If $\Var{(X)} = 0$ then $X$ takes a single value
    \item $\Var{(cX)} = c^2 \Var{(X)}$
    \item $\Var{(X + c)} = \Var{(X)}$
\end{itemize}
\begin{proposition}
    \begin{equation}
        \Var{(X)} = E[X^2] - \left(E[X]\right)^2
        \label{eqnAlternateVariance}
    \end{equation}
\end{proposition}
\begin{proof}
    \begin{align*}
        \Var{(X)} &= E\left[(X - E[X])^2\right] \\
        &= E\left[X^2 - 2XE[X] + (E[X])^2\right] \\
        &= E[X^2] - E[2XE[X]] + E\left[(E[X])^2\right] \\
        &= E[X^2] - 2E[X]E[X] + (E[X])^2 \\
        &= E[X^2] - (E[X])^2
    \end{align*}
\end{proof}
\begin{lemma}
    \begin{equation*}
        \Var{(X)} = \min_{c \in \R} E\left[(X - c)^2\right]
    \end{equation*}
    And is achieved when $c = E[X]$.
\end{lemma}
This lemma considers a quantity akin to the variance centred on multiple different points $c$. Then, the quantity is minimised when the centre is $E[X]$.
\begin{proof}
    Let $f(c) = E\left[(X - c)^2\right]$.
    \begin{align*}
        f(c) &= E[X^2] - 2cE[X] + c^2 \\
        f'(c) &= -2E[X] + 2c
    \end{align*}
    Which is zero when $c = E[X]$.
\end{proof}
We can calculate variances for different distributions.
\begin{example}[Variance of the binomial distribution]
    Consider $X \sim B(n, p)$.\par
    \begin{align*}
        E[X(X-1)] &= \sum_{k=0}^n k(k-1) P(X = k) \\
        &= \sum_{k=2}^n k(k-1) \choose{n}{k} p^k (1-p)^{n-k} \\
        &= \sum_{k=2}^n k(k-1) \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} \\
        &= \sum_{k=2}^n \frac{n(n-1)(n-2)!}{(k-2)!(n-k)!} p^{k-2} p^2 (1-p)^{n-k} \\
        &= \sum_{k=0}^{n-2} \choose{n-2}{k} p^{k} (1-p)^{n-2-k} p^2 n(n-1) \\
        &= p^2 n(n-1)
    \end{align*}
    So then the variance is:
    \begin{align*}
        \Var{(X)} &= p^2 n(n-1) + np - (np)^2 \\
        &= p^2 n^2 - p^2 n + np - n^2 p^2 \\
        &= np(1-p)
    \end{align*}
\end{example}
We have also that the variance of the Poisson distribution is $\lambda$, worked out similarly to the above.
\subsection{Covariance}
\begin{definition}{Covariance}
    Let $X$ and $Y$ be two random variables. The \underline{covariance} of $X$ and $Y$ is:
    \begin{equation}
        \Cov{(X, Y)} = E\left[(X - E[X])(Y - E[Y])\right]
        \label{eqnCovariance}
    \end{equation}
\end{definition}
We have some properties that follow immediately. Consider $X, Y$ and $Z$ random variables, and $c$ a real number.
\begin{enumerate}
    \item $\Cov{(X, Y)} = \Cov{(Y, X)}$
    \item $\Cov{(X, X)} = \Var{(X)}$
    \item $\Cov{(X, Y)} = E[XY] - E[X]E[Y]$.
    \item $\Cov{(cX, Y)} = c\Cov{(cX, Y)}$
    \item $\Cov{(X + c, Y)} = \Cov{(cX, Y)}$.
    \item $\Var{(X + Y)} = \Var{(X)} + \Var{(Y)} + 2\Cov{(X,Y)}$.
    \item $\Cov{(c, X)} = 0$
    \item $\Cov{(X + Y, Z)} = \Cov{(X, Z)} + \Cov{(Y, Z)}$
\end{enumerate}
An in general, for random variables $X_i$ and $Y_i$:
\begin{equation}
    \Var{\left(\sum_{i \in \N} X_i\right)} = \sum_{i=1}^N \Var{(X_i)} + \sum_{i \neq j}^N \Cov{(X_i, X_j)}
    \label{eqnVarianceSum}
\end{equation}
Therefore, equation~\ref{eqnVarianceSum} shows that for independent random variables, variance commutes with summation.
\begin{lemma}
    Let $X$ and $Y$ be independent random variables. Let $f, g : \R \mapsto \R$. Then $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$.
\end{lemma}
\begin{proof}
    For any $x$ and $y$, $P(X = x, Y = y) = P(X = x)P(Y = y)$.
    \begin{align*}
        E[f(X)g(Y)] &= \sum_{(x, y)} f(x) g(y) P(X = x, Y = y) \\
        &= \sum_{(x, y)} f(x) g(y) P(X = x)P(Y = y) \\
        &= \sum_x gf(x) P(X = x) \sum_y g(y) P(Y = y) \\
        &= E[f(X)]E[g(Y)]
    \end{align*}
\end{proof}
Therefore if $X$ and $Y$ are independent, their covariance is 0. However, the converse is not true:
\begin{example}[0 covariance does not imply independence]
    Let $X_1, X_2, X_3$ be independent Bernoulli random variables with probability $\frac{1}{2}$. Let $Y_1 = 2X_1 - 1$, $Y_2 = 2X_2 - 1$, $Z_1 = Y_1 X_3$ and $Z_2 = Y_2 X_3$.\par
    Then the expectation of $Y_1$ and $Y_2$ are 0 (since they have equal probability of being $1$ and $-1$), and therefore so are the expectations of $Z_1$ and $Z_2$.
    \begin{align*}
        \Var{(Z_1, Z_2)} &= E[Z_1 Z_2] - E[Z_1] E[Z_2] \\
        &= 0 - 0 - 0 = 0
    \end{align*}
    So $Z_1$ and $Z_2$ have 0 covariance.\par
    However, $P(Z_1 = 0, Z_2 = 0) = P(X_3 = 0) = \frac{1}{2}$, but $P(Z_1 = 0)P(Z_2 = 0) = \frac{1}{4}$, so they are not independent.
\end{example}
\end{document}