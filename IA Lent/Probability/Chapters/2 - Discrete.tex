\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
We consider the triple $(\Omega, \sigalg, P)$ to be a probability space. Let $\Omega$ be the set of events $\omega_1, \omega_2, \cdots$, and let $\sigalg$ be all subsets.\par
If we know $P(\omega_i)$ for all $i$, then any subset $A$ in $\sigalg$ has probability:
\begin{equation}
    P(A) = P\left(\bigcup_{\omega \in A} \{\omega\}\right) = \sum_{\omega \in A}P(\{\omega\})
    \label{eqnFiniteCountAdd}
\end{equation}
Then we write $P_i = P(\{\omega_i\})$, and call $(p_i)_{i \in \N}$ a discrete probability distribution. It must satisfy:
\begin{itemize}
    \item $p_i \geq 0~\forall i$,
    \item $\sum_{i \in \N} p_i = 1$.
\end{itemize}
\section{Important Discrete Distributions}
\begin{definition}{Bernoulli distribution}
    The \underline{Bernoulli distribution} with parameter $p$ models two outcomes: $\Omega = \{0, 1\}$. It is given by $p_1 = p$, so $p_0 = 1-p$.
\end{definition}
The Bernoulli distribution is used to model a single trial with two outcomes, such as a coin toss (where, if the coin is unbiased, $p = 0.5$).
\begin{definition}{Binomial distribution}
    The \underline{binomial distribution} with parameters $n$ and $p$ models the outcomes of $n$ trials of an event with two outcomes. $\Omega = \{1, \cdots, n\}$, and the probability of each event is:
    \begin{equation*}
        p_i = \choose{n}{i}p^i(1-p)^{n-i}
    \end{equation*}
    This can be derived from multiple independent Bernoulli-distributed events.
\end{definition}
\begin{example}[Multiple coin tosses]
    For a biased coin with probability of heads $p$, the Binomial distribution models the outcomes of $n$ independent coin tosses.
\end{example}
\begin{definition}{Multinomial distribution}
    The \underline{multinomial distribution} with parameters $n, k, (p_i)_{i=1}^k$ models the outcome of $n$ independent trials of an event with $k$ outcomes, each with probability $p_i$. $\Omega = \subsetselect{(n_1, n_2, \cdots, n_k)}{\sum_{i=1}^k n_i = n}$.\par
    Therefore the probability of any such tuple, $p_{n_1, \cdots, n_k}$ is:
    \begin{equation*}
        p_{n_1, \cdots, n_k} = \choose{n}{n_1, \cdots, n_k} p_1^{n_1} \cdots p_k^{n_k}
    \end{equation*}
\end{definition}
Note that the binomial distribution is the special case $n = 2$, with probabilities $p_2 = p, p_1 = 1-p$.
\begin{example}[Throwing balls into boxes]
    Given $n$ balls, each thrown independently into exactly one of $k$ boxes, the multinomial distribution models the probability of any final configuration of balls in boxes.
\end{example}
\begin{definition}{Geometric distribution}
    The \underline{geometric distribution} with parameter $p$ models repeating a trial with 2 outcomes until an outcome appears. $\Omega$ is the set of numbers of trials required, and so $\Omega = \N$. The probability that $i$ trials are needed is:
    \begin{equation*}
        p_i = (1-p)^{i-1}p
    \end{equation*}
    \label{defGeometricDist}
\end{definition}
The geometric distribution can model, for example, tossing a coin $k$ times until getting a head.
\begin{warning}
    There exist 2 similar ways of defining the geometric distribution. Either definition~\ref{defGeometricDist}, or $p_i = (1-p)^i$, which is the number of unsuccessful trials before a successful one. In the second case, $\Omega = \N_0$.
\end{warning}
\begin{definition}{Poisson distribution}
    The \underline{Poisson distribution} with parameter $\lambda$ models the number of occurrences of an event that occurs at an average rate $\lambda$ over an interval. Then the probability of $i$ occurrences is:
    \begin{equation*}
        p_i = e^{-\lambda}\frac{\lambda^i}{i!}
    \end{equation*}
\end{definition}
The Poisson distribution is the limit of the binomial distribution, and we can derive $p_i$ as such. Consider the following example:
\begin{example}[Customers arriving at a shop]
    Suppose customers arrive at a shop on an interval. Rescale time such that the interval is $[0, 1]$. Let $N \in \N$ to discretise $[0, 1]$:
        \begin{equation*}
            \left[\frac{i-1}{N}, \frac{i}{N}\right], i = 1, \cdots, N
        \end{equation*}
        Then let the probability a customer arrives in a sub-interval be $p$. Let arrivals in different intervals be independent. Then the probability that $k$ customers arrive in $[0, 1]$ is:
        \begin{equation*}
            \choose{N}{k}p^k(1-p)^k
        \end{equation*}
        Now let us take $p = \frac{\lambda}{N}$, where $\lambda$ is the rate of arrival.\par
        Then the probability that $k$ customers arrive becomes:
        \begin{align*}
            &\choose{N}{k} \left(\frac{\lambda}{N}\right)^k \left(1-\frac{\lambda}{N}\right)^{N-k} \\
            &= \frac{N!}{k!(N-k)!} \frac{\lambda^k}{N^k}\left(1-\frac{\lambda}{N}\right)^{N-k} \\
            &= \frac{\lambda^k}{k!} \frac{N(N-1)\cdots(N-k+1)}{N^k}\left(1-\frac{\lambda}{N}\right)^{N-k}
        \end{align*}
        And as $N \to \infty$, this tends to:
        \begin{equation*}
            e^{-\lambda} \frac{\lambda^k}{k!}
        \end{equation*}
        Which is the definition of the Poisson distribution.
\end{example}
\section{Random Variables}
Consider a probability space $(\Omega, \sigalg, P)$.
\begin{definition}{Random variable}
    A \underline{random variable} is a function $X: \Omega \mapsto \R$ with the property that for all $x \in \R$, $\subsetselect{\omega \in \Omega}{X(\omega) \leq x} \in \sigalg$.
\end{definition}
Intuitively, we think of a random variable as assigning a number to each event in $\Omega$. For example, if $X$ represents dice rolls, we assign the numbers 1 to 6 for each outcome in $\Omega$.\par
We also write $\{X \in A\}$ for the set:
\begin{equation*}
    \subsetselect{\omega \in \Omega}{X(\omega) \in A}
\end{equation*}
That is, the set of outcomes corresponding to the elements of $A$, after mapping using $X$. Then the defining property of a random variable becomes:
\begin{equation*}
    \{X \leq x\} \in \sigalg
\end{equation*}
Now we can intuit what this property means: it requires the subset of outcomes given by $\{X \leq x\}$ be a valid event, for all $x$. Using this, we can consider an ordering of events.\par
We also define the \underline{indicator function} for a set $A$:
\begin{align*}
    1_A : \Omega &\mapsto \{0, 1\} \\
    \omega &\mapsto
    \begin{cases}
        1 & \omega \in A \\
        0 & \omega \in A^C
    \end{cases}
\end{align*}
\begin{definition}{Probability distribution function}
    The \underline{probability distribution function} of $X$ is defined to be:
    \begin{align*}
        F_X : \R &\mapsto [0, 1] \\
        x &\mapsto P(X \leq x)
    \end{align*}
\end{definition}
Here we see the power of the random variable. The outcomes in $\Omega$ have no explicit ordering, but using the random variable to assign numbers to each outcome allows us to enjoy all the properties of real numbers.
\begin{definition}{random variables in $\R^n$}
    $(X_1, X_2, \cdots, X_n)$ is called a \underline{random variable in $\R^n$} if
    \begin{equation*}
        (X_1, \cdots, X_n) : \Omega \mapsto \R
    \end{equation*}
    is a function, and for all $x_1, \cdots, x_n$ then:
    \begin{equation*}
        \{X_1 < x_1, \cdots, X_n < x_n\} \in \sigalg
    \end{equation*}
\end{definition}
This is equivalent to saying that $X_1, \cdots, X_n$ are all random variables.
\subsection{Discrete Random Variables}
\begin{definition}{Discrete random variables}
    A random variable $X$ is a \underline{discrete random variable} if its range is finite, or a countable subset of $\R$.
\end{definition}
\begin{definition}{Probability mass function}
    Suppose $X$ takes values in the countable set $S$. For every $x \in S$, we write $p_x$ for $P(X = x)$\par
    We call $(p_x)_{x \in S}$ the \underline{probability mass function} or distribution of $X$. This completely determines the random variable.
\end{definition}
If, for example, the distribution of $X$ is the Bernoulli distribution then we say $X$ is a Bernoulli random variable, or $X$ has the Bernoulli distribution. Same goes for Geometric, Poisson, etc. For this, we write $X \sim B(n, p)$ which means $X$ has binomial distribution.
\begin{definition}{Independence of random variables}
    Recall that if $A$ and $B$ are events, then $P(A \cap B) = P(A) \times P(B)$ is the condition for independence. We can apply the same notion to random variables:\par
    Let $X_1, \cdots, X_n$ be discrete random variables with ranges $S_1, \cdots, S_n$. Then these are \underline{independent} if 
    \begin{equation*}
        P(X_1 = x_1, \cdots, X_n = x_n) = P(X_1 = x_1) \cdots P(X_n = x_n)
    \end{equation*}
    for any $x_i \in S_i, i = 1, \cdots, n$.
\end{definition}
\begin{example}[Tossing a biased coin]
    Consider tossing a coin, with probability of heads $p$, independently.\par
    $\Omega = \{0, 1\}^N$. That is, each $\omega \in \Omega$ is a string of 0 or 1.\par
    Define a random variable $X_k$ for each outcome, such that $X_k \in \{0, 1\}$ represents the outcome of the $k$th coin toss.\par
    Then $P(X_k = 1) = p, P(X_k = 0) = 1-p$, and so $X_k$ is a Bernoulli random variable.\par
    We can also consider the total number of heads.\par
    Let $S_N : \Omega \mapsto \{1, \cdots, N\}$ and:
    \begin{equation*}
        S_N(\omega) = \sum_{k=1}^N X_k(\omega_k)
    \end{equation*}
    So then $S_N$ is a Binomial random variable:
    \begin{equation*}
        P(S_N = k) = \choose{n}{k} p^k (1-p)^{N-k}
    \end{equation*}
\end{example}
\subsection{Expectation}
The power of random variables gives us the ability to define an expected value, which may not be in the range of the random variable, but gives an important result nonetheless.
\begin{definition}{Expectation}
    Let $X$ be a random variable with non-negative range.\par
    Then the \underline{expectation} of $X$ is:
    \begin{equation}
        E[X] = \sum_{\omega \in \Omega} X(\omega) P(\{\omega\})
        \label{eqnExpectationDiscrete}
    \end{equation}
\end{definition}
Note that if we define $\Omega_X$ to be the range of $X$, then equation~\ref{eqnExpectationDiscrete} can be written as:
\begin{equation*}
    E[X] = \sum_{x \in \Omega_X} x P(X = x)
\end{equation*}
So the expectation can be considered a weighted average of outcomes by probability.
\begin{example}[Expectation of the binomial distribution]
    Let $X \sim B(n, p)$.
    \begin{align*}
    E[X] &= \sum_{k=0}^N k P(X = k) \\
    &= \sum_{k=0}^N \frac{k \cdot N!}{k! (N-k)!} p^k (1-p)^{N-k} \\
    &= \sum_{k=1}^N \frac{N (N-1)!}{(k-1)!(N-k)!} p^k (1-p)^{N-k} \\
    &= Np \sum_{k=1}^N \frac{(N-1)!}{(k-1)!(N-k)!} p^{k-1} (1-p)^{N-k} \\
    &= Np \sum_{k=1}^N \choose{N-1}{k-1} p^{k-1} (1-p)^{N-k} \\
    &= Np \sum_{k=0}^{n-1} \choose{N-1}{k} p^k (1-p)^{N-k-1} \\
    &= Np
    \end{align*}
\end{example}
So far we have not defined $E[X]$ if $X$ takes negative values.\par
Let $X$ be any discrete random variable. Let $X_+ = \max{\{X, 0\}}$ and $X_- = \max{\{-X, 0\}}$.\par
Now we have two non-negative variables. Then the expectation of $X$ is given by:
\begin{equation}
    E[X] = E[X_+] - E[X_-]
    \label{eqnExpectationNegativeDiscrete}
\end{equation}
Note that it is possible for equation~\ref{eqnExpectationDiscrete} not to converge (if it is a sum to $\infty$). In this case we say that the expectation is not defined. If, however, we do get a valid (finite) expectation, we call $X$ integrable.
\begin{propositions}[Properties of expectation]{
        Let $X$ be a discrete random variable.
        \label{propsExpectationProps}
    }
    \item If $X \geq 0$, then $E[X] \geq 0$ \label{propExpecNonNegativity}
    \item If $X \geq 0$, and $E[X] = 0$, then $P(X = 0) = 1$. \label{propExpecZero}
    \item If $c$ is a real number, $E[cX] = cE[X]$ and $E[c + X] = c + E[X]$. \label{propExpecSumProd}
    \item Let $X_1, X_2, \cdots, X_n$ be integrable random variables. Let $c_1, \cdots, c_n$ be real numbers. Then expectation obeys linear combination:
        \begin{equation*}
            E\left[\sum_{i=1}^n c_i X_i\right] = \sum_{i=1}^n c_i E[X_i]
        \end{equation*}
        \label{propExpecLinearCombo}
\end{propositions}
\begin{lemma}
    Suppose $X_1, X_2, \cdots$ are non-negative random variables. Then:
    \begin{equation*}
        E\left[\sum_{n=1}^\infty X_n\right] = \sum_{n=1}^\infty E[X_n]
    \end{equation*}
    if the sums are convergent.
\end{lemma}
\begin{proof}
    \begin{align*}
        E\left[\sum_{n=1}^\infty X_n\right] &= \sum_{\omega \in \Omega} \sum_{n=1}^\infty X_n(\omega) P(\{\omega\}) \\
        &= \sum_{n=1}^\infty \sum_{\omega \in \Omega} X_n(\omega) P(\{\omega\}) \\
        &= \sum_{n=1}^\infty E[X_n]
    \end{align*}
\end{proof}
\begin{proposition}
    Suppose that $X$ is a random variable, and let $\Omega$ be countable. Let $g$ be a function $\R \mapsto \R$.
    \begin{equation*}
        E[g(X)] = \sum_{x \in \Sigma_X} g(x) P(X = x)
    \end{equation*}
    \label{propExpecFunc}
\end{proposition}
\begin{proof}
    Let $Y = g(X)$. Then its expectation is:
    \begin{equation*}
        E[Y] = \sum_{y \in \Sigma_Y} y P(Y = y)
    \end{equation*}
    Note that the set $\{Y = y\}$ is:
    \begin{align*}
        \subsetselect{\omega \in \Omega}{Y(\omega) = y} &= \subsetselect{\omega \in \Omega}{g(X(\omega)) = y} \\
        &= \{X \in g^{-1}(\{y\})\} \\
        &= \bigcup_{x \in g^{-1}(\{y\})} \{X = x\}
    \end{align*}
    So then we can evaluate $E[Y]$:
    \begin{align*}
        E[Y] &= \sum_{y \in \Omega_Y} y P(X \in g^{-1}(\{y\})) \\
        &= \sum_{y \in \Omega_Y} y \sum_{x \in g^{-1}(\{y\})} P(X = x) \\
        &= \sum_{y \in \Omega_Y} \sum_{x \in g^{-1}(\{y\})} g(x) P(X = x) \\
        &= \sum_{x \in \Omega_X} g(x) P(X = x)
    \end{align*}
\end{proof}
\subsection{Indicator Functions}
We can define indicator functions in a similar (but not identical) way to IA Numbers and Sets.
\begin{definition}{Indicator function}
    For an event $A \in \sigalg$, the \underline{indicator function} for $A$ is defined to be:
    \begin{equation*}
        I_A(\omega) =
        \begin{cases}
            1 & \omega \in A \\
            0 & \omega \notin A
        \end{cases}
    \end{equation*}
\end{definition}
Note that, as we simply use $X$ instead of $X(\omega)$, we often use $I_A$ instead of $I_A(\omega)$.\par
A very useful fact for probability is that the expectation of the indicator is the probability:
\begin{equation}
    E\left[I_A\right] = P(A)
    \label{eqnIndicatorExpec}
\end{equation}
The indicator function has a number of properties. For $A, B \in \sigalg$:
\begin{itemize}
    \item $I_A(\omega) = 1 - I_{A^C}(\omega)$.
    \item $I_{A \cap B}(\omega) = I_A(\omega) I_B(\omega)$
    \item $I_{A \cup B}(\omega) = 1 - (1 - I_A(\omega))(1 - I_B(\omega))$.
\end{itemize}
More generally, for a countable set $A_i \in \sigalg$,
\begin{equation}
    I_{A_1 \cup A_2, \cdots}(\omega) = 1 - \prod_{i \in \N} (1 - I_{A_i}(\omega))
    \label{eqnUnionIndicator}
\end{equation}
Then equation~\ref{eqnUnionIndicator} can be expanded, and when expectations are taken we get the inclusion-exclusion formula.
\begin{proposition}
    Let $X$ be a random variable taking values in the set $\N_0$.
    \begin{equation*}
        E[X] = \sum_{k=1}^\infty P(X \geq k) = \sum_{k = 0}^\infty P(X > k)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        X(\omega) &= \sum_{k=1}^\infty I_{X(\omega) \geq k} \\
        &= \sum_{k=0}^\infty I_{X(\omega) > k}
    \end{align*}
    Then taking the expectation:
    \begin{equation*}
        E[X] = \sum_{k=1}^\infty P(X \geq k) = \sum_{k=0}^\infty P(X > k)
    \end{equation*}
\end{proof}
\subsection{Variance}
\begin{definition}{Moment of a Random Variable}
    For a random variable $X$, and a natural number $r$, the \underline{$r$th moment of $X$} is $E[X^r]$.
\end{definition}
\begin{definition}{Variance}
    The \underline{variance} of a random variable $X$ is defined:
    \begin{equation*}
        \Var{(X)} = E\left[(X - E[X])^2\right]
    \end{equation*}
\end{definition}
Variance is a measure of the spread of $X$. A low variance means $X$ is very concentrated around a single point.\par
The standard deviation is defined as the square root of the variance.\par
We have some properties. Consider $X$ a random variable and $c$ a real number:
\begin{itemize}
    \item If $\Var{(X)} = 0$ then $X$ takes a single value
    \item $\Var{(cX)} = c^2 \Var{(X)}$
    \item $\Var{(X + c)} = \Var{(X)}$
\end{itemize}
\begin{proposition}
    \begin{equation}
        \Var{(X)} = E[X^2] - \left(E[X]\right)^2
        \label{eqnAlternateVariance}
    \end{equation}
\end{proposition}
\begin{proof}
    \begin{align*}
        \Var{(X)} &= E\left[(X - E[X])^2\right] \\
        &= E\left[X^2 - 2XE[X] + (E[X])^2\right] \\
        &= E[X^2] - E[2XE[X]] + E\left[(E[X])^2\right] \\
        &= E[X^2] - 2E[X]E[X] + (E[X])^2 \\
        &= E[X^2] - (E[X])^2
    \end{align*}
\end{proof}
\begin{lemma}
    \begin{equation*}
        \Var{(X)} = \min_{c \in \R} E\left[(X - c)^2\right]
    \end{equation*}
    And is achieved when $c = E[X]$.
\end{lemma}
This lemma considers a quantity akin to the variance centred on multiple different points $c$. Then, the quantity is minimised when the centre is $E[X]$.
\begin{proof}
    Let $f(c) = E\left[(X - c)^2\right]$.
    \begin{align*}
        f(c) &= E[X^2] - 2cE[X] + c^2 \\
        f'(c) &= -2E[X] + 2c
    \end{align*}
    Which is zero when $c = E[X]$.
\end{proof}
We can calculate variances for different distributions.
\begin{example}[Variance of the binomial distribution]
    Consider $X \sim B(n, p)$.\par
    \begin{align*}
        E[X(X-1)] &= \sum_{k=0}^n k(k-1) P(X = k) \\
        &= \sum_{k=2}^n k(k-1) \choose{n}{k} p^k (1-p)^{n-k} \\
        &= \sum_{k=2}^n k(k-1) \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} \\
        &= \sum_{k=2}^n \frac{n(n-1)(n-2)!}{(k-2)!(n-k)!} p^{k-2} p^2 (1-p)^{n-k} \\
        &= \sum_{k=0}^{n-2} \choose{n-2}{k} p^{k} (1-p)^{n-2-k} p^2 n(n-1) \\
        &= p^2 n(n-1)
    \end{align*}
    So then the variance is:
    \begin{align*}
        \Var{(X)} &= p^2 n(n-1) + np - (np)^2 \\
        &= p^2 n^2 - p^2 n + np - n^2 p^2 \\
        &= np(1-p)
    \end{align*}
\end{example}
We have also that the variance of the Poisson distribution is $\lambda$, worked out similarly to the above.
\subsection{Covariance}
\begin{definition}{Covariance}
    Let $X$ and $Y$ be two random variables. The \underline{covariance} of $X$ and $Y$ is:
    \begin{equation}
        \Cov{(X, Y)} = E\left[(X - E[X])(Y - E[Y])\right]
        \label{eqnCovariance}
    \end{equation}
\end{definition}
We have some properties that follow immediately. Consider $X, Y$ and $Z$ random variables, and $c$ a real number.
\begin{enumerate}
    \item $\Cov{(X, Y)} = \Cov{(Y, X)}$
    \item $\Cov{(X, X)} = \Var{(X)}$
    \item $\Cov{(X, Y)} = E[XY] - E[X]E[Y]$.
    \item $\Cov{(cX, Y)} = c\Cov{(cX, Y)}$
    \item $\Cov{(X + c, Y)} = \Cov{(cX, Y)}$.
    \item $\Var{(X + Y)} = \Var{(X)} + \Var{(Y)} + 2\Cov{(X,Y)}$.
    \item $\Cov{(c, X)} = 0$
    \item $\Cov{(X + Y, Z)} = \Cov{(X, Z)} + \Cov{(Y, Z)}$
\end{enumerate}
An in general, for random variables $X_i$ and $Y_i$:
\begin{equation}
    \Var{\left(\sum_{i \in \N} X_i\right)} = \sum_{i=1}^N \Var{(X_i)} + \sum_{i \neq j}^N \Cov{(X_i, X_j)}
    \label{eqnVarianceSum}
\end{equation}
Therefore, equation~\ref{eqnVarianceSum} shows that for independent random variables, variance commutes with summation.
\begin{lemma}
    Let $X$ and $Y$ be independent random variables. Let $f, g : \R \mapsto \R$. Then $E[f(X)g(Y)] = E[f(X)]E[g(Y)]$.
\end{lemma}
\begin{proof}
    For any $x$ and $y$, $P(X = x, Y = y) = P(X = x)P(Y = y)$.
    \begin{align*}
        E[f(X)g(Y)] &= \sum_{(x, y)} f(x) g(y) P(X = x, Y = y) \\
        &= \sum_{(x, y)} f(x) g(y) P(X = x)P(Y = y) \\
        &= \sum_x gf(x) P(X = x) \sum_y g(y) P(Y = y) \\
        &= E[f(X)]E[g(Y)]
    \end{align*}
\end{proof}
Therefore if $X$ and $Y$ are independent, their covariance is 0. However, the converse is not true:
\begin{example}[0 covariance does not imply independence]
    Let $X_1, X_2, X_3$ be independent Bernoulli random variables with probability $\frac{1}{2}$. Let $Y_1 = 2X_1 - 1$, $Y_2 = 2X_2 - 1$, $Z_1 = Y_1 X_3$ and $Z_2 = Y_2 X_3$.\par
    Then the expectation of $Y_1$ and $Y_2$ are 0 (since they have equal probability of being $1$ and $-1$), and therefore so are the expectations of $Z_1$ and $Z_2$.
    \begin{align*}
        \Var{(Z_1, Z_2)} &= E[Z_1 Z_2] - E[Z_1] E[Z_2] \\
        &= 0 - 0 - 0 = 0
    \end{align*}
    So $Z_1$ and $Z_2$ have 0 covariance.\par
    However, $P(Z_1 = 0, Z_2 = 0) = P(X_3 = 0) = \frac{1}{2}$, but $P(Z_1 = 0)P(Z_2 = 0) = \frac{1}{4}$, so they are not independent.
\end{example}
\section{Important Inequalities}
\subsection{Markov's Inequality and Corollaries}
\begin{theorem}[Markov's Inequality]
    Let $X$ be a non-negative random variable. Let $a > 0$.\par
    \begin{equation*}
        P(X \geq a) \leq \frac{E[X]}{a}
    \end{equation*}
\end{theorem}
\begin{proof}
    For any $a > 0$, $X \geq a I_{X \geq a}$. Then taking expectations:
    \begin{equation*}
        E[X] \geq a P(X \geq a)
    \end{equation*}
    And rearranging gives the result.
\end{proof}
\begin{lemma}[Chebyshev's Inequality]
    Let $X$ be a random variable with finite expectation and variance. Then for any $a > 0$,
    \begin{equation*}
        P(|X - E[X]| \geq a) \leq \frac{\Var{(X)}}{a^2}
    \end{equation*}
\end{lemma}
\begin{proof}
    \begin{align*}
        P(|X - E[x]|^2 \geq a^2) &\leq \frac{E[(X - E[X]^2)]}{a^2} \\
        &= \frac{\Var{(X)}}{a^2}
    \end{align*}
\end{proof}
\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $X$ and $Y$ be random variables. Then:
    \begin{equation*}
        E[|XY|] \leq \sqrt{E[X^2]E[Y^2]}
    \end{equation*}
\end{theorem}
\begin{proof}
    Assume that $E[X^2]$ and $E[Y^2]$ are finite, otherwise the statement is trivial.
    \begin{equation*}
        |XY| \leq \frac{1}{2} \left(X^2 + Y^2\right)
    \end{equation*}
    and then taking expectations,
    \begin{equation*}
        E[|XY|] \leq \frac{1}{2}\left(E[X^2] + E[Y^2]\right)
    \end{equation*}
    Re-label $X$ to be $|X|$ and $Y$ to be $Y$, so now $X$ and $Y$ are non-negative. Let $t \in \R$:
    \begin{align*}
        0 &\leq E\left[(X - tY)^2\right] \\
        &= E[X^2] - 2tE[XY] + t^2 E[Y^2]
    \end{align*}
    Then considering this as a quadratic, we require $b^2 - 4ac \leq 0$:
    \begin{align*}
        0 &\geq 4E[XY]^2 - 4E[X^2] E[Y^2] \\
        0 &\geq E[XY]^2 - E[X^2] E[Y^2] \\
        E[XY] &\leq \sqrt{E[X^2]E[Y^2]}
    \end{align*}
\end{proof}
\begin{remark}
    As in IA Vectors and Matrices, the case for equality is that one random variable is a multiple of the other (or that one is zero).
\end{remark}
\subsection{Jensen's Inequality}
\begin{definition}{Convex function}
    A function $f : \R \mapsto \R$ is \underline{convex} if for any $x$ and $y$, and $t \in [0, 1]$,
    \begin{equation*}
        f(tx + (1-t)y) \leq tf(x) + (1-t) f(y)
    \end{equation*}
    That is, $f$ is below any chord that joins $x$ and $y$, where $t$ parameterises the chord. See figure~\ref{figConvexCurve}.
\end{definition}
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[domain=0:2] plot (\x, \x * \x * \x * \x -4 * \x * \x * \x +6 * \x * \x -11/3 * \x +1) node[right] {$f(x)$};
        \coordinate (X) at (0.3, 0.34);
        \coordinate (Y) at (1.4, 0.492);
        \draw[fill] (X) circle[radius=0.5mm] node[below left] {$(x, f(x))$};
        \draw[fill] (Y) circle[radius=0.5mm] node[below right] {$(y, f(y))$};
        \draw (X) -- (Y);
        \draw[->] (0.5, -0.5) node[below, align=center] {Curve below \\ chord}
            -- ($(X) + (0.5, 0)$);
    \end{tikzpicture}
    \caption{Diagram of a convex function and a chord}
    \label{figConvexCurve}
\end{figure}
\begin{lemma}
    Let $f$ be a convex function. Then $f$ is the supremum of all the lines lying below it. In other words, for any $m$ there exist real numbers $a$ and $b$ such that $f(m) = am+b$ and $f(x) \geq ax + b~\forall x$.
    \label{lemTangentBelowConvex}
\end{lemma}
This means that for a point and corresponding tangent line, $f(x)$ is above this tangent line everywhere other than the point where they touch.
\begin{proof}
    Let $m \in \R$ and $x < m < y$. Then:
    \begin{equation}
        m = tx + (1-t)y
        \label{eqnInterpolation}
    \end{equation}
    for some $t \in [0, 1]$.\par
    $f(m) = f(tx + (1-t)y) \leq tf(x) + (1-t)f(y)$ from the definition.
    \begin{align*}
        \implies f(m) &\leq  tf(x) + (1-t)f(y) \\
        (1 - t) f(m) + tf(m) &\leq tf(x) + (1-t)f(y) \\
        t(f(m) - f(x)) &\leq (1-t)(f(y)-f(m))
    \end{align*}
    Then rearranging equation~\ref{eqnInterpolation}, $t = \frac{y - m}{y - x}$ and $1 - t = \frac{m - x}{y - x}$. Substituting this in the inequality gives:
    \begin{equation}
        \frac{f(m) - f(x)}{m - x} \leq \frac{f(y) - f(m)}{y-m}
        \label{eqnRatioInequality}
    \end{equation}
    Now define $a = \sup_{x < m} \frac{f(m) - f(x)}{m - x}$. Therefore, for all $x < m < y$, equation~\ref{eqnRatioInequality} gives:
    \begin{equation}
        \frac{f(m) - f(x)}{m - x} \leq a \leq \frac{f(y) - f(m)}{y - m}
        \label{eqnSupremumBounds}
    \end{equation}
    Then rearranging this inequality, for any $z$ in range:
    \begin{equation*}
        f(z) \geq a(z - m) + f(m)
    \end{equation*}
    As required. This is from the LHS of equation~\ref{eqnSupremumBounds} if $z < m$, and the RHS if $z > m$.
\end{proof}
\begin{theorem}[Jensen's Inequality]
    Let $f$ be convex and $X$ a random variable.\par
    Then $E[f(x)] \geq f(E[X])$.
    \label{thmJensenInequality}
\end{theorem}
\begin{proof}
    Let $m = E[X]$. By proposition~\ref{lemTangentBelowConvex}, there exist $a$ and $b$ such that
    \begin{equation*}
        f(m) = am + b, f(x) \geq ax + b~\forall x
    \end{equation*}
    Then $f(X) \geq aX + b$. Taking expectations:
    \begin{align*}
        E[f(X)] &\geq aE[X] + b \\
        &= am + b = f(m) \\
        &= f(E[X])
    \end{align*}
\end{proof}
\begin{corollary}[Equality in Jensen's Inequality]
    Let $X$ be a random variable and $f$ a convex function with the property that for $m = E[X]$ there exist $a$ and $b$ such that $f(m) = am + b$ and $f(x) > ax + b$ for any $x \neq m$.\par
    Then $f(E[X]) = E[f(X)]$ only if $X$ is constant.
    \label{corJensenEquality}
\end{corollary}
\begin{remark}
    This final statement can be extended to an \textit{if and only if} statement, since if $X$ is constant, then $X = E[X]$ and equality is trivially true since $f(X)$ is also constant.
\end{remark}
\begin{proof}
    We then consider what properties of $X$ give $E[f(X)] = f(E[X])$.\par
    We have that $f(X) \geq aX + b$, which means $f(X) - (aX + b) \geq 0$. Taking expectations,
    \begin{equation*}
        E[f(X)] \geq a E[X] + b = am + b = f(m) = f(E[X])
    \end{equation*}
    And for equality,
    \begin{align*}
        E[f(X)] &= f(E[X]) \\
        E[f(X) - (aX + b)] &= 0
    \end{align*}
    However, since $f(X) - (aX + b)$ is greater than or equal to zero, for it to have zero expectation it must be zero everywhere. That is, $P(f(X) = aX + b) = 1 \implies P(X = m) = 1$. That is, $X$ is constant.
\end{proof}
\begin{corollary}
    Let $f$ be a convex function and $x_1, \cdots, x_n$ be real numbers. Then:
    \begin{equation*}
        \frac{1}{n} \sum_{k=1}^n f(x_k) \geq f\left(\frac{1}{n}\sum_{k=1}^n x_k\right)
    \end{equation*}
    \label{corGenericAMGM}
\end{corollary}
\begin{proof}
    Let $X$ be a random variable taking values $x_1, \cdots, x_n$. Let $P(X = x_i) = \frac{1}{n}$
    \begin{align*}
        \frac{1}{n} \sum_{k=1}^n f(x_k) &= \sum_{k=1}^n P(X = x_k) f(x_k) \\
        &= E[f(X)]
    \end{align*}
    Note also that $E[X] = \frac{1}{n} \sum_{k=1}^n x_n$.
    Then apply Jensen's Inequality:
    \begin{align*}
        E[f(X)] &\geq f(E[X]) \\
        \frac{1}{n} \sum_{k=1}^n f(x_k) & \geq f\left(\frac{1}{n} \sum_{k=1}^n x_k\right)
    \end{align*}
\end{proof}
\begin{corollary}[AM-GM Inequality]
    Let $f(x) = -\log{(x)}$. This is a convex function.\par
    \begin{align*}
        -\frac{1}{n} \sum_{k=1}^n \log{x_k} & \geq -\log{(\frac{1}{n}\sum_{k=1}^n x_k)} \\
       \implies \left(\prod_{k=1}^n x_k\right)^{\frac{1}{n}} &\geq \frac{1}{n} \sum_{k=1}^n x_k \text{ by exponentiation}
    \end{align*}
    \label{corAMGM}
\end{corollary}
\section{Conditional Expectation}
\begin{definition}{Conditional expectation on an event}
    Let $X$ be a random variable, $B$ an event, and $P(B) > 0$. Then we define the \underline{conditional expectation}:
    \begin{equation}
        E[X | B] = \frac{E[X \times I_B]}{P(B)}
        \label{eqnConditionalExpec}
    \end{equation}
\end{definition}
\begin{lemma}[Law of Total Expectation]
    Let $X$ be a random variable and $(\Omega_n)$ be a partition of $\Omega$. Suppose that $P(\Omega_n) > 0$ for all $n$. Then:
    \begin{equation*}
        E[X] = \sum_n E[X | \Omega_n] \times P(\Omega_n)
    \end{equation*}
\end{lemma}
\begin{proof}
    Let $X = XI_\Omega$:
    \begin{align*}
        X &= XI_{\cup_n \Omega_n} \\
        &= \sum_n XI_{\Omega_n} \text{ as the } \Omega_n \text{ are disjoint}
    \end{align*}
    Taking expectations:
    \begin{align*}
        E[X] &= \sum_n E[XI_{\Omega_n}] \\
        &= \sum_n E[X | \Omega_n] P(\Omega_n) \text{ by rearranging equation~\ref{eqnConditionalExpec}}
    \end{align*}
\end{proof}
\begin{definition}{Joint distribution}
    Let $X_1, \cdots, X_n$ be discrete random variables. Their \underline{joint distribution} is defined to be:
    \begin{equation*}
        P(X_1 = x_1, \cdots, X_n = x_n)~\forall x_1, \cdots, x_n
    \end{equation*}
    That is, a single distribution that encompasses all the random variables.
\end{definition}
\begin{definition}{Marginal distribution}
    Given a joint distribution, the \underline{marginal distribution} is the distribution of only one of the random variables. It is obtained by summing probabilities over all the values of the other variables:
    \begin{equation*}
        P(X_1 = x_1) = \sum_{x_2, \cdots, x_n} P(X_1 = x_2, X_2 = x_2, \cdots, X_n = x_n)
    \end{equation*}
\end{definition}
\begin{definition}{Conditional distribution}
    Let $X$ and $Y$ be two random variables. Then the \underline{conditional distribution} of $X$ given $Y = y$ is defined to be:
    \begin{equation*}
        P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)}~\forall x
    \end{equation*}
\end{definition}
By the law of total probability,
\begin{equation*}
    P(X = x) = \sum_y P(X = x | Y = y) P(Y = y)
\end{equation*}
\begin{proposition}[Convolutions]
    Consider two independent discrete random variables $X$ and $Y$. Then the probability distribution given by their sum is a \underline{convolution}:
    \begin{align*}
        P(X + Y = z) &= \sum_y P(Y = y) P(X = z - y) \\
        \text{ or } P(X + Y = z) &= \sum_x P(X = x) P(Y = z - x)
    \end{align*}
\end{proposition}
\begin{proof}
    \begin{align*}
        P(X + Y = z) &= \sum_x P(X + Y = z, X = x) \\
        &= \sum_x P(Y = z - x, X = x) \\
        &= \sum_x P(X = x) P(Y = z - x) \text{ by independence}
    \end{align*}
    And the second formula is obtained by summing over $y$ instead of $x$.
\end{proof}
\begin{example}
    Let $X \sim Po(\lambda)$ and $Y \sim Po(\mu)$. Let them be independent.\par
    Then consider $P(X + Y = z)$.\par
    \begin{align*}
        P(X + Y = n) &= \sum_{k = 0}^n P(X + y = n, X = k) \\
        &= \sum_{k = 0}^n P(X = k) P(Y = n-k) \\
        &= \sum_{k = 0}^n e^{-\lambda} \frac{\lambda^k}{k!} e^{-\mu} \frac{\mu^{n-k}}{(n-k)!} \\
        &= \frac{e^{-(\lambda + \mu)}}{n!} \sum_{k = 0}^n \choose{n}{k} \mu^{n-k} \lambda^k \\
        &= \frac{e^{-(\lambda + \mu)}}{n!} (\lambda + \mu)^n
    \end{align*}
    And so $X + y \sim Po(\lambda + \mu)$.\par
    In general, adding two random variables is not so easy.
\end{example}
\begin{definition}{Conditional distribution}
    Let $X$ and $Y$ be random variables. The \underline{conditional distribution} of $X$ given $Y = y$ is:
    \begin{equation*}
        P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)}
    \end{equation*}
\end{definition}
Consider again $X$ and $Y$, and the event $Y = y$. Then the conditional expectation of $X$ given $Y = y$ is:
\begin{align*}
    E[X | Y = y] &= \frac{E[X I_{Y = y}]}{P(Y = y)} \\
    &= \sum_x \frac{xP(X = x, Y = y)}{P(Y = y)} \\
    &= \sum_x x P(X = x | Y = y)
\end{align*}    
Note that $E[X | Y = y]$ is only a function of $y$. Let this function be $g(y)$
We now want to define the conditional expectation given a random variable:
\begin{definition}{Conditional expectation given a random variable}
    Let $X$ and $Y$ be random variables. The \underline{conditional expectation} of $X$ given $Y$ is:
    \begin{equation*}
        E[X | Y] = g(Y)
    \end{equation*}
    where $g$ was defined above.
\end{definition}
Note that $E[X | Y]$ is a random variable, not a number. This is because it is a function of $Y$. We can alternatively write this as:
\begin{align*}
    E[X | Y] &= g(Y) \times 1 \\
    &= g(Y) \sum_y I_{Y = y} \\
    &= \sum_y g(Y) I_{Y = y} \\
    &= \sum_y g(y) I_{Y = y}
\end{align*}
\begin{example}
    \label{exCoinTossCondExpec}
    Consider tossing a coin, probability $p$ of heads, $n$ times independently. Set $X_i = I_{\text{toss } i \text{ is a head}}$.\par
    Let $Y_n$ be the total number of heads, $Y_n = X_1 + \cdots + X_n$.\par
    Then consider $E[X_1 | Y_n]$:
    \begin{align*}
        E[X_1 | Y_n] &= \sum_y g(y) I_{Y_n = y} \\
        \text{where } g(y) &= E[X_1 | Y_n = y], y \in \{1, \cdots, n\} \\
        &= 0 \times P(X_1 = 0 | Y_n = y) + 1 \times P(X_1 = 1 | Y_n = y) \\
        &= P(X_1 = 1 | Y_n = y) \\
        &= \frac{P(X_1 = 1, Y_n = y)}{P(Y_n = y)} \\
        &= \frac{p \choose{n-1}{y-1} p^{y-1} (1-p)^{n-y}}{\choose{n}{y} p^y (1-p)^{n-y}} \\
        &= \frac{y}{n}.
    \end{align*}
    So $g(y) = \frac{y}{n}$ and therefore $E[X_1 | Y_n] = g(Y_n) = \frac{Y_n}{n}$.
\end{example}
\begin{propositions}[Properties of conditional expectation] {
        Let $X$ and $Y$ be random variables. Let $c$ be a real number.
        \label{propsCondExpecProperties}
    }
    \item $E[cX | Y] = cE[X | Y]$, and $E[c | Y] = c$. \label{propCondExpecConstant}
    \item If $X_1, \cdots, X_n$ are random variables,
        \begin{equation*}
            E\left[\sum_{i=1}^n X_i | Y\right] = \sum_{i=1}^n E[X_i | Y]
        \end{equation*}
        \label{propCondExpecSum}
    \item $E[E[X | Y]] = E[X]$ \label{propCondExpecTwice}
    \item If $X$ and $Y$ are independent, $E[X | Y] = E[X]$. \label{propCondExpecIndep}
    \item If $Z$ is also a random variable, independent of $Y$, then:
        \begin{equation*}
            E\left[E[X | Y] | Z\right] = E[X]
        \end{equation*}
        \label{propCondExpecSecondIndepRV}
    \item Let $h : \R \mapsto \R$. Then $E[h(Y) X | Y] = h(Y) E[X | Y]$. \label{propCondExpecFactorOutFunc}
    \item $E[E[X | Y] | Y] = E[X | Y]$ and $E[X | X] = X$. \label{propCondExpecTwiceGiven}
\end{propositions}
\begin{proof}[Of \ref{propCondExpecTwice} to \ref{propCondExpecTwiceGiven}]
    \begin{enumerate}
        \setcounter{enumi}{2} % Start at 3
        \item The expectation of a conditional expectation is $E[X]$:
            \begin{equation*}
                E[X | Y] = \sum_y I_{Y = y} g(y)
            \end{equation*}
            Where $g(y) = E[X | Y = y]$. Then:
            \begin{align*}
                E[E[X | Y]] &= \sum_y g(y) P(Y = y) \\
                &= \sum_y E[X | Y = y] P(Y = y) \\
                &= \sum_y E[X \times I_{Y = y}] \\
                &= E[X]
            \end{align*}
        \item Expectation given an independent variable is $E[X]$:
            \begin{align*}
                E[X | Y] &= g(Y) = \sum_y g(y) I_{Y = y} \\
                &= \sum_y E[X | Y = y] I_{Y = y} \\
                &= \sum_y \left(\sum_x x P(X = x, Y = y)\right) I_{Y = y} \\
                &= \sum_y \left(\sum_x x P(X = x)\right) I_{Y = y} \\
                &= \sum_y E[X] I_{Y = y} \\
                &= E[X]
            \end{align*}
        \item Conditional expectation given a second independent random variable is $E[X]$:
            The claim is equivalent to showing that $g(Y) = E[X | Y]$ is independent of $Z$:
            \begin{align*}
                P(g(Y) = w, Z = z) &= \sum_{y \in g^{-1}(\{w\})} P(Y = y, Z = z) \\
                &= \sum_{y \in g^{-1}(\{w\})}
            \end{align*}
            And this is independent of $Z$, so by proposition~\ref{propCondExpecIndep}, we have that:
            \begin{equation*}
                E[E[X | Y] | Z] = E[E[X | Y]] = E[X] \text{ by proposition~\ref{propCondExpecTwice}}
            \end{equation*}
        \item \begin{align*}
                E[h(Y) X | Y] &= \sum_y I_{Y = y} E[h(Y) X | Y = y] \\
                &= \sum_y I_{Y = y} E[h(y) X | Y = y] \\
                &= \sum_y I_{Y = y} h(y) E[X | Y = y] \\
                &= h(Y) E[X | Y]
            \end{align*}
        \item This follows immediately from \ref{propCondExpecFactorOutFunc}, factoring out the function of $Y$, $E[X | Y]$, in the first case, and the function of $X$, $X$, in the second case.
    \end{enumerate}
\end{proof}
\begin{example}[Example~\ref{exCoinTossCondExpec} revisited]
    Consider again example~\ref{exCoinTossCondExpec}. We can now reach the result much more easily.\par
    By symmetry, $E[X_i | Y_n]$ must all be the same:
    \begin{align*}
        E[X_1 + X_2 + \cdots + X_n | Y_n] &= n E[X_1 | Y_n] \\
        E[Y_n | Y_n] &= n E[X_1 | Y_n] \\
        \therefore E[X_1 | Y_n] &= \frac{Y_n}{n}
    \end{align*}
\end{example}
\section{Random Walks}
\begin{definition}{Stochastic process}
    A stochastic process is a sequence of random variables, that are not necessarily independent.    
\end{definition}
Stochastic is simply another word for random.
\begin{definition}{Random walk}
    A \underline{random walk} is a random process that can be expressed by:
    \begin{equation*}
        X_n = x + Y_1 + \cdots + Y_n
    \end{equation*}
    Where $x$ is a number, $Y_i$ are independent and identically distributed random variables. $Y_i$ are known as the \underline{steps} of the random walk.
\end{definition}
Consider a simple random walk, wherein $Y_i$ is defined, with parameter $p$, by:
\begin{equation*}
    Y_i =
    \begin{cases}
        1 & \text{ probability } p \\
        -1 & \text{ probability } 1-p
    \end{cases}
\end{equation*}
\subsection{Extended Example: Gambler's Ruin}
A gambler plays a game where they can double their money, with probability $p$, or lose their money, with probability $1-p$. They start with £$x$ and each time bets £1. They stop when either they run out of money, or when they reach £$a$.\par
This can be modelled using a random walk.\par
First consider the probability that the gambler reaches £$a$ before reaching $0$, given their start point of £$x$. Let:
\begin{equation*}
    h(x) = P((X_n) \text{ reaches } a \text{ before reaching } 0~|~X_0 = x)
\end{equation*}
Denote $P(A | X_0 = x)$ as $P_x(A)$ for some event $A$.\par
Note that $h(a) = 1$, and $h(0) = 0$.\par
Also, for convenience, let $W$ be the event that $X$ hits $a$ before $0$ (a win).\par
Split the probability based on the value of $Y_1$:
\begin{align*}
    h(x) &= P_x(W) \\
    &= P_x(W | Y_1 = 1) + P_x(W | Y_1 = -1)\\
    &= P_{x+1}(W)P(Y_1 = 1)+ P_{x-1}(W)P(Y_1 = -1) \\
    &= h(x + 1)p + h(x - 1)(1 - p)
\end{align*}
Which gives a difference equation.\par
Consider the case $p = \frac{1}{2}$. This is a simple symmetric random walk.
\begin{align*}
    h(x) &= \frac{1}{2} h(x + 1) + \frac{1}{2} h(x - 1) \\
    h(x) - h(x - 1) &= h(x + 1) - h(x) \\
    h(x) &= \sum_{i=1}^x (h(i) - h(i - 1)) \\
    &= cx \text{ for some } c
\end{align*}
Then by boundary conditions, $c = \frac{1}{a}$ and $h(x) = \frac{x}{a}$.\par
Now consider the general case. Try a solution of the form $h(x) = \lambda^x$ in the difference equation:
\begin{align*}
    &\lambda^x = p \lambda^{x + 1} + (1 - p) \lambda^{x - 1} \\
    &\implies \lambda = p\lambda^2 + (1 - p) \\
    &\implies \lambda = 1 \text{ or } \frac{1 - p}{p}
\end{align*}
Reject the solution $\lambda = 1$ and apply the boundary conditions:
\begin{equation*}
    h(x) = \frac{\left(\frac{1-p}{p}\right)^x - 1}{\left(\frac{1-p}{p}\right)^a - 1}
\end{equation*}
\subsection{Time to Absorption}
\begin{definition}{Time to absorption}
    In a bounded random walk, with bounds $0$ and $a$, define the \\\underline{time to absorption} as:
    \begin{equation*}
        T = \min{\subsetselect{n \geq 0}{X_n \in \{0, a\}}}
    \end{equation*}
\end{definition}
Then we want to find, for a simple random walk, $E_x[T] = E[T | X_0 = x]$. Let this be $k(x)$.
\begin{align*}
    E_x[T] &= E_x[T | Y_1 = 1]P_x(Y_1 = 1) + E_x[T | Y_1 = -1]P_x(Y_1 = -1) \\
    &= E_x[T | Y_1 = 1] \times p + E_x[T | Y_1 = -1] \times (1-p) \\
    &= p(1 + k(x + 1)) + (1 - p)(1 + k(x - 1)) \\
    &= 1 + pk(x + 1) + (1-p)k(x - 1)
\end{align*}
This gives another difference equation. We have the boundary conditions $k(0) = k(a) = 0$.\par
Consider first the case $p = \frac{1}{2}$. Then the difference equation is:
\begin{equation*}
    k(x) = 1 + \frac{1}{2}k(x + 1) + \frac{1}{2}k(x - 1)
\end{equation*}
Then consider a solution of the form $Ax^2$. This gives the result that $k(x) = x(a - x)$.\par
Then consider $p \neq \frac{1}{2}$. Try a solution of the form $Cx$, and this gives the result that:
\begin{equation*}
    k(x) = \frac{1}{1 - 2p} x - \frac{1 - p}{1 - 2p}\frac{\left(\frac{1-p}{p}\right)^x - 1}{\left(\frac{1-p}{p}\right)^a - 1}
\end{equation*}
\begin{remark}
    Note the key idea here: each step, we can consider the following steps as a new random walk, starting from the new position. This is the defining idea of a Markov Chain. See the course IB Markov Chains.
\end{remark}
\section{Probability Generating Functions}
We have already seen the probability mass function for a random variable, the function that takes a possible value of the random variable as input, and returns as output its probability.
\begin{definition}{Probability generating function}
    For a random variable $X$, the \underline{probability generating function} is defined to be:
    \begin{equation*}
        p(z) = \sum_{r = 0}^\infty p_r z^r = E[z^X], |z| \leq 1
    \end{equation*}
    where $p_r = P(X = r)$. Note that this displays absolute convergence for any $|z| \leq 1$.
\end{definition}
\begin{theorem}
    The probability generating function uniquely determines the distribution of $X$.
\end{theorem}
\begin{proof}
    Suppose $(p_r)$ are two probability distributions with the same PGFs:
    \begin{equation}
        \forall |z| \leq 1, \sum_{r = 0}^\infty p_r z^r = \sum_{r = 0}^\infty q_r z^r
        \label{eqnEqualPGFs}
    \end{equation}
    \induction{$r = 0$}{
        Set $z = 0$. Then equation~\ref{eqnEqualPGFs} gives $p_0 = q_0$.
    }{$r \leq k$}{}
    {$r = k + 1$}{
        We have, by assumption, that
        \begin{equation*}
            \sum_{r = k + 1}^{\infty} p_r z^r = \sum_{r = k + 1}^{\infty} q_r z^r
        \end{equation*}
        Therefore divide by $z^{n+1}$ and take the limit as $z \to 0$. This gives $p_{k + 1} = q_{k + 1}$.
    }
\end{proof}
\begin{example}
    Let $X \sim B(n, p)$.\par
    Then the probability generating function is:
    \begin{align*}
        p(z) &= \sum_{r = 0}^n z^r \choose{n}{r} p^r (1-p)^{n-r} \\
        &= (zp + 1 - p)^n
    \end{align*}
\end{example}
\begin{proposition}
    Suppose that $(X_i)$ are a set of independent random variables, with PGFs $q_i(z)$.\par
    Then the PGF of their sum is:
    \begin{equation*}
        q(z) = \prod_i q_i(z)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        q(z) &= E[z^{X_1 + X_2 + \cdots + X_n}] \\
        &= E[z^{X_1} \times \cdots \times z^{X_n}] \\
        &= q_1(z) \times \cdots \times q_n(z)
    \end{align*}
\end{proof}
\begin{example}[Geometric distribution]
    Consider $X \sim Geo(p)$. Now consider $E[z^X]$:
    \begin{align*}
        E[z^X] &= \sum_{r=0}^\infty z^r (1-p)^r p \\
        &= \frac{p}{1-z(1-p)}
    \end{align*}
\end{example}
\begin{example}[Poisson distribution]
    Consider $X \sim Po(\lambda)$.
    \begin{align*}
        E[z^X] &= \sum_{k=0}^\infty z^k e^{-\lambda} \frac{\lambda^k}{k!} \\
        &= e^{-\lambda} e^{\lambda z} \\
        &= e^{\lambda (z-1)}
    \end{align*}
\end{example}
\begin{theorem}
    Given a random variable $X$, and probability generating function $p(z)$,
    \begin{equation*}
        \lim_{z \to 1-} p'(z) = E[X]
    \end{equation*}
\end{theorem}
\begin{proof}
    \begin{case}{$X$ has finite expectation.}
        Let $0 < z < 1$.
        \begin{align*}
            p'(z) &= \sum_{r=1}^\infty r p_r z^{r-1} \\
            ^\leq \sum_{r=1}^\infty rp_r = E[X]
        \end{align*}
        Note also that $p'(z)$ is an increasing function in $z$\par
        Therefore $\lim_{z \to 1^-} \leq E[X]$.\par
        Let $N > 0$. For any $\epsilon > 0$, let $N$ be such that:
        \begin{equation*}
            \sum_{r=1}^N rp_r \geq E[X] - \epsilon
        \end{equation*}
        Also $p'(z) \geq \sum_{r=1}^N rp_r ^{n-1}$\par
        \begin{equation*}
            \lim_{z \to 1^-} p'(z) \geq \sum_{r=1}^N rp_r \geq E[X] - \epsilon
        \end{equation*}
        Since this holds for any $\epsilon$,
        \begin{equation*}
            \lim_{z \to 1^-} p'(z) = E[X]
        \end{equation*}
    \end{case}
    \begin{case}{$X$ has infinite expectation.}
        In this case, for any $M$ there exists $N$ such that:
        \begin{equation*}
            \sum_{r=1}^N \geq M
        \end{equation*}
        And therefore, by the above,
        \begin{align*}
            \lim_{z \to 1^-} p'(z) &\geq \lim_{z \to 1^-} \sum_{r=1}^N rp_r z^{r-1} \\
            &=\sum_{r=1}^N rp_r \geq M 
        \end{align*}
        And therefore $\lim_{z \to 1-} = \infty$.
    \end{case}
\end{proof}
\begin{theorem}
    \begin{equation*}
        \lim_{z \to 1^-} p''(z) = E[X(X-1)]
    \end{equation*}
\end{theorem}
The proof is as above. More generally,
\begin{equation}
    \lim_{z \to 1^-} p^{(k)}(z) = E[X(X-1)(X-2) \cdots(X-k+1)]
    \label{eqnPGFDerivative}
\end{equation}
And therefore $\Var{(X)} = lim_{z \to 1^-} \left(p''(z) + p'(z) - (p'(z))^2\right)$.\par
Note also that we can recover $p_n$:
\begin{equation}
    P(X = n) = \frac{1}{n!} \frac{d^n p(z)}{dz^n} |_{z = 0}
    \label{eqnPGFProb}
\end{equation}
\begin{proposition}
    Let $X_1, \cdots, X_n$ be independent identically distributed random variables. Let $N$ be an independent random variable with values in $\N$. Define:
    \begin{align*}
        S_n &= \sum_{i=1}^n X_i \\
        S_N &= \sum_{i=1}^N X_i \\
        S_n(\omega) &= \sum_{i=1}^{N(\omega)} X_i(\omega)
    \end{align*}
    Let $q$ be the probability generating function of $N$, $q(z) = E[z^N]$; let $p$ be the probability generating function of $X_i$ (note that these all have the same distribution).
    Then the probability generating function of $S_N$, $r(z)$ is:
    \begin{equation*}
        r(z) = qp(z)
    \end{equation*}
\end{proposition}
\begin{proof}
    \begin{align*}
        r(z) &= E[z^{S_N}] = \sum_{n=0}^\infty E[z^{S_N} I_{N=n}] \\
        &= \sum_{n=0}^\infty E[z^{X_1 + X_2 + \cdots + X_n} I_{N=n}] \\
        &= \sum_{n=0}^\infty E[z^{X_1 + X_2 + \cdots + X_n}]P(N=n) \\
        &= \sum_{n=0}^\infty \left(p(z)\right)^n P(N=n) \\
        &= E[p(z)^N] \\
        &= q(p(z))
    \end{align*}
    So $r(z) = qp(z)$.
\end{proof}
We can provide an alternate proof:
\begin{proof}
    \begin{align*}
        r(z) &= E[z^{S_n}] \\
        &= E[E[z^{S_n} | N]]
    \end{align*}
    Now consider $E[Z^{S_n} | N] = g(N)$:
    \begin{align*}
        g(n) &= E[z^{S_N}|N=n] \\
        &= E[z^{S_n}|N=n] \\
        &= E[z^{S_N}]
    \end{align*}
    by independence. Therefore, we get the expectation:
    \begin{align*}
        E[z^{S_N} | N] &= g(N) = (p(z))^n \\
        r(z) &= q(p(z))
    \end{align*}
\end{proof}
Now we have the probability generating function $r(z)$, we can evaluate probabilities:
\begin{align*}
    E[S_N] &= \lim_{z \to 1^-} r'(z) \\
    &= \lim_{z \to 1^-} q'(p(z)) p'(z) \\
    &= E[N] E[X_i]
\end{align*}
We can also get variance:
\begin{equation*}
    \Var{(S_N)} = E[N] \Var{(X_i)} + \Var{(N)} \left(E[X_i]\right)^2
\end{equation*}
\subsection{Branching Processes}
Consider a model of population growth. Let $X_n$ be the number of individuals in each generation. Let $X_0 = 1$. Then each individual independently produces a random number of offspring, with distribution $g_k$. Therefore, $P(X_1 = k) = g_k$, since there is one individual in generation $0$.\par
Define:
\begin{equation*}
    \left(Y_{k, n}~|~k \geq 1, n \geq 0\right)
\end{equation*}
Then $Y_{k, n}$ is the number of offspring that the $k$th individual in generation $n$ produces. All of these are independent, and with distribution $g_k$. Then the relation between generations is:
\begin{equation*}
    X_{n + 1} = Y_{1, n} + \cdots + Y_{{X_n}, n}
\end{equation*}
Note that if $X_n = 0$, all subsequent generations have $X_m = 0$ (the population dies out).
\begin{theorem}
    \begin{equation*}
        E[X_n] = \left(E[X_1]\right)^n
    \end{equation*}
    \label{thmBranchingExpectation}
\end{theorem}
\begin{proof}
    \begin{align*}
        E[X_{n + 1}] &= E[E[X_{n + 1} | X_n]] \\
        E[X_{n + 1} | X_n = m] &= E[Y_{1, n} + \cdots + Y_{{X_n}, m} | X_n = m] \\
        &= E[Y_{1, n} + \cdots + Y_{m, n}] \\
        &= mE[X_1]
    \end{align*}
    Then
    \begin{align*}
        E[X_{n + 1} | X_n] &= X_n E[X_1] \\
        \therefore E[X_{n + 1}] &= E[X_n E[X_1]] = E[X_1] E[X_n] \\
        \therefore E[X_{n + 1}] &= \left(E[X_1]\right)^{n + 1} \text{ by chaining the formula}
    \end{align*}
\end{proof}
\begin{theorem}
    Let $G(z) = E[z^{X_1}]$ and $G_n(z) = E[z^{X_n}]$.\par
    Then $G_{n + 1}(z) = G(G_n(z))$.
    \label{thmBranchingGenFunc}
\end{theorem}
\begin{proof}
    \begin{align*}
        G_{n + 1}(z) &= E[z^{X_{n + 1}}] \\
        &= E[E[z^{X_{n+1}} | X_n]]
    \end{align*}
    \begin{align*}
        E[z^{X_{n + 1}} | Z_n = m] &= E[z^{Y_{1, n} + \cdots + Y_{X_n, 1}} | X_n = m] \\
        &= E[z^{Y_1 + \cdots + Y_m} | X_n = m] \\
        &= \left(E[z^{X_1}]\right)^m
    \end{align*}
    and therefore $E[Z^{X_{n + 1}} | X_n] = \left(G(z)\right)^{X_n}$
    \begin{equation*}
        G_{n + 1}(z) = E[\left(G(z)\right)^{X_n}] = G_n(G(z))
    \end{equation*}
\end{proof}
Now consider the probability that the population goes extinct. Set:
\begin{equation*}
    q = P(X_n = 0 \text{ for some } n \geq 1)
\end{equation*}
and $q_n = P(X_n = 0)$. Therefore, $A_n = \{X_n = 0\}$ is a subset of $A_{n + 1}$. Now we have an increasing family of events, and $q_n \to q$ as $n \to \infty$.
\begin{proposition}
    $q_{n + 1} = G(q_n)$, where $G$ is as defined before: $G(z) = E[z^{X_1}]$.
    \label{propExtinctionProb}
\end{proposition}
\begin{proof}[by conditioning on $X_n$]
    \begin{align*}
        q_{n + 1} &= P(X_{n + 1} = 0) \\
        &= G_{n + 1}(0) \text{ by expanding out the expectation function} \\
        &= G(G_n(0)) \text{ by theorem~\ref{thmBranchingGenFunc}}
    \end{align*}
\end{proof}
\begin{proof}[by conditioning on $X_1$]
    We condition on $X_1 = m$. Let $X_n^{(1)}, \cdots, X_n^{(m)}$ be independent and identically distributed branching processes with the same offspring distribution as $X_1$. We can do this because the offspring from the second generation can be thought of as another independent branching process as before.\par
    Note also that $X_{n + 1} = X_n^{(1)} + \cdots + X_n^{(m)}$ given $X_1 = m$\par
    \begin{align*}
        q_{n + 1} &= P(X_{n + 1} = 0) \\
        &= \sum_m P(X_{n + 1} = 0 | X_1 = m)P(X_1 = m) \\
        &= \sum_m P(X_1 = m) \left(P(X_n^{(1)} = 0)\right)^m \text{ by independence} \\
        &= \sum_m P(X_1 = m) \left(P(X_n = 0)\right)^m \\
        &= G(P(X_n = 0)) \\
        &= G(q_n)
    \end{align*}
\end{proof}
Now we have that $q_n \to q$ and $q_{n + 1} = G(q_n)$, so by the continuity of $G$, the limit probability $q$ must satisfy $q = G(q)$.\par
We can reason by theorem~\ref{thmBranchingExpectation}:
\begin{align*}
    &\text{If } E[X_1] < 1 \implies E[X_n] \to 0 \implies q = 1 \\
    &\text{If } E[X_1] > 1 \implies E[X_n] \to \infty \implies q < 1 \\
    &\text{If } E[X_1] = 1 \implies E[X_n] = 1
\end{align*}
\begin{theorem}
    Suppose $P(X_1 = 1) < 1$. Then the extinction probability is the minimal non-negative solution to $x = G(x)$. Moreover, $q < 1$ if and only if $E[X_1] > 1$.
    \label{thmExtinctionProb}
\end{theorem}
\begin{proof}
    We know that $q = G(q)$. Let $t$ be the smallest solution of $x = G(x)$. We will show that $t = q$, by showing $q \leq t$.\par
    \induction{$n = 0$}{$q_0 = 0 \leq t$ as required.}
    {$n = k$}{Suppose $q_k \leq t$}
    {$n = k + 1$}{
        Note that $G(z) = E[z^{X_1}]$ is an increasing function.
        \begin{equation*}
            q_{n + 1} = G(q_n) \leq G(t) = t
        \end{equation*}
    }
    Now we have that $q_n \leq t~\forall n$, so by taking the limit, $q \leq t$. But, since $t$ is the smallest possible solution to $x = G(x)$, and $q$ is a solution, $t = q$.\par
    It now remains to prove that $q < 1 \Leftrightarrow E[X_1] > 1$.
    \begin{case}{$g_0 + g_1 = 1$}
        Then $P(X_1 \leq 1) = 1$ and $E[X_1] = g_1$. Therefore, $G(z) = g_0 + g_1 z = 1 - E[X_1] + E[X_1] z$. Then solving $G(z) = z$:
        \begin{equation*}
            1 - E[X_1] = \left(1 - E[X_1]\right)z \implies z = 1
        \end{equation*}
        Then $E[X_1] < 1$ since $P(X_1 = 1) < 1$.
    \end{case}
    \begin{case}{$g_0 + g_1 < 1$}
        Define $H(z) = G(z) - z$. Then certainly $H(1) = 0$, and we will prove that there can exist at most one more in $[0, 1)$.\par
        Consider $H''(z)$:
        \begin{equation*}
            H''(z) = \sum_{r=2}^\infty r(r-1) g_r z^{r-2}
        \end{equation*}
        This is strictly positive by assumption of the case ($g_0 + g_1 < 1$). Therefore $H'(z)$ is strictly increasing on $[0, 1)$, and this implies $H$ can have at most $1$ other root in the range. To see this, suppose there exist two (or more) other roots in the range. Then $H'$ would have to be equal to zero at two points (between each root) by Rolle's Theorem, which is a contradiction.\par
        Consider now the case that there is no second solution, $q = 1$.
        \begin{equation*}
            \lim_{z \to 1^-} H'(z) = \lim_{z \to 1^-} G'(z) - 1
        \end{equation*}
        Note that $H(0) = G(0) = g_0 \geq 0$, and $H(1) = 0$, so this implies that $H(z) \geq 0$ for all $z \in [0, 1)$.\par
        Then we can consider the limit definition of $H'$:
        \begin{equation*}
            \lim_{z \to 1^-} H'(z) = \lim_{z \to 1^-} \frac{H(z) - H(1)}{z - 1} \leq 0
        \end{equation*}
        But $\lim_{z \to 1^-} H'(z) = E[X] - 1$, so indeed $E[x] \leq 1$.\par
        Consider the case that there does exist a second solution, $r < 1$.\par
        Now $H(r) = 0$, and $H(1) = 0$, so there exists some $w \in (r, 1)$ such that $H'(w) = 0$ by Rolle's Theorem.\par
        We have that $H'$ is strictly increasing, so for any $z > w$, $H'(z) > H'(w) = 0$.\par
        Therefore $\lim_{z \to 1^-} H'(z) > 0$, which means $E[X] - 1 > 0$, and $E[X] > 1$ as required.
    \end{case}
\end{proof}
\end{document}