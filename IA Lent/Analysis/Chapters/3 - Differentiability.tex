\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
\begin{definition}{Differentiability}
    Let $f : E \subseteq \C \mapsto \C$. Let $x \in E$ be a limit point. Then $f$ is \underline{differentiable} at $x$ with derivative $f'(x)$ if:
    \begin{equation}
        \lim_{y \to x} \frac{f(y) - f(x)}{y - x} = f'(x)
        \label{eqnDifferentiability}
    \end{equation}
    Further, if this is true for any point $x \in E$, then $f$ is \underline{differentiable on $E$}.
\end{definition}
Assume from now on that $E$ has no isolated points, and further that $E$ is an interval or disc.\par
\begin{remarks}
    \item Most of the time we will deal with $f : E \subseteq \R \mapsto \R$.
    \item There are other common notations for the derivative. These have been seen in IA Differential Equations.
    \item $y$ may be swapped for $x + h$ as $h \to 0$
    \item Defining $\epsilon(h) = f(x + h) - f(x) - hf'(x)$, then requiring $\epsilon(h) / h \to 0$ as $h \to 0$ gives an alternative definition:
        $f$ is differentiable at $x$ if and only if there exists $A$ and $\epsilon(h)$ such that
        \begin{equation*}
            f(x + h) = f(x) + hA + \epsilon(h)
        \end{equation*}
        Where $\epsilon(h)$ obeys $\lim_{h \to 0} \frac{\epsilon(h)}{h} = 0$.\par
        As an aside, this definition works in higher dimension.
    \item If $f$ is differentiable at $x$ then it must be continuous there, since $f(x + h) \to f(x)$ as $h \to 0$.
\end{remarks}
\begin{examples}{}
    \item $Id : \R \mapsto \R$ such that $Id(x) = x$. Then:
        \begin{align*}
            \frac{Id(x + h) - Id(x)}{h} &= \frac{x + h - x}{h}
            &= 1
        \end{align*}
    \item $f : \R \mapsto \R$ such that $f(x) = |x|$. This is differentiable for $x \neq 0$. However, we get two different values for $f'(0)$ for $h \to 0^+$ and $h \to 0^-$ which means it is not differentiable there.
\end{examples}
\section{Properties of Differentiability}
\begin{propositions}{
        Let $f : E \mapsto \C$. Let $E$ have no isolated points. Let also $g : E \mapsto \C$.
        \label{propsDiffProperties}
    }
    \item If $f(x) = c$, then $f$ is differentiable with derivative 0. \label{propConstantDiffability}    
    \item If $f$ and $g$ are differentiable at $x$, then so is $f(x) + g(x)$ with derivative $f'(x) = g'(x)$ \label{propSumDiffability}
    \item If $f$ and $g$ are differentiable at $x$, then so is $f(x)g(x)$ with derivative $f'(x) g(x) = f(x) g'(x)$. \label{propProdDiffability}
    \item If $f$ is differentiable at $x$ and $f(y) \neq 0~\forall y \in E$, then $frac{1}{f(x)}$ is differentiable with derivative $\frac{-f'(x)}{f(x)^2}$. \label{propReciprocalDiffability}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item $\lim_{h \to 0} \frac{c - c}{h} = 0$.
        \item
            \begin{align*}
                &\lim_{h \to 0} \frac{f(x + h) + g(x + h) - f(x) - g(x)}{h} \\
                &= \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} + \lim_{h \to 0} \frac{g(x + h) - g(x)}{g}
            \end{align*}
            Using proposition~\ref{propLimitSum}
        \item 
            \begin{align*}
                &\lim_{h \to 0} \frac{f(x + h)g(x + h) - f(x) g(x)}{h} \\
                &= \lim_{h \to 0} \frac{f(x + h) g(x + h) - f(x + h) g(x) + f(x + h) g(x) - f(x) g(x)}{h} \\
                &= \lim_{h \to 0} f(x + h) \frac{g(x + h) - g(x)}{h} + \lim_{h \to 0} g(x) \frac{f(x + h) - f(x)}{h} \\
                &= \lim_{h \to 0} f(x + h) g'(x) + g(x) f'(x) \\
                &= f(x) g'(x) + f'(x) g(x)
            \end{align*}
            Since $f$ is continuous.
        \item 
            \begin{align*}
                &\lim_{h \to 0} \frac{1/f(x + h) + 1/f(x)}{h} \\
                &= \lim_{h \to 0} \frac{1}{f(x) f(x + h)} \frac{f(x) - f(x + h)}{h} \\
                &= -\lim_{h \to 0} \frac{1}{f(x) f(x + h)} f'(x) \\
                &= -\frac{f'(x)}{f(x)^2}
            \end{align*}
    \end{enumerate}
\end{proof}
\begin{example}
    Let $f_n(x) = x^n$ for $n \in \N_0$.\par
    Then $n = 0 \implies f_0(x) = 1$ and so $f'_0(x) = 0$.\par
    Similarly, $f'_1(x) = 1$. Then claim that for any $n$, $f_n$ is differentiable on $\R$ with $f'_n(x) = nx^{n-1}$.\par
    \induction{$n = 1$}{Shown above.}
    {$n = k-1$}{}
    {$n = k$}{
        Then $f_k(x) = f_{k-1}(x) f_1(x)$. The by the product rule $f_k$ is differentiable on $\R$ and this derivative is:
        \begin{align*}
            f_k(x) &= f'_{k-1} f_1(x) + f_{k-1}(x) f'_1(x) \\
            &= (k-1)x^{n-2} \times x + x^{k-1} \times 1 \\
            &= kx^{k-1}
        \end{align*}
    }
    Also, we now have all the negative powers of $x$ by the quotient rule, for $x^n$ where $n$ is a negative integer, $f$ is differentiable on $\R \backslash \{0\}$ and its derivative is:
    \begin{align*}
        f'_n(x) &= \frac{-f'_{-n}(x)}{f_{-n}(x)^2} \\
        &= \frac{-(-nx^{-n-1})}{\left(x^{-n}\right)^2} \\
        &= nx^{n-1}
    \end{align*}
    So the power rule holds for all integers $n$.
    \label{exPowerRule}
\end{example}
Combining this with the results in proposition~\ref{propsDiffProperties}, we see that polynomials are differentiable on all of $\R$, and that rational functions (functions $\frac{p(x)}{q(x)}$ where $p$ and $q$ are polynomials with no common roots) are differentiable on $R$ excluding the roots of $q$.\par
Note that this logic can be applied to the complex plane to get that these results hold in $\C$.
\begin{theorem}[Chain Rule]
    Suppose that $U, V \subseteq \C$. Let $f : U \mapsto V$ and $g : V \mapsto \C$ are such that $f$ is differentiable at $a \in U$ and $g$ is differentiable at $f(a) \in V$. Then the composition is differentiable at $a$ with derivative:
    \begin{equation}
        (g \circ f)'(a) = g'(f(a)) f'(a)
        \label{eqnChainRule}
    \end{equation}
    \label{thmChainRule}
\end{theorem}
\begin{remark}
    Though this formula should be very familiar, the statement that this composition is differentiable is quite powerful.
\end{remark}
\begin{proof}
    Let $b = f(a)$.\par
    We know that:
    \begin{align}
        f(x) &= f(a) + (x-a) f'(a) + \epsilon_f(x) (x - a) \text{ where } \lim_{x \to a} \epsilon_f(x) = 0 \label{eqnDiffDefF} \\
        g(y) &= g(b) + (y-b) g'(b) + \epsilon_g(y) (y - b) \text{ where } \lim_{y \to b} \epsilon_g(y) = 0 \label{eqnDiffDefG}
    \end{align}
    Notice that by setting $\epsilon_f(a) = 0, \epsilon_g(b) = 0$, we have that $\epsilon_f$ and $\epsilon_g$ are continuous at $a$ and $b$ (respectively).\par
    Then set $y = f(x)$ in equation~\ref{eqnDiffDefG}:
    \begin{equation*}
        f(f(x)) = g(b) + (f(x) - b) g'(b) + \epsilon_g(f(x))(f(x) - b)
    \end{equation*}
    Then using equation~\ref{eqnDiffDefF}:
    \begin{align*}
        g(f(x)) &= g(b) = \left((x - a) f'(a) + \epsilon_f(x)(x - a)\right) g'(b) \\
        &+ \epsilon_g(f(x))\left((x - a) f'(a) + \epsilon_f(x) (x - a)\right) \\
        &= g(f(a)) + (x - a)f'(a) g'(f(a)) \\
        &+ (x - a) \left[g'(f(a)) \epsilon_f(x) + f'(a) \epsilon_g(f(x)) + \epsilon_g(f(x)) \epsilon_g(x)\right]
    \end{align*}
    Now let $\sigma(x) = g'(f(a)) \epsilon_f(x) + f'(a) \epsilon_g(f(x)) + \epsilon_g(f(x)) \epsilon_g(x)$, and consider $x \to a$. We have that $f$, $g$, $\epsilon_f$ and $\epsilon_g$ are continuous, so:
    $\epsilon_g(f(x)) \to 0$, $\epsilon_f(x) \to 0$ so $\sigma(x) \to 0$ as $x \to a$. So $\sigma = \epsilon_{g \circ f}$ and:
    \begin{equation*}
        g(f(x)) = g(f(a)) + (x - a) f'(a) g'(f(a)) + (x - a) \epsilon_{g \circ f}(x)
    \end{equation*}
\end{proof}
\begin{examples}{}
    \item $f(x) = \sin{(x^2)}$. Assume that $\sin$ is continuous everywhere. Then this is a composition of differentiable functions and is thus differentiable at all real $x$.
        \begin{equation*}
            f'(x) = 2x\cos{(x^2)}
        \end{equation*}
    \item $f(x) = x\sin{\left(\frac{1}{x}\right)}$ for non-zero $x$ and $f(0) = 0$. For non-zero $x$, theorem~\ref{thmChainRule} gives us that this is differentiable. However, at $x = 0$, we consider:
        \begin{equation*}
            \lim_{t \to 0} \frac{f(t) - f(0)}{t} = \lim_{t \to 0} \sin{\frac{1}{t}}
        \end{equation*}
        So this function is not differentiable at $x = 0$, despite being continuous (see example~\ref{exXSinOneOverXContinuity})
    \item $f(x) - x^2 \sin{\left(\frac{1}{x}\right)}$ for $x$ non-zero, and $f(0) = 0$. This is again differentiable for $x$ non-zero. Consider differentiability at $x = 0$:
        \begin{equation*}
            \lim_{t \to 0} \frac{f(t) - f(0)}{t} = \lim_{t \to 0}  t \sin{\left(\frac{1}{t}\right)}
        \end{equation*}
        And this does tend to 0. This shows that, if $f$ is differentiable, $f'$ need not even be continuous.
\end{examples}
\section{The Mean Value Theorem}
\begin{theorem}[Rolle's Theorem]
    Let $f : [a, b] \mapsto \R$. Let it be continuous on $[a, b]$ and differentiable on $(a, b)$. If $f(a) = f(b)$, then there exists some $c$ in $(a, b)$ such that $f'(c) = 0$.
    \label{thmRolles}
\end{theorem}
That is, if a function has the same output for two separate inputs, then it must have a turning point between those points (or be constant).
\begin{proof}
    Consider theorem~\ref{thmExtremeValue}. This shows that there must exist some $y$ and $Y$ in $[a, b]$ such that $m = f(y) \leq f(x) \leq f(Y) = M$. We want to show that at an extreme value, the derivative is zero.
    \begin{case}{$m = M$}
        If $m = M$, then the function must be constant $f(x) = f(a)$. Therefore, the derivative of $f$ is 0 at any $x$.
    \end{case}
    \begin{case}{$M > f(a)$}
        This then gives that $Y \in (a, b)$.\par
        Now let $h_n$ be a sequence decreasing to $0$.
        \begin{align*}
            &\frac{f(Y + h_n) - f(Y)}{h_n} \leq 0 \text{ since } f(Y) \text{ is a maximum.} \\
            &\frac{f(Y - h_n) - f(Y)}{h_n} \geq 0
        \end{align*}
        So in the limit as $h_n \to 0$,
        \begin{equation*}
            0 \leq \frac{f(Y + h) - f(Y)}{h} \leq 0
        \end{equation*}
        And so $f'(Y) = 0$.
    \end{case}
    \begin{case}{$m < f(a)$}
        This case is almost identical to the previous, $f'(y) = 0$.
    \end{case}
    In all cases, we have a valid value for $c$.
\end{proof}
\begin{theorem}[Mean Value Theorem]
    Let $f : [a, b] \mapsto \R$. Let $f$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that:
    \begin{equation*}
        f(b) - f(a) = f'(c)(b - a) \Leftrightarrow f'(c) = \frac{f(b) - f(a)}{b - a}
    \end{equation*}
    \label{thmMeanValue}
\end{theorem}
This theorem captures the idea that for the graph of any function, at some point $c$ the gradient will match that of the line between its endpoints. See figure~\ref{figMeanValue}. Another intuition is that if a car is driven 50 miles in 1 hour, at some point it must have been going at 50 miles per hour.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \draw[domain=0:4,samples=50] plot (\x, {(\x-1)*sin(\x r)}) node[right] {$f(x)$};
        \coordinate (A) at (0, 0);
        \coordinate (B) at (4, -2.27);
        \coordinate (C) at (2.48, 0.905);

        \coordinate (B1) at ($(B)-0.6*(1, -0.568)$);
        \coordinate (C1) at ($(C)-0.6*(1, -0.568)$);
        \coordinate (C2) at ($(C)+0.6*(1, -0.568)$);

        \draw[dashed] (A) -- (B);
        \draw[dashed] (C1) -- (C2);

        \foreach \coord/\name in {(A)/a, (B)/b}
            \draw[fill] \coord circle[radius=0.4mm] node[below left] {$\name, f(\name)$};
        \draw[fill] (C) circle[radius=0.4mm] node[above right] {$c, f(c)$};

        \node[anchor=west] (D) at (4, 1) {Same gradients};
        \draw[->] (D) -- (B1);
        \draw[->] (D) -- (C2);
    \end{tikzpicture}
    \caption{Illustration of the Mean Value Theorem}
    \label{figMeanValue}
\end{figure}
\begin{proof}
    We consider $\phi(x) = (x - a)(f(b) - f(a)) - (b - a)(f(x) - f(a))$. This must be continuous on $[a, b]$ and differentiable on $(a, b)$. Furthermore, $\phi(a) = \phi(b) = 0$. Now $\phi$ satisfies the condition of theorem~\ref{thmRolles}, so at some point $c$ the derivative $\phi'(x)$ is zero. Now:
    \begin{align*}
        0 &= \phi'(c) \\
        &= (f(b) - f(a)) - (b - a)f'(c) \\
        \implies f(b) - f(a) &= f'(c)(b - a)
    \end{align*}
\end{proof}
\begin{remark}
    The Mean Value Theorem allows local properties (to do with the derivative of a function) to be exported to global properties with non-infinitesimal scales.
\end{remark}
\begin{corollary}
    If $f$ is differentiable on an open interval $I$ containing $a$, then this is equivalent to:\par
    Given $h$ such that $a + h \in I$, then there exists $\theta(h) \in (0, 1)$ such that:
    \begin{equation*}
        f(a + h) = f(a) + hf'(a + h\theta)
    \end{equation*}
\end{corollary}
The proof is by theorem~\ref{thmMeanValue}, setting $b = a + h$.
\begin{corollary}
    Assume that $f : [a, b] \mapsto \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Then if $f'(x) \geq 0 \forall x \in [a, b]$ then $f$ is increasing. Also, if this derivative is strictly positive then $f$ is strictly increasing. Further, if $f'(x) = 0$ for all $x$, then $f$ is constant.
    \label{corDerivativeSign}
\end{corollary}
\begin{proof}
    First consider $f'(x) \geq 0$. Consider points $a \leq x < y \leq b$. Then apply theorem~\ref{thmMeanValue} on $[x, y]$ to $f$. Therefore there exists some $c \in (x, y)$ such that:
    \begin{align*}
        f(y) - f(x) &= f'(c)(y-x) \geq 0 \\
        \implies f(x) &\leq f(y)
    \end{align*}
    Note that the same can be said for strictly positive, and strictly/non-strictly negative derivative.\par
    Now assume that $f'(x) = 0$. Then choose $x \in (a, b]$, and apply theorem~\ref{thmMeanValue}, so we have $c \in (a, x)$ such that $f(x) - f(a) = 0$. That is, $f(x) = f(a)$. And since $x$ was arbitrary, $f(x) = f(a)$ for all $x$.
\end{proof}
\begin{warning}
    The Mean Value Theorem does not necessarily hold for more general sets. For example, consider:
    \begin{align*}
        f : \Q &\mapsto \Q \\
        x &\mapsto
        \begin{cases}
            0 & x^2 < 2 \\
            1 & x^2 > 2
        \end{cases}
    \end{align*}
    Here $f$ is differentiable with $f'(x) = 0$ for every $x$ (note this is because $\sqrt{2}$ is not in the domain) but $f$ is not constant.
\end{warning}
\begin{theorem}[Inverse Function Theorem II]
    Suppose that $f : [a, b] \mapsto \R$ and differentiable on $(a, b)$. Let $f'(x) > 0$ for all $x$ in the interval.\par
    Define $f(a) = c, f(b) = d$. Then $f : [a, b] \mapsto [c, d]$ is a bijection, and $f^{-1}$ exists and is differentiable on $(c, d)$ with derivative:
    \begin{equation*}
        (f^{-1})'(y) = \left[f'(f^{-1}(y))\right]^{-1}
    \end{equation*}
    \label{thmInverseFunctionII}
\end{theorem}
\begin{proof}
    By corollary~\ref{corDerivativeSign} $f$ is strictly increasing. Therefore, theorem~\ref{thmInverseFunctionI} gives that $f$ is bijective, and that $f^{-1}$ exists, is continuous, and is strictly increasing.
    \begin{subproof}{$f^{-1}$ is differentiable on $(c, d)$ with the given derivative.}
        Let $y \in (c, d)$ and set $x = f^{-1}(y)$. Given $h$ such that $y + h \in (c, d)$, define $k$ such that $y + h = f(x + k)$
        \begin{align*}
            \frac{f^{-1}(y + h) - f^{-1}(y)}{h} &= \frac{k + x - x}{f(x + k) - y} \\
            &= \frac{k}{f(x + k) - f(x)}
        \end{align*}
        Now for any $\epsilon > 0$, there exists $\delta$ such that $0 < |k| < \delta$:
        \begin{equation*}
            \left|\frac{k}{f(x + k) - f(x) - \frac{1}{f'(x)}}\right| < \epsilon
        \end{equation*}
        And also note that since $f^{-1}$ is continuous, there exists $\delta' > 0$ such that:
        \begin{equation*}
            0 < |h| < \delta' \implies 0 < |k| < \delta
        \end{equation*}
        And therefore by the above,
        \begin{equation*}
            \left|\frac{f^{-1}(y + h) - f^{-1}(y)}{h} - \frac{1}{f'(x)}\right| < \epsilon
        \end{equation*}
        Therefore $f^{-1}$ is differentiable at $y$, with derivative:
        \begin{equation*}
            (f^{-1})'(y) = \frac{1}{f'(x)} = \frac{1}{f'(f^{-1}(y))}
        \end{equation*}
    \end{subproof}
\end{proof}
\begin{corollary}
    The set of functions $f(x) = x^{\frac{1}{n}}$ is differentiable on the interval $(0, R)$ with derivative $\frac{1}{n} x^{\frac{1}{n} - 1}$
    \label{corNthRootDiffable}
\end{corollary}
\begin{proof}
    Consider the function:
    \begin{align*}
        f_n : [0, R^{\frac{1}{n}}] &\mapsto \R \\
        x \mapsto x^n
    \end{align*}
    Then by theorem~\ref{thmInverseFunctionII}, $g_n = f_n^{-1}$ is differentiable on the required interval. Its derivative is:
    \begin{align*}
        g'_n(y) &= \frac{1}{(f'_n(g(y)))} \\
        &= \frac{1}{n (g(y)^{n-1})} \\
        &= \frac{1}{n}\frac{1}{(y^{\frac{1}{n}})^{n-1}} \\
        &= \frac{1}{n}\frac{1}{y^{1 - \frac{1}{n}}} \\
        &= \frac{1}{n} y^{\frac{1}{n} - 1}
    \end{align*}
\end{proof}
\begin{remark}
    By this corollary and example~\ref{exPowerRule}, we have (by induction) that the power rule holds for all rational powers.
\end{remark}
We can give a generalisation of theorem~\ref{thmMeanValue}:
\begin{theorem}[Cauchy's Mean Value Theorem]
    Let $f, g : [a, b] \mapsto \R$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that:
    \begin{equation*}
        g'(c)(f(a) - f(b)) = f'(c)(g(a) - g(b))
    \end{equation*}
    \label{thmCauchyMeanValue}
\end{theorem}
\begin{proof}
    Let $\phi(x) = (g(x) - g(a))(f(b) - f(a)) - (g(b) - g(a))(f(x) - f(a))$. Again $\phi$ is continuous on $[a, b]$ and differentiable on $(a, b)$ and $\phi(a) = \phi(b) = 0$. Therefore for some $c$, $\phi'(c) = 0$ by theorem~\ref{thmRolles}, and this gives:
    \begin{align*}
        0 &= \phi'(c) \\
        &= g'(c) (f(b) - f(a)) - f'(c)(g(b) - g(a))
    \end{align*}
\end{proof}
\begin{remark}
    This is the foundation for L'H\^opital's Rule, proved on the example sheet.
\end{remark}
\end{document}