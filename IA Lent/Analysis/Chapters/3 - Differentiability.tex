\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
\begin{definition}{Differentiability}
    Let $f : E \subseteq \C \mapsto \C$. Let $x \in E$ be a limit point. Then $f$ is \underline{differentiable} at $x$ with derivative $f'(x)$ if:
    \begin{equation}
        \lim_{y \to x} \frac{f(y) - f(x)}{y - x} = f'(x)
        \label{eqnDifferentiability}
    \end{equation}
    Further, if this is true for any point $x \in E$, then $f$ is \underline{differentiable on $E$}.
\end{definition}
Assume from now on that $E$ has no isolated points, and further that $E$ is an interval or disc.\par
\begin{remarks}
    \item Most of the time we will deal with $f : E \subseteq \R \mapsto \R$.
    \item There are other common notations for the derivative. These have been seen in IA Differential Equations.
    \item $y$ may be swapped for $x + h$ as $h \to 0$
    \item Defining $\epsilon(h) = f(x + h) - f(x) - hf'(x)$, then requiring $\epsilon(h) / h \to 0$ as $h \to 0$ gives an alternative definition:
        $f$ is differentiable at $x$ if and only if there exists $A$ and $\epsilon(h)$ such that
        \begin{equation*}
            f(x + h) = f(x) + hA + \epsilon(h)
        \end{equation*}
        Where $\epsilon(h)$ obeys $\lim_{h \to 0} \frac{\epsilon(h)}{h} = 0$.\par
        As an aside, this definition works in higher dimension.
    \item If $f$ is differentiable at $x$ then it must be continuous there, since $f(x + h) \to f(x)$ as $h \to 0$.
\end{remarks}
\begin{examples}{}
    \item $Id : \R \mapsto \R$ such that $Id(x) = x$. Then:
        \begin{align*}
            \frac{Id(x + h) - Id(x)}{h} &= \frac{x + h - x}{h}
            &= 1
        \end{align*}
    \item $f : \R \mapsto \R$ such that $f(x) = |x|$. This is differentiable for $x \neq 0$. However, we get two different values for $f'(0)$ for $h \to 0^+$ and $h \to 0^-$ which means it is not differentiable there.
\end{examples}
\section{Properties of Differentiability}
\begin{propositions}{
        Let $f : E \mapsto \C$. Let $E$ have no isolated points. Let also $g : E \mapsto \C$.
        \label{propsDiffProperties}
    }
    \item If $f(x) = c$, then $f$ is differentiable with derivative 0. \label{propConstantDiffability}    
    \item If $f$ and $g$ are differentiable at $x$, then so is $f(x) + g(x)$ with derivative $f'(x) = g'(x)$ \label{propSumDiffability}
    \item If $f$ and $g$ are differentiable at $x$, then so is $f(x)g(x)$ with derivative $f'(x) g(x) = f(x) g'(x)$. \label{propProdDiffability}
    \item If $f$ is differentiable at $x$ and $f(y) \neq 0~\forall y \in E$, then $frac{1}{f(x)}$ is differentiable with derivative $\frac{-f'(x)}{f(x)^2}$. \label{propReciprocalDiffability}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item $\lim_{h \to 0} \frac{c - c}{h} = 0$.
        \item
            \begin{align*}
                &\lim_{h \to 0} \frac{f(x + h) + g(x + h) - f(x) - g(x)}{h} \\
                &= \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} + \lim_{h \to 0} \frac{g(x + h) - g(x)}{g}
            \end{align*}
            Using proposition~\ref{propLimitSum}
        \item 
            \begin{align*}
                &\lim_{h \to 0} \frac{f(x + h)g(x + h) - f(x) g(x)}{h} \\
                &= \lim_{h \to 0} \frac{f(x + h) g(x + h) - f(x + h) g(x)}{h} \\
                &+\frac{f(x + h) g(x) - f(x) g(x)}{h} \\
                &= \lim_{h \to 0} f(x + h) \frac{g(x + h) - g(x)}{h} + \lim_{h \to 0} g(x) \frac{f(x + h) - f(x)}{h} \\
                &= \lim_{h \to 0} f(x + h) g'(x) + g(x) f'(x) \\
                &= f(x) g'(x) + f'(x) g(x)
            \end{align*}
            Since $f$ is continuous.
        \item 
            \begin{align*}
                &\lim_{h \to 0} \frac{1/f(x + h) + 1/f(x)}{h} \\
                &= \lim_{h \to 0} \frac{1}{f(x) f(x + h)} \frac{f(x) - f(x + h)}{h} \\
                &= -\lim_{h \to 0} \frac{1}{f(x) f(x + h)} f'(x) \\
                &= -\frac{f'(x)}{f(x)^2}
            \end{align*}
    \end{enumerate}
\end{proof}
\begin{example}
    Let $f_n(x) = x^n$ for $n \in \N_0$.\par
    Then $n = 0 \implies f_0(x) = 1$ and so $f'_0(x) = 0$.\par
    Similarly, $f'_1(x) = 1$. Then claim that for any $n$, $f_n$ is differentiable on $\R$ with $f'_n(x) = nx^{n-1}$.\par
    \induction{$n = 1$}{Shown above.}
    {$n = k-1$}{}
    {$n = k$}{
        Then $f_k(x) = f_{k-1}(x) f_1(x)$. The by the product rule $f_k$ is differentiable on $\R$ and this derivative is:
        \begin{align*}
            f_k(x) &= f'_{k-1} f_1(x) + f_{k-1}(x) f'_1(x) \\
            &= (k-1)x^{n-2} \times x + x^{k-1} \times 1 \\
            &= kx^{k-1}
        \end{align*}
    }
    Also, we now have all the negative powers of $x$ by the quotient rule, for $x^n$ where $n$ is a negative integer, $f$ is differentiable on $\R \backslash \{0\}$ and its derivative is:
    \begin{align*}
        f'_n(x) &= \frac{-f'_{-n}(x)}{f_{-n}(x)^2} \\
        &= \frac{-(-nx^{-n-1})}{\left(x^{-n}\right)^2} \\
        &= nx^{n-1}
    \end{align*}
    So the power rule holds for all integers $n$.
    \label{exPowerRule}
\end{example}
Combining this with the results in proposition~\ref{propsDiffProperties}, we see that polynomials are differentiable on all of $\R$, and that rational functions (functions $\frac{p(x)}{q(x)}$ where $p$ and $q$ are polynomials with no common roots) are differentiable on $R$ excluding the roots of $q$.\par
Note that this logic can be applied to the complex plane to get that these results hold in $\C$.
\begin{theorem}[Chain Rule]
    Suppose that $U, V \subseteq \C$. Let $f : U \mapsto V$ and $g : V \mapsto \C$ are such that $f$ is differentiable at $a \in U$ and $g$ is differentiable at $f(a) \in V$. Then the composition is differentiable at $a$ with derivative:
    \begin{equation}
        (g \circ f)'(a) = g'(f(a)) f'(a)
        \label{eqnChainRule}
    \end{equation}
    \label{thmChainRule}
\end{theorem}
\begin{remark}
    Though this formula should be very familiar, the statement that this composition is differentiable is quite powerful.
\end{remark}
\begin{proof}
    Let $b = f(a)$.\par
    We know that:
    \begin{align}
        f(x) &= f(a) + (x-a) f'(a) + \epsilon_f(x) (x - a) \text{ where } \lim_{x \to a} \epsilon_f(x) = 0 \label{eqnDiffDefF} \\
        g(y) &= g(b) + (y-b) g'(b) + \epsilon_g(y) (y - b) \text{ where } \lim_{y \to b} \epsilon_g(y) = 0 \label{eqnDiffDefG}
    \end{align}
    Notice that by setting $\epsilon_f(a) = 0, \epsilon_g(b) = 0$, we have that $\epsilon_f$ and $\epsilon_g$ are continuous at $a$ and $b$ (respectively).\par
    Then set $y = f(x)$ in equation~\ref{eqnDiffDefG}:
    \begin{equation*}
        f(f(x)) = g(b) + (f(x) - b) g'(b) + \epsilon_g(f(x))(f(x) - b)
    \end{equation*}
    Then using equation~\ref{eqnDiffDefF}:
    \begin{align*}
        g(f(x)) &= g(b) = \left((x - a) f'(a) + \epsilon_f(x)(x - a)\right) g'(b) \\
        &+ \epsilon_g(f(x))\left((x - a) f'(a) + \epsilon_f(x) (x - a)\right) \\
        &= g(f(a)) + (x - a)f'(a) g'(f(a)) \\
        &+ (x - a) \left[g'(f(a)) \epsilon_f(x) + f'(a) \epsilon_g(f(x)) + \epsilon_g(f(x)) \epsilon_g(x)\right]
    \end{align*}
    Now let $\sigma(x) = g'(f(a)) \epsilon_f(x) + f'(a) \epsilon_g(f(x)) + \epsilon_g(f(x)) \epsilon_g(x)$, and consider $x \to a$. We have that $f$, $g$, $\epsilon_f$ and $\epsilon_g$ are continuous, so:
    $\epsilon_g(f(x)) \to 0$, $\epsilon_f(x) \to 0$ so $\sigma(x) \to 0$ as $x \to a$. So $\sigma = \epsilon_{g \circ f}$ and:
    \begin{equation*}
        g(f(x)) = g(f(a)) + (x - a) f'(a) g'(f(a)) + (x - a) \epsilon_{g \circ f}(x)
    \end{equation*}
\end{proof}
\begin{examples}{}
    \item $f(x) = \sin{(x^2)}$. Assume that $\sin$ is continuous everywhere. Then this is a composition of differentiable functions and is thus differentiable at all real $x$.
        \begin{equation*}
            f'(x) = 2x\cos{(x^2)}
        \end{equation*}
    \item $f(x) = x\sin{\left(\frac{1}{x}\right)}$ for non-zero $x$ and $f(0) = 0$. For non-zero $x$, theorem~\ref{thmChainRule} gives us that this is differentiable. However, at $x = 0$, we consider:
        \begin{equation*}
            \lim_{t \to 0} \frac{f(t) - f(0)}{t} = \lim_{t \to 0} \sin{\frac{1}{t}}
        \end{equation*}
        So this function is not differentiable at $x = 0$, despite being continuous (see example~\ref{exXSinOneOverXContinuity})
    \item $f(x) - x^2 \sin{\left(\frac{1}{x}\right)}$ for $x$ non-zero, and $f(0) = 0$. This is again differentiable for $x$ non-zero. Consider differentiability at $x = 0$:
        \begin{equation*}
            \lim_{t \to 0} \frac{f(t) - f(0)}{t} = \lim_{t \to 0}  t \sin{\left(\frac{1}{t}\right)}
        \end{equation*}
        And this does tend to 0. This shows that, if $f$ is differentiable, $f'$ need not even be continuous.
\end{examples}
\section{The Mean Value Theorem}
\begin{theorem}[Rolle's Theorem]
    Let $f : [a, b] \mapsto \R$. Let it be continuous on $[a, b]$ and differentiable on $(a, b)$. If $f(a) = f(b)$, then there exists some $c$ in $(a, b)$ such that $f'(c) = 0$.
    \label{thmRolles}
\end{theorem}
That is, if a function has the same output for two separate inputs, then it must have a turning point between those points (or be constant).
\begin{proof}
    Consider theorem~\ref{thmExtremeValue}. This shows that there must exist some $y$ and $Y$ in $[a, b]$ such that $m = f(y) \leq f(x) \leq f(Y) = M$. We want to show that at an extreme value, the derivative is zero.
    \begin{case}{$m = M$}
        If $m = M$, then the function must be constant $f(x) = f(a)$. Therefore, the derivative of $f$ is 0 at any $x$.
    \end{case}
    \begin{case}{$M > f(a)$}
        This then gives that $Y \in (a, b)$.\par
        Now let $h_n$ be a sequence decreasing to $0$.
        \begin{align*}
            &\frac{f(Y + h_n) - f(Y)}{h_n} \leq 0 \text{ since } f(Y) \text{ is a maximum.} \\
            &\frac{f(Y - h_n) - f(Y)}{h_n} \geq 0
        \end{align*}
        So in the limit as $h_n \to 0$,
        \begin{equation*}
            0 \leq \frac{f(Y + h) - f(Y)}{h} \leq 0
        \end{equation*}
        And so $f'(Y) = 0$.
    \end{case}
    \begin{case}{$m < f(a)$}
        This case is almost identical to the previous, $f'(y) = 0$.
    \end{case}
    In all cases, we have a valid value for $c$.
\end{proof}
\begin{theorem}[Mean Value Theorem]
    Let $f : [a, b] \mapsto \R$. Let $f$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that:
    \begin{equation*}
        f(b) - f(a) = f'(c)(b - a) \Leftrightarrow f'(c) = \frac{f(b) - f(a)}{b - a}
    \end{equation*}
    \label{thmMeanValue}
\end{theorem}
This theorem captures the idea that for the graph of any function, at some point $c$ the gradient will match that of the line between its endpoints. See figure~\ref{figMeanValue}. Another intuition is that if a car is driven 50 miles in 1 hour, at some point it must have been going at 50 miles per hour.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \draw[domain=0:4,samples=50] plot (\x, {(\x-1)*sin(\x r)}) node[right] {$f(x)$};
        \coordinate (A) at (0, 0);
        \coordinate (B) at (4, -2.27);
        \coordinate (C) at (2.48, 0.905);

        \coordinate (B1) at ($(B)-0.6*(1, -0.568)$);
        \coordinate (C1) at ($(C)-0.6*(1, -0.568)$);
        \coordinate (C2) at ($(C)+0.6*(1, -0.568)$);

        \draw[dashed] (A) -- (B);
        \draw[dashed] (C1) -- (C2);

        \foreach \coord/\name in {(A)/a, (B)/b}
            \draw[fill] \coord circle[radius=0.4mm] node[below left] {$\name, f(\name)$};
        \draw[fill] (C) circle[radius=0.4mm] node[above right] {$c, f(c)$};

        \node[anchor=west] (D) at (4, 1) {Same gradients};
        \draw[->] (D) -- (B1);
        \draw[->] (D) -- (C2);
    \end{tikzpicture}
    \caption{Illustration of the Mean Value Theorem}
    \label{figMeanValue}
\end{figure}
\begin{proof}
    We consider $\phi(x) = (x - a)(f(b) - f(a)) - (b - a)(f(x) - f(a))$. This must be continuous on $[a, b]$ and differentiable on $(a, b)$. Furthermore, $\phi(a) = \phi(b) = 0$. Now $\phi$ satisfies the condition of theorem~\ref{thmRolles}, so at some point $c$ the derivative $\phi'(x)$ is zero. Now:
    \begin{align*}
        0 &= \phi'(c) \\
        &= (f(b) - f(a)) - (b - a)f'(c) \\
        \implies f(b) - f(a) &= f'(c)(b - a)
    \end{align*}
\end{proof}
\begin{remark}
    The Mean Value Theorem allows local properties (to do with the derivative of a function) to be exported to global properties with non-infinitesimal scales.
\end{remark}
\begin{corollary}
    If $f$ is differentiable on an open interval $I$ containing $a$, then this is equivalent to:\par
    Given $h$ such that $a + h \in I$, then there exists $\theta(h) \in (0, 1)$ such that:
    \begin{equation*}
        f(a + h) = f(a) + hf'(a + h\theta)
    \end{equation*}
\end{corollary}
The proof is by theorem~\ref{thmMeanValue}, setting $b = a + h$.
\begin{corollary}
    Assume that $f : [a, b] \mapsto \R$ is continuous on $[a, b]$ and differentiable on $(a, b)$. Then if $f'(x) \geq 0 \forall x \in [a, b]$ then $f$ is increasing. Also, if this derivative is strictly positive then $f$ is strictly increasing. Further, if $f'(x) = 0$ for all $x$, then $f$ is constant.
    \label{corDerivativeSign}
\end{corollary}
\begin{proof}
    First consider $f'(x) \geq 0$. Consider points $a \leq x < y \leq b$. Then apply theorem~\ref{thmMeanValue} on $[x, y]$ to $f$. Therefore there exists some $c \in (x, y)$ such that:
    \begin{align*}
        f(y) - f(x) &= f'(c)(y-x) \geq 0 \\
        \implies f(x) &\leq f(y)
    \end{align*}
    Note that the same can be said for strictly positive, and strictly/non-strictly negative derivative.\par
    Now assume that $f'(x) = 0$. Then choose $x \in (a, b]$, and apply theorem~\ref{thmMeanValue}, so we have $c \in (a, x)$ such that $f(x) - f(a) = 0$. That is, $f(x) = f(a)$. And since $x$ was arbitrary, $f(x) = f(a)$ for all $x$.
\end{proof}
\begin{warning}
    The Mean Value Theorem does not necessarily hold for more general sets. For example, consider:
    \begin{align*}
        f : \Q &\mapsto \Q \\
        x &\mapsto
        \begin{cases}
            0 & x^2 < 2 \\
            1 & x^2 > 2
        \end{cases}
    \end{align*}
    Here $f$ is differentiable with $f'(x) = 0$ for every $x$ (note this is because $\sqrt{2}$ is not in the domain) but $f$ is not constant.
\end{warning}
\begin{theorem}[Inverse Function Theorem II]
    Suppose that $f : [a, b] \mapsto \R$ and differentiable on $(a, b)$. Let $f'(x) > 0$ for all $x$ in the interval.\par
    Define $f(a) = c, f(b) = d$. Then $f : [a, b] \mapsto [c, d]$ is a bijection, and $f^{-1}$ exists and is differentiable on $(c, d)$ with derivative:
    \begin{equation*}
        (f^{-1})'(y) = \left[f'(f^{-1}(y))\right]^{-1}
    \end{equation*}
    \label{thmInverseFunctionII}
\end{theorem}
\begin{proof}
    By corollary~\ref{corDerivativeSign} $f$ is strictly increasing. Therefore, theorem~\ref{thmInverseFunctionI} gives that $f$ is bijective, and that $f^{-1}$ exists, is continuous, and is strictly increasing.
    \begin{subproof}{$f^{-1}$ is differentiable on $(c, d)$ with the given derivative.}
        Let $y \in (c, d)$ and set $x = f^{-1}(y)$. Given $h$ such that $y + h \in (c, d)$, define $k$ such that $y + h = f(x + k)$
        \begin{align*}
            \frac{f^{-1}(y + h) - f^{-1}(y)}{h} &= \frac{k + x - x}{f(x + k) - y} \\
            &= \frac{k}{f(x + k) - f(x)}
        \end{align*}
        Now for any $\epsilon > 0$, there exists $\delta$ such that $0 < |k| < \delta$:
        \begin{equation*}
            \left|\frac{k}{f(x + k) - f(x) - \frac{1}{f'(x)}}\right| < \epsilon
        \end{equation*}
        And also note that since $f^{-1}$ is continuous, there exists $\delta' > 0$ such that:
        \begin{equation*}
            0 < |h| < \delta' \implies 0 < |k| < \delta
        \end{equation*}
        And therefore by the above,
        \begin{equation*}
            \left|\frac{f^{-1}(y + h) - f^{-1}(y)}{h} - \frac{1}{f'(x)}\right| < \epsilon
        \end{equation*}
        Therefore $f^{-1}$ is differentiable at $y$, with derivative:
        \begin{equation*}
            (f^{-1})'(y) = \frac{1}{f'(x)} = \frac{1}{f'(f^{-1}(y))}
        \end{equation*}
    \end{subproof}
\end{proof}
\begin{corollary}
    The set of functions $f(x) = x^{\frac{1}{n}}$ is differentiable on the interval $(0, R)$ with derivative $\frac{1}{n} x^{\frac{1}{n} - 1}$
    \label{corNthRootDiffable}
\end{corollary}
\begin{proof}
    Consider the function:
    \begin{align*}
        f_n : [0, R^{\frac{1}{n}}] &\mapsto \R \\
        x \mapsto x^n
    \end{align*}
    Then by theorem~\ref{thmInverseFunctionII}, $g_n = f_n^{-1}$ is differentiable on the required interval. Its derivative is:
    \begin{align*}
        g'_n(y) &= \frac{1}{(f'_n(g(y)))} \\
        &= \frac{1}{n (g(y)^{n-1})} \\
        &= \frac{1}{n}\frac{1}{(y^{\frac{1}{n}})^{n-1}} \\
        &= \frac{1}{n}\frac{1}{y^{1 - \frac{1}{n}}} \\
        &= \frac{1}{n} y^{\frac{1}{n} - 1}
    \end{align*}
\end{proof}
\begin{remark}
    By this corollary and example~\ref{exPowerRule}, we have (by induction) that the power rule holds for all rational powers.
\end{remark}
We can give a generalisation of theorem~\ref{thmMeanValue}:
\begin{theorem}[Cauchy's Mean Value Theorem]
    Let $f, g : [a, b] \mapsto \R$ be continuous on $[a, b]$ and differentiable on $(a, b)$. Then there exists $c \in (a, b)$ such that:
    \begin{equation*}
        g'(c)(f(a) - f(b)) = f'(c)(g(a) - g(b))
    \end{equation*}
    \label{thmCauchyMeanValue}
\end{theorem}
\begin{proof}
    Let $\phi(x) = (g(x) - g(a))(f(b) - f(a)) - (g(b) - g(a))(f(x) - f(a))$. Again $\phi$ is continuous on $[a, b]$ and differentiable on $(a, b)$ and $\phi(a) = \phi(b) = 0$. Therefore for some $c$, $\phi'(c) = 0$ by theorem~\ref{thmRolles}, and this gives:
    \begin{align*}
        0 &= \phi'(c) \\
        &= g'(c) (f(b) - f(a)) - f'(c)(g(b) - g(a))
    \end{align*}
\end{proof}
\begin{remark}
    This is the foundation for L'H\^opital's Rule, proved on the example sheet.
\end{remark}
\section{Taking Higher Derivatives}
\begin{definition}{$n$-times differentiability}
    Suppose $E \in \C$ with no isolated points. Suppose also that $f : E \mapsto \C$ is differentiable on $E$.\par
    Then consider $f' : E \mapsto \C$ which maps $x$ to the derivative of $f$ at $x$.\par
    If this function is differentiable at some $x \in E$, then $f$ is \\\underline{twice-differentiable} at $x$. We write the derivative $f''(x)$ or $f^{(2)}(x)$. If $f$ has this property at every $x \in E$, then $f$ is \underline{twice-differentiable on $E$}. Iterating this given $n$-times differentiability.
\end{definition}
\begin{definition}{Smoothness}
    If $f$ is $n$-times differentiable for every $n \in \N$, then $f$ is \underline{smooth}.
\end{definition}
\subsection{Taylor's Theorem}
\begin{theorem}[Taylor's Theorem I]
    Suppose that $f$ and its derivatives to order $n - 1$ are continuous on some interval $[a, a + h]$, and that $f$ is $n$ times differentiable on the interval $(a, a + h)$. Then there exists some $\theta \in (0, 1)$ such that:
    \begin{equation*}
        f(a + h) = f(a) + hf'(a) + \frac{h^2}{2!} f''(a) + \cdots + \frac{h^{n-1}}{(n-1)!} f^{(n-1)}(a) + \frac{h^n}{n!} f^{(n)}(a + \theta h)
    \end{equation*}
    \label{thmTaylorsI}
\end{theorem}
\begin{remarks}
    \item Setting $n = 1$ gives exactly theorem~\ref{thmMeanValue}, so this result is an $n$th order generalisation of it.
    \item The remainder term, $R_n = \frac{h^n}{n!} f^{(n)}(a + \theta h)$ is Lagrange's Form of the remainder.
    \item If $h < 0$ then select instead the intervals $[a + h, a]$ and $(a + h, a)$, and apply the theorem to $-f(-a)$ to get the result as required.
    \item If $f$ is $n$ times differentiable on $[a, a + h]$, then all the conditions hold.
\end{remarks}
\begin{proof}[In the style of theorem~\ref{thmMeanValue}]
    Relabel $f(x)$ to $f(x + a)$, so we can choose $a = 0$ wlog. Then consider the function, for $0 \leq t \leq h$
    \begin{equation*}
        \phi(t) = f(t) - f(0) - tf'(0) - \cdots - \frac{t^{n-1}}{(n-1)!} f^{(n-1)}(0) - \frac{t^n}{n!}B
    \end{equation*}
    Where we choose $B$ such that $\phi(h) = 0$:
    \begin{equation}
        \frac{h^n}{n!} B = f(h) - f(0) - hf'(0) - \cdots - \frac{h^{n-1}}{(n-1)!} f^{(n-1)}(0)
        \label{eqnRemainderTerm}
    \end{equation}
    Then we repeatedly apply theorem~\ref{thmRolles} to $\phi$ to find another expression for $B$.\par
    We first observe that $\phi$ and its first $n - 1$ derivatives are continuous on $[0, h]$ and differentiable on $(0, h)$. Note also that $\phi$ and its first $n - 1$ derivatives of $\phi$ are equal to $0$ at $t = 0$. We have also chosen $B$ such that $\phi(0) = \phi(h) = 0$.\par
    Therefore, by theorem~\ref{thmRolles} there exists $\theta_1 \in (0, 1)$ such that $\phi'(\theta_1h) = 0$.\par
    We apply theorem~\ref{thmRolles} again: there exists $\theta_2 \in (0, 1)$ such that $\phi^{(2)}(\theta_2\theta_1h) = 0$.\par
    So by application $n-2$ more times, there exists $\theta_n \in (0, 1)$ such that $\phi^{(n)}(\theta_n \cdot \cdots \cdot \theta_1h) = 0$. Therefore, let $\theta$ be this product: $\theta = \theta_n \cdot \cdots \cdot \theta_1$. Note that $\theta \in (0, 1)$.\par
    Now, $\theta^{(n)}(\theta h) = 0$. But $\theta^{(n)}(t) = f^{(n)}(t) - B$, so $B = f^{(n)}(\theta h)$.\par
    Inserting this expression for $B$ into equation~\ref{eqnRemainderTerm} and rearranging:
    \begin{equation*}
        f(h) = f(0) + hf'(0) + \cdots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0) + \frac{h^n}{n!} f^{(n)}(\theta h)
    \end{equation*}
\end{proof}
\begin{remark}
    This form of the remainder is known as Lagrange's form of the remainder.
\end{remark}
We can also offer an alternative proof by choosing $\phi$ more cleverly:
\begin{proof}[Using a clever choice for $\phi$]
    Again we assume that $a = 0$. This time, for $0 \leq t \leq h$, let:
    \begin{equation*}
        \phi(t) = f(h) - f(t) - (h - t)f'(t) - \frac{(h - t)^2}{2!} - \cdots - \frac{(h - t)^{n-1}}{(n-1)!} f^{(n-1)}(t)
    \end{equation*}
    Here $\phi$ is only once differentiable on $(0, h)$:
    \begin{align*}
        \phi'(t) &= -f'(t) + f'(t) - (h - t)f''(t) + (h - t)f''(t)\\
        &- \frac{(h - t)^2}{2!} f'''(t) + \cdots - \frac{(h - t)^{n - 1}}{(n-1)!} f^{(n)}(t) \\
        &= -\frac{(h-t)^{n-1}}{(n-1)!} f^{(n)}(t)
    \end{align*}
    So now set:
    \begin{equation*}
        \psi(t) = \phi(t) - \left(\frac{h - t}{h}\right)^p \phi(0)
    \end{equation*}
    For $1 \leq p \leq n$.\par
    This gives $\psi(0) = \psi(h) = 0$. Then by theorem~\ref{thmRolles} there exists $\theta \in (0, 1)$ such that $\psi'(\theta h) = 0$. But note that:
    \begin{equation*}
        \psi'(\theta h) = \phi'(\theta h) + p \frac{(1 - \theta)^{p-1}}{h} \phi(0)
    \end{equation*}
    Which gives:
    \begin{align*}
        &0 = - \frac{h^{n-1}}{(n-1)!}(1 - \theta)^{n-1} f^{(n)} (\theta h) \\
        &+ p \frac{(1 - \theta)^{p-1}}{h} \left(f(h) - f(0) - hf'(0) - \cdots - \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0)\right) \\
        &\implies f(h) = f(0) + hf'(0) + \cdots + \frac{h^{n-1}}{(n-1)!}f^{(n-1)}(0)\\
        &+ \frac{h^n}{(n-1)!} \frac{(1 - \theta)^{n-1}}{p(1-\theta)^{p-1}} f^{(n)} (\theta h)
    \end{align*}
    Then setting $p = n$ gives the required result.
\end{proof}
\begin{remark}
    Not that we did not have to choose $p = n$. Instead, choosing $p = 1$ gives an alternate form of the remainder, Cauchy's form:
\end{remark}
\begin{theorem}[Taylor's Theorem II]
    With the same hypotheses of theorem~\ref{thmTaylorsI}, we have:
    \begin{align*}
        f(a + h) &= f(a) + hf'(a) + \cdots + \frac{h^{n-1}}{(n-1)!} f^{(n-1)}(a) \\
        &+ \frac{(1 - \theta)^{n-1} f^{(n)} (a + \theta h) h^n}{(n - 1)!}
    \end{align*}
    \label{thmTaylorsII}
\end{theorem}
\begin{remarks}
    \item Both versions of the theorem give a \underline{Taylor Polynomial}, of degree $n - 1$ and a remainder term in $h^n$ using the $n$th derivative of $f$.
    \item Taylor's Theorem does not assert that the infinite series approximates the function, even if it is smooth.
    \item To show that a function is equal to its Taylor series requires estimates and effort. We need to show that the remainder term tends to $0$ as $n \to \infty$ with $h$ fixed.
\end{remarks}
\begin{example}[The Binomial Series]
    Consider $f(x) = (1 + x)^r$.\par
    We have not yet defined irrational powers, so consider $r \in \Q$.\par
    We claim that $(1 + x)^r = 1 + \choose{r}{1} x + \cdots + \choose{r}{n} x^n$, and that the series converges absolutely in the limit $n \to \infty$. Here $\choose{r}{n}$ is the generalised binomial coefficient:
    \begin{equation*}
        \choose{r}{n} = \frac{1}{n!} \prod_{k = 0}^{n - 1} (r - k)
    \end{equation*}
    Then the $n$th derivative of $x$ is:
    \begin{equation*}
        f^{(n)}(x) = r(r-1)\cdots(r-n+1)(1+x)^{r-n}
    \end{equation*}
    Then by theorem~\ref{thmTaylorsI}, and assuming that $|x|<1, n \geq r$,
    \begin{equation*}
        (1 + x)^r = 1 + \choose{r}{1} x + \cdots + \choose{r}{n-1}x^{n-1} + \choose{n}{r} \frac{x^n}{(1 + \theta x)^{n-r}}
    \end{equation*}
    If $x \geq 0$, then $(1 + \theta x)^{n-r} \geq 1$, so:
    \begin{align*}
        &\implies 0 < \frac{1}{(1 + \theta x)^{n-r}} \leq 1 \\
        &\implies |R_n| = \left|\choose{r}{n} \frac{x^n}{(1 + \theta x)^{n-r}}\right| \leq \left|\choose{r}{n}x^n\right|
    \end{align*}
    Now observe that:
    \begin{equation*}
        \sum_{n \geq 0} \choose{r}{n}x^n \text{ converges absolutely}
    \end{equation*}
    This is by the ratio test,
    \begin{equation*}
        \left|\frac{a_{n+1}}{a_n}\right| = \left|\frac{r-n}{n+1}\right||x|
    \end{equation*}
    The RHS converges to $|x|$ which is less than $1$ by assumption. Note now that by the $n$th term test, the terms go to $0$ and so the remainder term $R_n$ goes to $0$.\par
    In the case that $x$ is negative, we cannot as easily use the Lagrange form of the remainder. However, we can employ Cauchy's form:
    \begin{align*}
        R_n &= (1 - \theta)^{n-1} \frac{r(r-1)\cdots(r-n+1)}{(n-1)!} (1 + \theta x)^{r-n} x^n \\
        &= r \choose{r-1}{n-1} \left(\frac{1-\theta}{1+\theta x}\right)^{n-1} (1 + \theta x)^{r-1} x^n
    \end{align*}
    Note here that the quantity in brackets is less than $1$, so we gain the following bound:
    \begin{equation*}
        |R_n| \leq r|x| \left|\choose{r-1}{n-1}x^{n-1}\right|(1+\theta x)^{r-1}
    \end{equation*}
    Note also that the final term can be bounded:
    \begin{equation*}
        (1 + \theta x)^{r-1} \leq \max{\{(1 - |x|)^{r-1}, (1 + |x|)^{r-1}\}}
    \end{equation*}
    And therefore we have a bound on $R_n$:
    \begin{equation*}
        |R_n| \leq r|x|\max{\{(1 - |x|)^{r-1}, (1 + |x|)^{r-1}\}} \left|\choose{r-1}{n-1} x^n\right|
    \end{equation*}
    Which tends to $0$ as $n \to \infty$.
\end{example}
\subsection{Complex Differentiation}
We defined differentiability for functions $f: E\subseteq \C \mapsto \C$. Results on sums, products, and the chain rule, apply to functions of a complex variable. However, complex differentiability is in general more restrictive than differentiability on $\R$.
\begin{example}
    Polynomials are differentiable on the complex plane. However, consider the function of conjugation:
    \begin{align*}
        f : \C &\mapsto \C \\
        z &\mapsto \bar{z}
    \end{align*}
    $f$ is continuous, because $|z - a| = |\bar{z} - \bar{a}$, so $|z - a| < \epsilon \implies |\bar{z} - \bar{a}| < \epsilon$.\par
    However, consider the difference quotient:
    \begin{equation*}
        \frac{f(z) - f(a)}{z - a} = \frac{\bar{z} - \bar{a}}{z - a}
    \end{equation*}
    Then suppose $z_n = a + h_n$, where $h_n$ is a real sequence tending to $0$. Then the difference quotient gives $\frac{h_n}{h_n} = 1$. If instead $z_n = a + ih(n)$, the difference quotient gives $-1$. So $f$ is complex differentiable nowhere.
\end{example}
The condition of complex differentiability is more restrictive than real differentiability, but gives stronger conclusions about this class of functions. For example, if $D = \{z \in \C, |z| < 1\}$ (the open disc of radius $1$ in $\C$). If $f : D \mapsto \C$ is complex differentiable on $D$, we can get much stronger conclusions:
\begin{itemize}
    \item $f$ is infinitely differentiable on $D$
    \item The Taylor series of $f$ about $0$ converges to $f$ on the whole of $D$.
\end{itemize}
Neither condition is true for real functions, in general. See IB Complex Methods/Complex Analysis.
\end{document}