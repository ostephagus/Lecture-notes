\documentclass[../Main.tex]{subfiles}

\begin{document}
We want to consider functions defined by power series of the form:
\begin{equation}
    f(z) = \sum_{n=0}^\infty a_n (z - z_0)^n
\end{equation}
Where $a_n$, $z$ and $z_0$ are complex numbers. By translation, we can assume $z_0 = 0$.
\section{Convergence of power series}
\subsection{Radius of Convergence}
This series may not converge for all $z \in \C$. The first task, therefore, is to understand the set of points for which a function of this form converges.
\begin{lemma}
    If a power series $\sum_{n=0}^\infty a_n z^n$ converges, and $|w| < |z|$, then the power series:
    \begin{equation*}
        \sum_{n=0}^\infty a_n w^n
    \end{equation*}
    converges absolutely.
    \label{lemConvergenceRegionIsDisc}
\end{lemma}
\begin{proof}
    Since $\sum_{n=0}^\infty a_n z^n$ converges, $a_n z^n \to 0$.\par
    Therefore $\exists K > 0$ such that $|a_n z^n| \leq K$ for all $n$.\par
    Now consider, for $z \neq 0$ (note that the case $z = 0$ is trivial),
    \begin{align*}
        |a_n w^n| &= |a_n z^n| \left|\frac{w}{z}\right|^n \\
        &\leq K\rho^n
    \end{align*}
    where $\rho = \frac{|w|}{|z|}$, and is less than $1$ by assumption. Therefore, by the comparison to the sum $\sum_{n=0}^\infty K\rho^n$, the required power series converges.
\end{proof}
\begin{remark}
    We will use this result to show that every power series has a well-defined radius of convergence.
\end{remark}
\begin{theorem}
    Let $\sum_{n=0}^\infty a_n z^n$ be a power series. Then there exists some $R \in [0, \infty]$, the \underline{radius of convergence}, such that the series converges absolutely for $|z| < R$, and diverges for $|z| > R$.
    \label{thmRadiusOfConvergence}
\end{theorem}
\begin{proof}
    Let $A = \subsetselect{r \geq 0}{\exists z \in \C, |z| = r, \sum_{n=0}^\infty a_n z^n \text{ converges}}$. That is, the set of radii of points where the sequence converges.

    Clearly $0 \in A$, so $A$ is non-empty. If $A$ is not bounded above, define $R = \infty$. If not, $A$ is bounded above and non-empty and so has a supremum. In this case, define $R = \sup A$.

    From the definition of $R$, $\sum_{n=0}^\infty a_n z^n$ diverges for $|z| > R$.\par
    Consider now $w \in \C$ with $|w| < R$. Then there exists $r \in A$ with $|w| < r$. This means there exists $z \in \C$ with $|z| = r$ and $\sum_{n=0}^\infty a_n z^n$ convergent. Then by Lemma~\ref{lemConvergenceRegionIsDisc}, $\sum_{n=0}^\infty a_n w^n$ converges absolutely.
\end{proof}
\begin{remarks}
    \item If $R = 0$, then the sum converges only for $z = 0$.
    \item If $R = \infty$, then the sum converges for any $z \in \C$.
    \item If neither of the above are true, the theorem gives no information about $|z| = R$. In this case, we can have any possible subset of this circle convergent.
\end{remarks}
Using ideas from the section on Series, we can get useful ways to find the radius of convergence.
\begin{lemma}
    If $\left|\frac{a_{n+1}}{a_n}\right| \to l$ as $n \to \infty$, then $R = \frac{1}{l}$. If $l = \infty$, then $R = 0$ and if $l = 0$ then $R = \infty$.
    \label{lemRatioReciprocalRadius}
\end{lemma}
\begin{proof}
    Use the ratio test. Consider:
    \begin{align*}
	\lim_{n \to \infty} \left|\frac{a_{n+1} z^{n+1}}{a_n z^n}\right| = \lim_{n \to \infty} \left|\frac{a_{n+1}}{a_n}\right| |z| = l|z|
    \end{align*}
    Therefore by the ratio test, if $l|z| < 1$ the power series converges absolutely, and if $l|z| > 1$ then the power series diverges.

    If $l = \infty$, then $l|z|$ is always greater than $1$ for positive $|z|$, and thus the series diverges for all $z \neq 0$.

    If $l = 0$, any finite $z$ has $l|z| = 0$ and thus the power series converges on all $\C$, $R = \infty$.
\end{proof}
\begin{lemma}
    If $|a_n|^\frac{1}{n} \to l$ then $R = \frac{1}{l}$.
    \label{lemRootReciprocalRadius}
\end{lemma}
\begin{proof}
    Consider:
    \begin{equation*}
        \lim_{n \to \infty} \left(a_n z^n\right)^{\frac{1}{n}} = \lim_{n \to \infty} z (a_n)^{\frac{1}{n}}
    \end{equation*}
    The proof then proceeds as before.
\end{proof}
\begin{examples}{}
    \item $\sum_{n=0}^\infty \frac{z^n}{n!}$ has ratio of terms going to $0$, so the radius of convergence is $\infty$ and the series converges on the whole of $\C$.
    \item $\sum_{n=0}^\infty n! z^n$ has ratio of terms going to $\infty$, so the radius of convergence is $0$, and the series diverges everywhere except $z = 0$.
    \item $\sum_{n=0}^\infty z^n$ has radius of convergence $1$. Note that in this case the series diverges everywhere on the boundary circle.
    \item $\sum_{n=1}^\infty \frac{z^n}{n^2}$ has convergence for $|z| \leq 1$ by comparison to the series in $\frac{1}{n^2}$. Therefore, the series converges on the boundary circle.
    \item $\sum_{n=1}^\infty \frac{z^n}{n}$ has radius of convergence 1. For $z = 1$ the series diverges (this is the harmonic series). For $|z| = 1$, $z \neq 1$, consider:
        \begin{align*}
		(1-z)\sum_{n=1}^N \frac{z^n}{n} &= \sum_{n=1}^N \left(\frac{z^n}{n} - \frac{z^{n+1}}{n}\right) \\
		&= \sum_{n=1}^N \left(\frac{z^{n+1}}{n+1} - \frac{z^{n+1}}{n}\right) + z - \frac{z^{N+1}}{N+1} \\
		&= z - z \sum_{n=1}^N \frac{1}{n(n+1)} z^n - \frac{z^{N+1}}{N+1}
	\end{align*}
	And therefore this series converges absolutely by comparison to the sum in $\frac{1}{n(n+1)}$. Therefore, $\sum_{n=1}^\infty \frac{z^n}{n}$ converges for $|z| \leq 1$
\end{examples}
So we have examples of divergence, absolute convergence, and conditional convergence except at a single point. This illustrates the fact that we can find any set of convergence on the circle $|z| = R$.
\subsection{Differentiating a Power Series(*)}
We can show that inside the disc, power series are well-behaved.
\begin{theorem}
    Consider the function:
    \begin{equation*}
        f(z) = \sum_{n = 0}^\infty a_n z^n
    \end{equation*}
    and suppose it has radius of convergence $R$. Then $f$ is differentiable on $\subsetselect{z}{|z| < R}$ with derivative:
    \begin{equation*}
        f'(z) = \sum_{n = 0}^\infty n a_n z^{n-1}
    \end{equation*}
    \label{thmPowerSeriesDerivative}
\end{theorem}
\begin{remark}
    This theorem can be iterated, since the result of a differentiation is a power series, so power series are infinitely differentiable inside their radii of convergence.
\end{remark}
The rest of this subsection is \textbf{non-examinable}.\par
We need some lemmas in order to prove theorem~\ref{thmPowerSeriesDerivative}.
\begin{lemma}
    If $f(z)$ has radius of convergence $R$, then so do:
    \begin{equation*}
        \sum_{n = 0}^\infty n a_n z^{n-1} \text{ and } \sum_{n = 0}^\infty n(n-1) a_n z^{n-2}
    \end{equation*}
    \label{lemPowerSeriesDerivativesConverge}
\end{lemma}
\begin{proof}
    Suppose that $0 < |z| < R$ (note that the result trivially holds when $z = 0$).\par
    Then there exists $r$ such that $|z| < r < R$. We know that $\sum_n a_n r^n$ converges absolutely, so $a_n r^n \to 0$ and there exists some $K$ that bounds the sequence:
    \begin{equation*}
        |a_n r^n| < L~\forall n \geq 0
    \end{equation*}
    Therefore we can bound:
    \begin{align*}
        |n a_n z^{n-1}| &\leq \frac{|a_n r^n|}{|z|} n \left|\frac{z}{r}\right|^n \\
        &\leq \frac{K}{|z|} n \rho^n \text{ for } \rho = \frac{|z|}{r} < 1
    \end{align*}
    But note that this converges by the ratio test: the ratio of terms tends to $\rho < 1$ and therefore the series converges absolutely. Therefore, the required series converges by the comparison test.\par
    Also, suppose that $\sum_n n a_n w^{n-1}$ converges for some $|w| > R$. Then $\sum_n n a_n z^{n-1}$ converges absolutely for some $z$ with $|w| > |z| > R$.
    \begin{equation*}
        |n a_n z^{n-1}| \geq \frac{1}{|z|} |a_n z^n|
    \end{equation*}
    Which implies that $\sum_n a_n z^n$ converges absolutely.\contradiction\par
    So $\sum_n n a_n z^{n-1}$ has radius of convergence $R$. Iterating the same process with this new series gives the second required result.
\end{proof}
\begin{lemma}
    We can bound: 
    \begin{equation*}
        \choose{n}{r} \leq n(n-1)\choose{n-2}{r-2}$ for any $n, r \in \N$, $2 \leq r \leq n
    \end{equation*}
    And further,
    \begin{equation*}
        |(z + h)^n - z^n - nhz^{n-1}| \leq n(n-1)[|z| + |h|]^{n-2} |h|^2
    \end{equation*}
    \label{lemChooseBounds}
\end{lemma}
\begin{proof}
    The first is simple:
    \begin{align*}
        \choose{n}{r} \div \choose{n-2}{r-2} &= \frac{n!}{r!(n-r)!} \times \frac{(n-r)!(r-2)!}{(n-2)!} \\
        &= \frac{n(n-1)}{r(r-1)} \\
        &\leq n(n-1)
    \end{align*}
    as required. Then consider the second expression:
    \begin{align*}
        |(z + h)^n - z^n - nhz^{n-1}| &= \left|\sum_{r-2}^n \choose{n}{r} z^{n-r} h^r\right| \\
        &\leq \sum_{r=2}^n \choose{n}{r} |z|^{n-r} |h|^r \\ 
        &\leq \sum_{r=2}^n n(n-1) \choose{n-2}{r-2} |z|^{n-r} |h|^{r-2} |h|^2 \\ 
        &\leq n(n-1) |h|^2 \left(|h| + |z|\right)^{n-2}
    \end{align*}
    as required.
\end{proof}
Now we can prove the theorem:
\begin{proof}[of theorem~\ref{thmPowerSeriesDerivative}]
    By lemma~\ref{lemPowerSeriesDerivativesConverge}, we can define:
    \begin{equation*}
        g(z) = \sum_{n=0}^\infty na_n z^{n-1}
    \end{equation*}
    for $|z| < R$.\par
    Consider the difference quotient:
    \begin{equation*}
        I = \frac{f(z + h) - f(h) - hg(z)}{h}
    \end{equation*}
    Now, fix $z$ such that $|z| < R$ and assume that $|z| + |h| < r < R$ for some $r$.
    \begin{equation*}
        I = \frac{1}{h} \sum_{n = 0}^\infty \left[(z + h)^n - z^n - hnz^{n-1}\right]a_n
    \end{equation*}
    Then consider its magnitude:
    \begin{align*}
        |I| &= \frac{1}{|h|} \left|\lim_{N \to \infty} \sum_{n = 0}^N a_n\left[(z _ h)^n - z^n - hnz^{n-1}\right]\right| \\
        &= \lim_{N \to \infty} \left|\frac{1}{|h|} \sum_{n = 0}^N a_n\left[(z _ h)^n - z^n - hnz^{n-1}\right]\right| \text{ by continuity of } |z|
    \end{align*}
    Therefore, define:
    \begin{align*}
        I_n &= \frac{1}{n} \sum_{n = 0}^N a_n \left[(z + h)^n - z^n - hnz^{n-1}\right] \\
       |I_n| &\leq \frac{1}{|h|} \sum_{n = 0}^N |a_n| |(z + h)^n - z^n - hnz^{n-1}| \\
       &\leq \frac{1}{h} \sum_{n = 0}^N |a_n| n(n-1) \left(|z| + |h|\right)^{n-2} |h|^2 \text{ by lemma~\ref{lemChooseBounds}}\\
       &\leq |h| \sum_{n = 0}^N n(n-1) |a_n| r^{n-2} \\
       &\leq |h| \sum_{n = 0}^\infty n(n-1) |a_n| r^{n-2} = |h| A_r
    \end{align*}
    and the final sum converges by the assumption $r < R$ and lemma~\ref{lemPowerSeriesDerivativesConverge}.
    So we have that $|I| \leq |h| A_r$, and $|h| A_r \to 0$ as $h \to 0$, as required.
\end{proof}
\section{Exponential Functions}
We saw previously that the sum:
\begin{equation*}
    \sum_{n = 0}^\infty \frac{z^n}{n!}
\end{equation*}
converges for all $z \in \C$.

We define initially a function $e$:
\begin{align*}
    e : \C &\mapsto \C \\
    z &\mapsto e(z) = \sum_{n = 0}^\infty \frac{z^n}{n!}
\end{align*}
We immediately have that $e$ is differentiable, and also that its derivative is itself.
\begin{proposition}
    \begin{equation*}
        e(a + b) = e(a) e(b)
    \end{equation*}
    \label{propExpAdditivity}
\end{proposition}
\begin{proof}
    To show this, we need the following fact:\par
    \begin{subproof}{If $F : \C \mapsto \C$ satisfies $F'(z) = 0~\forall z \in \C$, then $F$ is constant.}
        For each $z \in \C$, consider $g : \R \mapsto \C$ such that $g(t) = F(tz)$. By the chain rule, $g'(t) = zF'(tz) = 0$.\par
        Then, if we write $g(t) = u(t) + iv(t)$ for real functions $u$ and $v$, we must have that $g'(t) = u'(t) + iv'(t)$, both of which must be 0. Now we must have that $u$ and $v$ are constant, and therefore $g$ is constant, and so is $F$ by corollary~\ref{corDerivativeSign}.
    \end{subproof}
    Now for any $a, b$ consider $F(z) = e(a + b - z) e(z)$. This is differentiable on all $\C$ with derivative:
    \begin{align*}
        F'(z) &= -e'(a + b - z) e(z) + e(a + b - z) - e'(z) \\
        &= -e(a + b - z) e(z) + e(a + b - z) - e(z) = 0\\
    \end{align*}
    Now we have that $F$ is constant, so $F(0) = F(b)$:
    \begin{align*}
        e(a + b) e(0) &= e(a) e(b) \\
        e(a + b) \sum_{n = 0}^\infty \frac{0^n}{n!} &= e(a) e(b) \\
        e(a + b) &= e(a) e(b)
    \end{align*}
\end{proof}
We can also consider the function $e$ with real inputs. There are no complex coefficients in the definition so $e : \R \mapsto \R$.
\begin{propositions}{
        Consider this function $e : \R \mapsto \R$:
        \label{propsExpRealProps}
    }   
    \item $e$ is everywhere differentiable with derivative itself \label{propExpDifferentiableReal}
    \item $e(x + y) = e(x) e(y)$ \label{propExpAdditivityReal}
    \item $e(x)$ is strictly positive \label{propExpPositive}
    \item $e$ is strictly increasing \label{propExpIncreasing}
    \item $e(x) \to \infty$ as $x \to \infty$, and $e(x) \to 0$ as $x \to -\infty$ \label{propExpLimits}
    \item $e : \R \mapsto (0, \infty)$ is a bijection. \label{propExpBijection}
\end{propositions}
\begin{proof}
    The first two claims are already proven.
    \begin{enumerate}
        \setcounter{enumi}{2} %Start at 3
        \item If $x > 0$ then clearly $e(x) = \sum_{n = 0}^\infty \frac{x^n}{n!} > 0$ since all coefficients are positive.
        
            If $x = 0$, $e(0) = 1$ and if $e < 0$, note that $e(x - x) = e(x) e(-x) = 1$, so $e(x)$ must be positive since $e(-x)$ is positive by the first case.
        \item $e'(x) = e(x) > 0$ so $e$ is strictly increasing for all $x$.
        \item For $x > 0$, all terms are positive so we can bound below by the first two:
            \begin{equation*}
                e(x) > 1 + x 
            \end{equation*}
            And therefore as $x \to \infty$, $1 + x \to \infty$, so $e(x) \to \infty$.

            For the case $x < 0$, recall that $e(x) = (e(-x))^{-1}$. Therefore, as $x \to -\infty$, $-x \to \infty$ and we must have $e(x) \to 0$.
        \item Consider the map $e : \R \mapsto (0, \infty)$.
            Injectivity is immediate since $e$ is increasing (see the proof of theorem~\ref{thmInverseFunctionII}). For surjectivity, suppose that $y \in (0, \infty)$. Then due to the limits of $e(x)$ as $x$ tends to positive or negative infinity, there must exist $a$ and $b$ such that:
            \begin{equation*}
                e(a) < y < e(b)
            \end{equation*}
            We can then use theorem~\ref{thmInverseFunctionII} on this restricted domain to show that $e$ is surjective.
    \end{enumerate}
\end{proof}
From propositions~\ref{propExpAdditivity} and \ref{propExpBijection}, we note that $e$ is a group isomorphism from the group of real number under addition to the group of positive real numbers under multiplication.\par
Since $e$ is a bijection in this way, it has an inverse $l = e^{-1} : (0, \infty) \mapsto \R$.
\begin{propositions}{
        Consider $l : (0, \infty) \mapsto \R$.
        \label{propsLogRealProps}
    }
    \item $l : (0, \infty) \mapsto \R$ is a bijection and is the inverse of $e$. \label{propLogInverseOfExp}
    \item $l$ is differentiable and $l'(t) = \frac{1}{t}$ \label{propLogDerivative}
    \item $l(st) = l(s) + l(t)$ \label{propLogMultity}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item This is immediate from the definition of $l$.
        \item Theorem~\ref{thmInverseFunctionII} gives that $l$ is differentiable, and we can find this derivative:
            \begin{equation*}
                l'(t) = \frac{1}{e'(l(t))} = \frac{1}{e(l(t))} = \frac{1}{t}
            \end{equation*}
        \item See IA Groups, using the isomorphism already established.
    \end{enumerate}
\end{proof}
\begin{remark}
    $l(t) \to -\infty$ as $t \to 0^+$ and $l(t) \to \infty$ as $t \to \infty$.
\end{remark}
Define, for any $\alpha \in \R$, and for $x \in (0, \infty)$:
\begin{equation*}
    r_{\alpha}(x) = e(\alpha l(x))
\end{equation*}
\begin{propositions}{
        Suppose that $x, y > 0$, $\alpha, \beta \in \R$.
        \label{propsPwrProps}
    }
    \item $r_\alpha(xy) = r_\alpha(x) r_\alpha(y)$ \label{propPrwMultiply}
    \item $r_{\alpha + \beta}(x) = r_\alpha(x)r_\beta(x)$ \label{propPwrSum}
    \item $r_\alpha(r_\beta(x)) = r_{\alpha\beta}(x)$ \label{propPwrChain}
    \item $r_1(x) = x$ \label{propPwr1Identity}
    \item $r_0(x) = 1$ \label{propPwrZero}
\end{propositions}
\begin{proof}
    All proofs using the properties already established about $e$ and $l$.
\end{proof}
Now suppose that $p$ and $q$ are positive. Consider $r_p(x)$. We can write this as:
\begin{equation*}
    r_{\sum_{k=1}^p 1}
\end{equation*}
and we can iterate proposition~\ref{propPwrSum} to get $\prod_{k = 1}^p r_1(x) = x^p$ as required.

Now consider $r_{1/q}(x)$:
\begin{align*}
    \left(r_{1/q}(x)\right)^q &= \prod_{k = 1}^q r_{1/q}(x) \\
    &= r_{\sum_{k = 1}^q 1/q}(x) \\
    &= r_1(x) = x
\end{align*}
And therefore $r_{1/q} = x^{\frac{1}{q}}$ as we defined.\par
Also note that:
\begin{align*}
    r_\alpha(x) r_{-\alpha}(x) &= r_0(x) \\
    &=1 \\
    \therefore r_{-\alpha} &= \frac{1}{r_\alpha(x)}
\end{align*}
Now we can show that $e(x) \equiv \exp(x)$ and denote $l$ by $\log(x)$. Further, $x^\alpha = r_\alpha(x)$.\par
We define $e = \sum_{n=0}^\infty \frac{1}{n!}$ and then $\exp(x) = e^x$.\par
By the chain rule,
\begin{align*}
    \frac{d}{dx}(x^\alpha) &= \frac{d}{dx}(e^{\alpha \log(x)}) \\
    &= e^{\alpha \log(x)} \frac{\alpha}{x} \\
    &= \alpha x^{\alpha - 1}
\end{align*}
Or, differentiating with respect to the exponent:
\begin{align*}
    \frac{d}{dx} (a^x) &= \frac{d}{dx}(e^{x \log(a)}) \\
    &= \log(a) e^{x \log(a)} \\
    &= \log(a) a^x
\end{align*}
\begin{propositions}{
        For any $r > 0$ we have that:
        \label{propsExpLimits}
    }
    \item $x^r e^{-x} \to 0$ as $x \to \infty$
    \item $x^{-r} \log(x) \to 0$ as $x \to \infty$
    \item $x^r \log(x) \to 0$ as $x \to 0^+$
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item For $x > 0$,
            \begin{equation*}
                e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots > \frac{x^n}{n!}
            \end{equation*}
            for any $n \in \N$. Choose $n$ such that $n > r$. Then:
            \begin{equation*}
                0 \leq \frac{x^r}{e^x} \leq x^r \frac{n!}{x^n} = \frac{n!}{x^{n-r}}
            \end{equation*}
            And thus as $x \to \infty$, the upper bound goes to $0$, so $x^r e^{-x} \to 0$.
        \item Choose $m$ such that $m \geq \frac{2}{r}$.
            \begin{equation*}
                0 \leq \frac{x^{\frac{1}{r}}}{e^x} \leq \frac{m!}{x^{m - \frac{1}{r}}} \leq \frac{m!}{x^{\frac{1}{r}}}
            \end{equation*}
            Then for $y$ sufficiently large, let $x = \log(y)$.
            \begin{equation*}
                0 \leq \frac{(\log(y))^\frac{1}{r}}{y} \leq \frac{m!}{(\log(y))^\frac{1}{r}}
            \end{equation*}
            Then since raising to a positive power preserves inequalities:
            \begin{equation*}
                0 \leq \frac{\log(y)}{y^r} \leq \frac{m!}{\log(y)}
            \end{equation*}
            The upper bound goes to $0$ as $y \to \infty$, so $y^{-r} \log(y)$ must go to $0$.
        \item Let $z = y^{-1}$. Then from above:
            \begin{equation*}
                0 \leq z^r \log(z) \leq \frac{(m!)^r}{-\log(z)}
            \end{equation*}
            And as $z \to 0^+$, the upper bound goes to $0$, so $z^r \log(z) \to 0$.
    \end{enumerate}
\end{proof}
\section{Trigonometric Functions}
\subsection{Defining the Trigonometric Functions}
We define the trigonometric functions in terms of their power series:
\begin{definition}{Cosine}
    For $z \in \C$, define the \underline{cosine} of $z$ to be:
    \begin{equation*}
        \cos(z) = \sum_{k = 0}^\infty (-1)^k \frac{z^{2k}}{(2k)!}
    \end{equation*}
\end{definition}
\begin{definition}{Sine}
    For $z \in \C$, define the \underline{sine} of $z$ to be:
    \begin{equation*}
        \sin(z) = \sum_{k = 0}^\infty (-1)^k \frac{z^{2k+1}}{(2k+1)!}
    \end{equation*}
\end{definition}
Both series have infinite radius of convergence (by the Ratio Test), and we can therefore term-by-term differentiate this to give:
\begin{align*}
    \frac{d}{dx} \cos(x) &= -\sin(x) \\
    \frac{d}{dx} \sin(x) &= \cos(x) \\
\end{align*}
\subsection{Important Identities}
We also observe that:
\begin{align*}
    e^{iz} &= \lim_{n \to \infty} \left(\sum_{k = 0}^{2k + 1} \frac{(iz)^k}{k!}\right) \\
    &= \lim_{n \to \infty} \left(\sum_{k = 0}^n \frac{(iz)^{2k}}{(2k)!} + \sum_{k = 0}^n \frac{(iz)^{2k+1}}{(2k+1)!}\right) \\
    &= \lim_{n \to \infty} \left(\sum_{k = 0}^n (-1)^k \frac{z^{2k}}{(2k)!} + i\sum_{k = 0}^n (-1)^k \frac{z^{2k+1}}{(2k+1)!}\right) \\
    &= \cos(z) + i\sin(z)
\end{align*}
And therefore:
\begin{align*}
    \cos(z) &= \frac{1}{2} \left(e^{iz} + e^{-iz}\right) \\
    \sin(z) &= \frac{1}{2i} \left(e^{iz} - e^{-iz}\right)
\end{align*}
This gives many trigonometric identities. For example, $\cos(z) = \cos(-z)$, $\sin(z) = -\sin(-z)$.\par
We also have addition formulae. For complex numbers $z$ and $w$:
\begin{align}
    \sin(z + w) &= \sin(z) \cos(w) + \cos(z) \sin(w) \label{eqnSinAddition} \\
    \cos(z + w) &= \cos(z) \cos(w) - \sin(z) \sin(w) \label{eqnCosAddition}
\end{align}
Also, setting $w = -z$ in equation~\ref{eqnCosAddition} gives:
\begin{equation}
    \cos^2(z) + \sin^2(z) = 1
    \label{eqnSquareTrig}
\end{equation}
Further, if $x$ is a real number then so are its sine and cosine (by considering its power series), and so equation~\ref{eqnSquareTrig} implies that $|\sin(x)|$ and $|\cos(x)|$ are both less than or equal to $1$. Note that this need not be true for complex arguments.
\subsection{Periodicity of Trigonometric Functions}
\begin{lemma}
    There exists a smallest positive integer $\omega$, where $\sqrt{2} < \frac{\omega}{2} \sqrt{3}$ such that:
    \begin{equation*}
        \cos\left(\frac{\omega}{2}\right) = 0
    \end{equation*}
    \label{lemRootOfCos}
\end{lemma}
\begin{proof}
    Suppose $0 \leq x \leq 2$. Then $\cos(x)$ is:
    \begin{align*}
        \sin(x) &= \left(x - \frac{x^3}{3!}\right) + \cdots + \frac{x^{2n-1}}{(2n-1)!}\left(1 - \frac{x^2}{2n(2n+1)}\right) + \cdots \\
        &>0
    \end{align*}
    And also $\frac{d}{dx} \cos(x) = -\sin(x)$, so $\cos(x)$ is strictly decreasing on the interval $[0, 2]$ by corollary~\ref{corDerivativeSign}. Therefore $\cos$ has at most one root in $[0, 2]$. We now consider the interval $\left[\sqrt{2}, \sqrt{3}\right]$.\par
    This is a subset of $[0, 2]$ so $\cos$ is decreasing. We consider the sign of $\cos$ at either end of the interval.\par
    \begin{align*}
        \cos(\sqrt{2}) &= 1 - \frac{(\sqrt{2})^2}{2!} + \frac{(\sqrt{2})^4}{4!} + \cdots \\
        &= \sum_{n = 0}^\infty \frac{(\sqrt{2})^{4n}}{(4n)!}\left(1 - \frac{2}{(4n + 1)(4n + 2)}\right) \\
        &>0
    \end{align*}
    \begin{align*}
        \cos(\sqrt{3}) &= 1 - \frac{(\sqrt{3})^2}{2!} + \frac{(\sqrt{3})^4}{4!} - \left(\frac{(\sqrt{3})^6}{6!} - \frac{(\sqrt{3})^8}{8!}\right) \\
        &- \cdots - \frac{(\sqrt{3})^{4n+2}}{(4n+2)!}\left(1 - \frac{3}{(4n + 3)(4n + 4)}\right)
    \end{align*}
    Which are definitely less than zero except the first few terms which must be checked directly:
    \begin{equation*}
        1 - \frac{3}{2} + \frac{9}{24} = -\frac{1}{8}
    \end{equation*}
    So $\cos{\sqrt{3}} < 0$.
    Therefore by theorem~\ref{thmIntermediateVal}, there exists at least one root in $[\sqrt{2}, \sqrt{3}]$.\par
    Therefore there is exactly one root in $[0, 2]$ with the required bounds.
\end{proof}
\begin{corollary}
    \begin{equation*}
        \sin\left(\frac{\omega}{2}\right) = 1
    \end{equation*}
\end{corollary}
\begin{proof}
    We have that $\cos\left(\frac{\omega}{2}\right) = 0$, so $\sin\frac{\omega}{2} = \pm 1$ by equation~\ref{eqnSquareTrig}. In the proof of lemma~\ref{lemRootOfCos}, it was shown that $\sin$ is positive on this interval, so $\sin\frac{\omega}{2} = 1$.
\end{proof}
We will now define $\pi$ to be $\omega$
\begin{theorem}
    For all $z \in \C$:
    \begin{align*}
        \sin\left(z + \frac{\pi}{2}\right) &= \cos(z) \\
        \sin\left(z + \pi\right) &= -\sin(z) \\
        \sin\left(z + 2\pi\right) &= \sin(z)
    \end{align*}
    \label{thmTrigPeriodicity}
\end{theorem}
\begin{proof}
    All proofs by lemma~\ref{lemRootOfCos} and addition formulae.
\end{proof}
Note that this immediately gives:
\begin{align*}
    e^{iz + 2\pi i} &= \cos(z + 2\pi) + i\sin(z + 2\pi) \\
    &= \cos(z) + i\sin(z) = e^{-iz}
\end{align*}
So $e^z$ also has period $2\pi$. Note also that $e^{i\pi} = -\cos(0) + i(\sin(0)) = -1$.\par
\begin{remark}
    Given a pair of vectors $\vec{x}, \vec{y} \in \R^2$, define:
    \begin{equation*}
        \vec{x} \cdot \vec{y} = x_1 y_1 + x_2 y_2
    \end{equation*}
    Where $x_i$ are the components of the vectors.\par
    Define also the length of a vector:
    \begin{equation*} 
        ||\vec{x}|| = \sqrt{\vec{x} \cdot \vec{x}}
    \end{equation*}
    From the Cauchy-Schwarz Inequality:
    \begin{equation*}
        |\vec{x} \cdot \vec{x}| \leq ||\vec{x}||~~||\vec{y}||
    \end{equation*}
    So for $\vec{x}, \vec{y} \neq 0$,
    \begin{equation*}
        \frac{\vec{x} \cdot \vec{y}}{||\vec{x}||~~||\vec{y}||} \leq 1
    \end{equation*}
    We then define the angle between $\vec{x}$ and $\vec{y}$ to be the unique $\theta \in [0, \pi]$ such that:
    \begin{equation*}
        \cos{\theta} = \frac{\vec{x} \cdot \vec{y}}{||\vec{x}||~~||\vec{y}||}
    \end{equation*}
    And in this way we can rigorously define angles.
\end{remark}
\subsection{Hyperbolic Functions}
We can also define:
\begin{definition}{Hyperbolic Cosine}
    The function $\cosh$ is defined as:
    \begin{equation*}
        \cosh(z) = \frac{1}{2}\left(e^z + e^{-z}\right)
    \end{equation*}
\end{definition}
\begin{definition}{Hyperbolic Sine}
    The function $\sinh$ is defined as:
    \begin{equation*}
        \sinh(z) = \frac{1}{2}\left(e^z - e^{-z}\right)
    \end{equation*}
\end{definition}
We have various identities, which can be derived easily from these definitions where required.
\end{document}