\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Fundamental Theorem of Algebra}
\begin{definition}{Multiplicity}
    Let $P(z)$ be a polynomial. If $(z-\omega)^k$ divides $P(z)$ but $(z-\omega)^{k+1}$ does not, we say $\omega$ is a $k$-times repeated root, or $\omega$ has \underline{multiplicity} $k$.
\end{definition}
\begin{theorem}[Fundamental Theorem of Algebra]
    Let $P(z)$ be a polynomial of degree $m$. That is, $P(z) = \sum_{j=1}^{n}c_j z^j$ where $c_j \in \mathbb{C}$ and $C_m \neq 0$. Then there are exactly $m$ solutions to the equation $P(z) = 0$ when counting multiplicity.
    \label{thmFundamenalAlgebra}
\end{theorem}
For the proof, see Part IB Complex Analysis.
\begin{corollary}
    $P(z) = c_m \prod_{j=1}^{m}(z-\omega_j)$ where $\omega_j$ solve $P(z) = 0$.
\end{corollary}
\section{Definitions and Remarks}
\begin{definition}{Eigenvectors and Eigenvalues}
    Consider a linear map $\alpha : \mathbb{C}^n \mapsto \mathbb{C}^n$ with associated matrix $A$. Then $\vec{x} \neq \vec{0}$ is an \underline{eigenvector} and $\lambda$ is the associated \underline{eigenvalue} if:
    \begin{equation}
        A \vec{x} = \lambda \vec{x} \label{eqnEgnVecDef}
    \end{equation}
\end{definition}
Hence we get the following:
\begin{eqnarray}
    (A-\lambda I)\vec{x} = \vec{0} \label{eqnEgnVecMatrix}\\
    \implies \vec{x} \in \ker{(A-\lambda I)} \label{eqnEgnVecInKer}\\
    \implies \det{(A-\lambda I)} = 0 \label{eqnEgnVecDet}
\end{eqnarray}
\underline{Remarks:}
\begin{enumerate}
    \item If $\det{(A-\lambda I)} = 0$, then there is at least one solution to Equation~\ref{eqnEgnVecMatrix}, which implies that $\lambda$ is an eigenvalue if and only if \ref{eqnEgnVecDet} is satisfied.
    \item Equation~\ref{eqnEgnVecDet} is the \underline{characteristic equation} of $A$. We also have the \underline{characteristic polynomial} of A, since the determinant equation is a polynomial in $\lambda$.
\end{enumerate}
\section{More on the Characteristic Polynomial}
We define the characteristic polynomial as the determinant of the matrix $A - \lambda I$.
\begin{equation}\label{eqnCharPolynomial}
    P_A(\lambda) = \epsilon_{j_1 j_2 \cdots j_n}(A_{j_1 1} - \lambda \delta_{j_1 1})(A_{j_2 2} - \lambda \delta_{j_2 2}) \cdots (A_{j_n n} - \lambda \delta_{j_n n})
\end{equation}
We can see that this is $n$-degree polynomial in $\lambda$. We can then write it in terms of the coefficients of $\lambda$:
\begin{equation}\label{eqnCharPolynomialCoeffts}
    P_A(\lambda) = c_0 + c_1 \lambda + \cdots + c_n \lambda^n
\end{equation}
We can now use the theorems we know about roots of polynomials.
\begin{enumerate}
    \item $P_A(\lambda)$ has $n$ roots, accounting for multiplicity. So an $n \times n$ matrix has $n$ eigenvalues, accounting for multiplicity.
    \item If $A$ is real, then all the coefficients of \ref{eqnCharPolynomialCoeffts} are real numbers. Hence, all of the eigenvectors are real or come in complex conjugate pairs.
    \item From \ref{eqnCharPolynomial}, the coefficients of $\lambda^n$ and $\lambda^{n-1}$ come from the single term: $(A_{1 1} - \lambda)(A_{2 2} - \lambda) \cdots (A_{n n} - \lambda)$. Thus the coefficient $c_n$ is $(-1)^n$, and the coefficient $c_{n-1}$ is $(-1)^{n-1}(A_{1 1} + A_{2 2} + \cdots + A_{n n})$, which is $(-1)^{n-1} Tr(A)$.
    \item We also know that the sum of the eigenvalues is equal to $(-1)^{n-1}$, so we have that the sum of eigenvectors is equal to the trace of the matrix.
    \item $c_0$ is equal to the value of $P_A(0)$. Using Equation~\ref{eqnEgnVecDet}, we get that this is $\det{A}$. So, using roots of polynomials results, the product of the eigenvectors is equal to the determinant of the matrix.
\end{enumerate}
\section{Eigenspaces and Multiplicity}
The kernel of the matrix $(A - \lambda I)$ is:
\begin{equation*}
\{\vec{x} \in \mathbb{C}^n | A\vec{x} = \lambda \vec{x}\}
\end{equation*}
Note that it includes the zero vector, which is not an eigenvector.
\begin{definition}{Eigenspace}
    We have already shown that the kernel forms a subspace, so let the subspace formed by this kernel be the \underline{eigenspace}, and denote it $e_\lambda$.
\end{definition}
\begin{definition}{Algebraic multiplicity}
    The \underline{algebraic multiplicit}, $M_\lambda$, of an eigenvalue $\lambda$ is the multiplicity of $\lambda$ as a root of the characteristic polynomial.
\end{definition}
If $M_\lambda < 1$ then $\lambda$ is said to be \underline{degenerate}.
\begin{definition}{Geometric multiplicity}
    The \underline{geometric multiplciity}, $m_\lambda$, is the maximum number of linearly independent eigenvectors corresponding to the eigenvalue $\lambda$. That is, $m_\lambda = \dim{e_\lambda}$.
\end{definition}
\begin{definition}{Defect}
    The \underline{defect}, $\Delta_\lambda$ is defined by $\Delta_\lambda = M_\lambda - m_\lambda$. It is always non-negative.
\end{definition}
\begin{theorem}
    Suppose an $n\times n$ matrix $A$ has $n$ distinct eigenvalues $\lambda_1, \lambda_2, \cdots, \lambda_n$. Then the corresponding eigenvectors are linearly independent.
    \label{thmLIEigenvectors}
\end{theorem}
\begin{proof}
Assume, on the contrary, that the eigenvectors $\vec{x_1}, \vec{x_2}, \cdots, \vec{x_r}$ are linearly dependent, so $\exists d_i \text{ s.t. } \sum_{i=1}^{r}d_i x_i = \vec{0}$. After relabelling, let this be the shortest non-trivial combination of linearly dependent eigenvectors.\par
The apply the transformation $(A - \lambda_1 I)$:
\begin{equation*}
    d_1(\lambda_1 - \lambda_1)\vec{x_1} + d_2(\lambda_2 - \lambda_1)\vec{x_2} + \cdots + d_r(\lambda_r - \lambda_1)\vec{x_r} = \vec{0}
\end{equation*}
But the first term is zero, so we have a new shortest combination of linearly dependent eigenvectors. \contradiction
\end{proof}
\begin{corollary}
    If $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors, that form a basis of the ambient space $\mathbb{R}^n$ or $\mathbb{C}^n$.
\end{corollary}
We then consider some examples of matrices in $\mathbb{R}^3$ and $\mathbb{C}^3$.
\begin{example}[Reflection matrix]
    Consider a reflection of the plane defined by the unit normal $\hat{v}$. Under the reflection, $\hat{n}\mapsto -\hat{n}$, so $\hat{n}$ is an eigenvector with eigenvalue $-1$. Therefore, $m_{-1}=1$.\par
    Furthermore, any vector in the plane is unchanged by the reflection, so we have a plane of eigenvectors with eigenvalue $1$. $m_1 = 2$. We have zero defect, so these eigenvectors span the space.
\end{example}
\begin{example}[Rotation matrix]
    For a rotation through angle $\theta$ about an axis parallel to $\hat{n}$, those vectors parallel to the axis are unchanged, so $\hat{n}$ is an eigenvector with eigenvalue $1$. $m_1 = 1$. If the rotation matrix acts on $\mathbb{R}^3$, there are no further eigenvalues and the eigenvectors do not span the space. However, we have the complex eigenvalues $e^{i\theta}$ and $e^{-i\theta}$, so in $\mathbb{C}^3$, the eigenvectors do span the space.
\end{example}
\begin{example}[Shear matrix]
    For a shear matrix in $\mathbb{R}^2$, the only eigenvalue is $1$, corresponding to the eigenvector $\begin{pmatrix}1 \\ 0\end{pmatrix}$. Thus, the eigenvectors do not span the space.
\end{example}
If an $n\times n$ matrix $A$ has $n$ distinct eigenvalues, then by Theorem~\ref{thmLIEigenvectors}, it must have $n$ linearly independent eigenvectors. That means, with respect to the eigenvector basis, A is \underline{diagonalisable}:
\begin{equation*}
    A =
    \begin{pmatrix}
        \lambda_1 & 0 & 0 & \cdots & 0 \\
        0 & \lambda_2 & 0 & \cdots & 0 \\
        0 & 0 & \lambda_3 & & \vdots \\
        \vdots & \vdots & & \ddots & 0 \\
        0 & 0 & \cdots & 0 & \lambda_n
    \end{pmatrix}
\end{equation*}
where $\lambda_i$ are the $n$ distinct eigenvalues.
\section{Change of Basis}
\subsection{Transforming Basis Vectors}
Consider the bases of $\C^n$, $\{\vec{e_1}, \vec{e_2}, \cdots, \vec{e_n}\}$ and $\{\tvec{e_1}, \tvec{e_2}, \cdots, \tvec{e_n}\}$\par
Let $\tvec{e_j} = \sum_{i=1}^{n} P_{ij} \vec{e_i}$, for some matrix $P$.\par
Then the element $P_{ij}$ is the $i$th component of $\tvec{e_j}$ with respect to the original basis. $P$ has the columns $\tvec{e_j}$ with respect to this original basis.\par
Similarly, we can write $\vec{e_j} = \sum_{i=1}^{n} Q_{ij} \tvec{e_i}$, and therefore $Q$ has the columns $\vec{e_i}$ with respect to the tilde basis.\par
Doing this process twice,
\begin{align*}
    \tvec{e_j} &= \sum_{i=1}^{n} \left(\sum_{k=1}^{n} Q_{ki} \tvec{e_k}\right) P_{ij} \\
    &= \sum_{k=1}^{n} \tvec{e_k} \sum_{i=1}^{n} Q_{ki} P_{ij}
\end{align*}
But since these are basis vectors, they are therefore linearly independent and so $QP = I$, and $Q = P^{-1}$.\par
\subsection{Transformation Law for Vectors}
Consider the bases $\vec{e_i}$ and $\tvec{e_i}$ as above. Then consider a vector represented by $\vec{v}$ in the original basis, and $\tvec{v}$ in the new basis.\par
Now, using the change-of-basis matrix $P$ from above,
\begin{align*}
    \vec{v} &= \sum_{j=1}^{n} \tilde{v}_j \tvec{e_j} \\
    &= \sum_{j=1}^{n} \tilde{v}_j \sum_{i=1}^{n} P_{ij} \vec{e_i} \\
    &= \sum_{i=1}^{n} \left(\sum_{j=1}^{n} \tilde{v}_j P_{ij}\right) \vec{e_i}
\end{align*}
And therefore, since $v = \sum_{i=1}^{n} v_i \vec{e_i}$, by matching up the final line:
\begin{equation}
    v_i = \sum_{j=1}^{n} \tilde{v}_j P_{ij}
\end{equation}
And, without index notation:
\begin{equation}
    \tvec{v} = P^{-1} \vec{v}
    \label{eqnVecTransform}
\end{equation}
\subsection{Transformation Law for Matrices}
Consider a linear map:
\begin{align*}
    \alpha : \C^n & \mapsto \C^n \\
    \vec{x} &\mapsto \vec{x}' = A\vec{x}
\end{align*}
Also consider the vectors $\vec{u}, \vec{u}' = Au, \tvec{u}$ and $\tvec{u}'$, where the bases are the standard basis and the $\tvec{e_i}$ basis.\par
Using the transformation matrix $P$ defined above:
\begin{align*}
    \vec{u}' &= A\vec{u} \\
    \implies P \tvec{u}' &= AP\tvec{u} \\
    \implies \tvec{u}' = P^{-1} A P \tvec{u}
\end{align*}
So the matrix $A$ in the new basis is:
\begin{equation}
    \tilde{A} = P^{-1} A P
    \label{eqnMatrixTransform}
\end{equation}
\section{Similar Matrices}
\begin{definition}{Similar matrices}
    Two square matrices $A$ and $B$ are \underline{similar} if there exists a matrix $P$ such that:
    \begin{equation*}
        B = P^{-1} A P
    \end{equation*}
    That is, they represent the same linear map under different bases.
\end{definition}
We have the following properties:
\begin{enumerate}
    \item $\det{A} = \det{B}$.
        \begin{align*}
            \det{B} &= \det{(P^{-1} A P)} \\
            &= \det{P^{-1}} \det{A} \det{P} \\
            &= \det{A} \frac{\det{P}}{\det{P}}~\tick
        \end{align*}
    \item $\tr{A} = \tr{B}$
        \begin{align*}
            B_{ii} &= [P^{-1}AP]_{ii} \\
            &= P_{ij}^{-1} A_{jk} P_{ki} \\
            &= P_{ij}^{-1} P_{ki} A_{jk} \\
            &= \delta_{kj} A_{jk} \\
            &= A_{jj}~\tick
        \end{align*}
    \item $A$ and $B$ have the same characteristic polynomial, and the same eigenvalues. Eigenvectors will be different because different bases are used.
        \begin{align*}
            P_B(\lambda) &= \det{(B - \lambda I)} \\
            &= \det{(P^{-1} A P - \lambda I)} \\
            &= \det{(P^{-1}[AP - \lambda I]P)} \\
            &= \det{(A - \lambda I)} \det{P} \det{P^{-1}}~\tick
        \end{align*}
\end{enumerate}
\subsection{Diagonal Matrices}
\begin{definition}{Diagonalisability}
An $n \times n$ matrix $A$ is \underline{diagonalisable} if it is similar to a diagonal matrix. That is, a matrix of the form:
    \begin{equation*}
        \begin{pmatrix}
            \lambda_1 & 0 & 0 & \cdots & 0 \\
            0 & \lambda_2 & 0 & \cdots & 0 \\
            0 & 0 & \lambda_3 & & \vdots \\
            \vdots & \vdots & & \ddots & 0 \\
            0 & 0 & \cdots & 0 & \lambda_n
        \end{pmatrix}
    \end{equation*}
\end{definition}
This is equivalent to the statement that the eigenvectors form a basis.\par
The requirement that $A$ has $n$ distinct eigenvalues is sufficient but not necessary for diagonalisability. We can use the eigenvector basis to diagonalise a matrix:
\begin{example}
    Define the matrix $A$:
    \begin{equation*}
        A = 
        \begin{pmatrix}
            -2 & 2 & -3 \\
            2 & 1 & 6 \\
            -1 & -2 & 0
        \end{pmatrix}
    \end{equation*}
    With eigenvectors:
    \begin{equation*}
        \begin{pmatrix}
            1 \\ 2 \\ -1
        \end{pmatrix},
        \begin{pmatrix}
            -2 \\ 1 \\ 0
        \end{pmatrix},
        \begin{pmatrix}
            3 \\ 0 \\ 1
        \end{pmatrix}
    \end{equation*}
    And therefore the change-of-basis matrix has these eigenvectors as its columns:
    \begin{equation*}
        P = 
        \begin{pmatrix}
            1 & -2 & 3 \\
            2 & 1 & 0 \\
            -1  & 0 & 1
        \end{pmatrix}
    \end{equation*}
    And since we have 3 linearly independent eigenvectors, we have the diagonal matrix:
    \begin{equation*}
        P^{-1} A P = 
        \begin{pmatrix}
            5 & 0 & 0 \\
            0 & -3 & 0 \\
            0 & 0 & -3
        \end{pmatrix}
    \end{equation*}
    Also written as $\diag{\{5, -3, -3\}}$. Note that we have a repeated eigenvalue but still $A$ is diagonalisable.
\end{example}
\begin{theorem}
    Let $\lambda_1, \lambda_2, \cdots, \lambda_r$ be distinct eigenvalues of an $n \times n$ matrix $A$. Note that $r \leq n$, as some eigenvalues may be repeated.\par
    Let the sets $B_1, B_2, \cdots, B_r$ be the bases of the eigenspaces $E_{\lambda_1}, E_{\lambda_2}, \cdots, E_{\lambda_r}$.\par
    Then the union of all of these sets is linearly independent.
    \label{thmEspacesLI}
\end{theorem}
\begin{proof}
    Let $B_1 = \{\vec{x_1}^{(1)}, \vec{x_2}^{(1)}, \cdots, \vec{x_{m_{\lambda_1}}}^{(1)}\}$, where $m_{\lambda_1}$ is the dimension of the eigenspace. Define the other $B_i$ similarly.\par
    Then consider the general linear combination of all elments of $B$:
    \begin{equation}
        \sum_{i=1}^{r} \sum_{j=1}^{m_{\lambda_i}} \alpha_{ij} \vec{x_j}^{(i)} = \vec{0}
        \label{eqnBasisLinearCombo}
    \end{equation}
    where $\alpha_{ij}$ are constants, not all zero.\par
    Now consider the matrix:
    \begin{equation*}
        M_K = \prod_{k = 1, 2, \cdots, \bar{K}, \cdots, r} (A - \lambda_k I)
    \end{equation*}
    And then left-multiply equation~\ref{eqnBasisLinearCombo} by it:
    \begin{align*}
        &\sum_{j=1}^{m_{\lambda_K}} \alpha_{Kj} \prod_{k = 1, \cdots, \bar{K}, \cdots, r} (A - \lambda_k I) \vec{x_j}^{(K)} \\
        &= \sum_{j=1}^{m_{\lambda_K}} \alpha_{Kj} \prod_{k = 1, \cdots, \bar{K}, \cdots, r} (A\vec{x_j}^{(K)} - \lambda_k \vec{x_j}^{(K)}) \\
        &= \sum_{j=1}^{m_{\lambda_K}} \alpha_{Kj} \prod_{k = 1, \cdots, \bar{K}, \cdots, r} (\lambda_K - \lambda_k) \vec{x_j}^{(K)} \text{ since } \vec{x_j}^{(K)} \text{ is an eigenvector } \\
    \end{align*}
    However, by assumption $\lambda_K \neq \lambda_k$, so the only term that can be zero is $\alpha_Kj$, and must be zero for all $j$.\par
    Since $K$ was chosen arbitrarily, we can use the same logic to get that all of the $\alpha_{ij}$ coefficients must be zero, and therefore the union is linearly independent.
\end{proof}
\begin{remark}
    We now have a very useful test for diagonalisability, since we can relate this to the defect of the matrix.\par
    For a matrix $A$, find its eigenvalues and corresponding eigenvectors. If there is no defect for any of the eigenvalues, that is, the matrix has no defect, then it is diagonalisable.
\end{remark}
\subsection{Canonical Form of a 2 \texorpdfstring{$\times$}{x} 2 matrix}
We can categorise any $2 \times 2$ complex matrix by its similar matrices. This is encompassed in the following proposition:
\begin{proposition}
    A $2 \times 2$ complex matrix $A$ is similar to:
    \begin{enumerate}
        \item $\begin{pmatrix}
            \lambda_1 & 0 \\
            0 & \lambda_2
        \end{pmatrix}, \lambda_1 \neq \lambda_2$
        \label{form2EVals}
        \item $\begin{pmatrix}
            \lambda & 0 \\
            0 & \lambda
        \end{pmatrix}$
        \label{form1EVal}
        \item $\begin{pmatrix}
            \lambda & 1 \\
            0 & \lambda
        \end{pmatrix}$.
        \label{formNonDiag}
    \end{enumerate}
\end{proposition}
\begin{proof}
    \begin{case}{$A$ has two distinct eigenvalues, $\lambda_1$ and $\lambda_2$}
        In this case the matrix is diagonalisable and we have form \ref{form2EVals}.
    \end{case}
    \begin{case}{$A$ has a repeated eigenvalue $\lambda$, but $\dim{(E_\lambda)} = 2$}
        In this case there is zero defect and therefore the matrix is diagonalisable, and we have form \ref{form1EVal}.
    \end{case}
    \begin{case}{$A$ has a repeated eigenvalue $\lambda$, and $\dim{(E_\lambda)} = 1$}
        This case has a defect of 1. Let $\vec{v}$ be an eigenvector. Also let $\vec{w}$ be linearly independent of $\vec{v}$, so $\spanset{\{\vec{v}, \vec{w}\}} = \C^2$.\par
        Now let $A\vec{w} = \alpha \vec{v} + \beta \vec{w}$. Change basis to $\{\vec{v}, \vec{w}\}$. In the new basis, then, $A$ becomes the matrix:
        \begin{equation*}
            \tilde{A} =
            \begin{pmatrix}
                \lambda & \alpha \\
                0 & \beta
            \end{pmatrix}
        \end{equation*}
        But since $A$ and $\tilde{A}$ must have the same characteristic equation (some multiple of $(\lambda - p)^2$), we must have that $\beta = \lambda$.\par
        Further, we introduce $\vec{u} = (\tilde{A} - \lambda_I) \vec{w}$.\par
        Note that $\vec{u}$ is non-zero, but that $(\tilde{A} - \lambda I) \vec{u} = (\tilde{A} - \lambda I)^2 \vec{v} = \vec{0}$, so $\vec{u}$ must be an eigenvector of $A$ and must have eigenvalue $\lambda$.\par
        We therefore have:
        \begin{align*}
            \vec{u} &= \tilde{A} \vec{w} - \lambda \vec{w} \\
            \implies \tilde{A} \vec{w} &= \vec{u} + \lambda \vec{w}
        \end{align*}
        And therefore changing basis again to $\{\vec{u}, \vec{w}\}$ gives the required matrix for form \ref{formNonDiag}.
    \end{case}
\end{proof}
\begin{remark}
    There is a generalisation of this result to n dimensions, not proved in this course, which looks like:
    \begin{equation*}
        \begin{pmatrix}
            \lambda_1 & \mu_1 & 0 & \cdots & 0 \\
            0 & \lambda_2 & \mu_2 & & \vdots \\
            0 & 0 & \lambda_3 & \ddots & 0 \\
            \vdots & \vdots & & \ddots & \mu_{n-1} \\
            0 & 0 & \cdots & 0 & \lambda_n
        \end{pmatrix}
    \end{equation*}
    where $\lambda_i$ are the eigenvectors, and $\mu_i$ are either zero or one.
\end{remark}
\subsection{Algebra with Diagonal Matrices}
Diagonalisation allows us to define algebraic operations with matrices, and then use matrices as arguments for functions.
\begin{theorem}
    Every $n \times n$ complex matrix $A$ satisfies its own characteristic equation.
\end{theorem}
\begin{proof}[for diagonalisable matrices]
    Let $D$ be the diagonal form of $A$, $D = P^{-1} A P$.\par
    Note that repeated multiplication is much simplier:
    \begin{equation*}
        D^k = P^{-1} A P P^{-1} A P \cdots P^{-1} A P = P^{-1} A^k P
    \end{equation*}
    This means we can put $D$ into its own characteristic equation:
    \begin{equation*}
        p_D(D) = p_D(P^{-1} A P) = p^{-1} p_D(A) P
    \end{equation*}
    But since these matrices are similar, they have the same characteristic equation:
    \begin{align*}
        p_A(D) &= P^{-1} p_A(A) P \\
        &= P^{-1} \diag{(p_A(\lambda_1), p_A(\lambda_2)), \cdots, p_A(\lambda_n)} P^{-1}  \\
        &= P^{-1} 0 P = 0 \text{ since all eigenvalues solve the characteristic equation}
    \end{align*}
\end{proof}
\begin{corollary}
    If $A^{-1}$ exists,
    \begin{equation*}
        A^{-1} p_A(A) = A^{-1} (c_0 I + c_1 A + \cdots + c_n A^n)
    \end{equation*}
    And $c_0$ is non-zero since det $A$ is invertible:
    \begin{equation*}
        A^{-1} = -\frac{1}{c_0} \left(c_1 I + c_2 A + \cdots + c_n A^{n-1}\right)
    \end{equation*}
\end{corollary}
\begin{remark}
    This is more an interesting other way to define the matrix inverse, it is by no means a better method of computing it.
\end{remark}
We can also define the matrix exponential! For a matrix $A$,
\begin{equation}
    e^A = I + A + \frac{1}{2} A^2 + \frac{1}{6} A^3 + \cdots
    \label{eqnMatrixExp}
\end{equation}
And then if $A$ is diagonalisable,
\begin{align*}
    P^{-1} e^A P &= P^{-1} I P + P^{-1} A P + \frac{1}{2}P^{-1} A^2 P + \cdots \\
    &= I + D + \frac{1}{2} D^2 + \cdots \\
    &= \diag{(e^{\lambda_1}, \cdots, e^{\lambda_n})}
\end{align*}
And therefore we have equation~\ref{eqnMatrixExp} in a more convenient form:
\begin{equation}
    e^A = P \diag{(e^{\lambda_1}, \cdots, e^{\lambda_n})} P^{-1}
    \label{eqnMatrixExpSimpler}
\end{equation}
\section{Hermitian Matrices}
\subsection{Eigenvectors and Eigenvalues}
\begin{theorem}
    Eigenvalues of a hermitian matrix are real.
    \label{thmHermitianEVals}
\end{theorem}
\begin{proof}
    Let $H$ be a hermitian matrix. Let $\lambda$ be an eigenvalue and $\vec{v}$ be a non-zero eigenvector.
    \begin{equation*}
        H \vec{v} = \lambda \vec{v}
    \end{equation*}
    Pre-multiply by the row vector $\vec{v}^\dagger$:
    \begin{equation}
        \vec{v}^\dagger H \vec{v} = \lambda \vec{v}^\dagger \vec{v}
        \label{eqnHermVecAndTransp}
    \end{equation}
    Taking the hermitian conjugate of equation~\ref{eqnHermVecAndTransp}:
    \begin{align}
       & \left(\vec{v}^\dagger H \vec{v}\right)^\dagger = \left(\lambda \vec{v}^\dagger \vec{v}\right)^\dagger \nonumber \\
       \implies & \vec{v}^\dagger H^\dagger \left(\vec{v}^\dagger\right)^\dagger = \lambda^\dagger \vec{v}^\dagger \left(\vec{v}^\dagger\right)^\dagger \nonumber \\
       \implies & \vec{v}^\dagger H^\dagger \vec{v} = \bar{\lambda} \vec{v} \vec{v}^\dagger \label{eqnHermVecAndTranspConj}
    \end{align}
    Then subtracting equation~\ref{eqnHermVecAndTranspConj} from equation~\ref{eqnHermVecAndTransp}:
    \begin{equation*}
        0 = \left(\lambda - \bar{\lambda}\right)\vec{v}^\dagger \vec{v}
    \end{equation*}
    But $\vec{v}$ is non-zero, so we must have $\lambda - \bar{\lambda} = 0$.\par
    That is, $\lambda$ is real.
\end{proof}
\begin{theorem}
    The eigenvectors of a hermitian matrix corresponding to distinct eigenvalues are orthogonal.
    \label{thmHermitianEVecs}
\end{theorem}
\begin{proof}
    Consider two distinct eigenvalues $\lambda_1$ and $\lambda_2$, and corresponding eigenvectors $\vec{v_1}$ and $\vec{v_2}$.\par
    Then $H \vec{v_1} = \lambda_1 \vec{v_1}$, and $H \vec{v_2} = \lambda_2 \vec{v_2}$\par
    Pre-multiplying by the hermitian conjugates of these vectors:
    \begin{align}
        \text{By } \vec{v_2}^\dagger:&~~\vec{v_2}^\dagger H \vec{v_1} = \lambda_1 \vec{v_2}^\dagger \vec{v_1} \label{eqnPremult2} \\
        \text{By } \vec{v_1}^\dagger:&~~\vec{v_1}^\dagger H \vec{v_2} = \lambda_2 \vec{v_1}^\dagger \vec{v_2} \label{eqnPremult1}
    \end{align}
    Taking the hermitian conjugate of equation~\ref{eqnPremult1}:
    \begin{align}
        \vec{v_2}^\dagger H \vec{v_1} &= \lambda_2^\dagger \vec{v_2}^\dagger \vec{v_1} \nonumber \\
        &= \lambda_2 \vec{v_2}^\dagger \vec{v_1} \text{ by theorem~\ref{thmHermitianEVals}} \label{eqnPremult1Conj} 
    \end{align}
    Then subtracting equation~\ref{eqnPremult1Conj} from \ref{eqnPremult2}:
    \begin{equation*}
        (\lambda_1 - \lambda_2)\vec{v_2}^\dagger \vec{v_1} = 0
    \end{equation*}
    But, $\lambda_1 - \lambda_2 \neq 0$ by assumption, so we must have $\vec{v_2}^\dagger \vec{v_1} = 0$.\par
    That is, these vectors are orthogonal.
\end{proof}
\begin{corollary}
    If a hermitian matrix has $n$ distinct eigenvectors, then the eigenvectors form an orthonormal basis.
    \label{corHermitianEVecBasis}
\end{corollary}
\begin{proof}
    Theorem~\ref{thmHermitianEVecs} gives us that, if all eigenvalues are distinct, all eigenvectors are orthogonal. If all such eigenvectors are normalised, this is an orthonormal basis.
\end{proof}
\subsection{Gram-Schmidt Orthogonalisation}
Consider a set of vectors $B = \{\vec{w_1}, \vec{w_2}, \cdots, \vec{w_r}\}$ of linearly independent vectors. We want to form an orthogonal set $\tilde{B} = \{\vec{v_1}, \cdots, \vec{v_r}\}$.\par
\begin{definition}{Projection of a vector}
    The component of a vector $w$ in the direction of another vector $v$ is called the \underline{projection of $w$ onto $v$}. It is calculated:
    \begin{equation}
        P_{\vec{v}}(\vec{w}) = \frac{\vec{v} \cdot \vec{w}}{\vec{v} \cdot \vec{v}} \vec{v}
        \label{eqnVecProjection}
    \end{equation}
\end{definition}
We can therefore obtain a method for constructing $\tilde{B}$:
\begin{enumerate}
    \item Let $\vec{v_1} = \vec{w_1}$
    \item Let $\vec{v_2} = \vec{w_2} - P_{\vec{v_1}}(\vec{w_2})$, to remove the component of $v_1$ from $w_2$
    \item Continue this process:
        \begin{equation*}
            \vec{v_i} = \vec{w_i} - \sum_{j=1}^{i-1} P_{\vec{v_j}}(\vec{w_i})
        \end{equation*}
        until $i = r$.
\end{enumerate}
\begin{proposition}
    Let $U$ be a transformation matrix from an orthonormal basis to a new orthonormal basis $\{\vec{u_1}, \vec{u_2}, \cdots, \vec{u_n}\}$. Then $U$ is a unitary matrix.
    \label{propUnitaryTransform}
\end{proposition}
\begin{proof}
    Note that $U$ has the vector $u_i$ in its $i$th column.\par
    Now consider $U^\dagger U$:
    \begin{align*}
        [U^\dagger U]_{ij} &= (U^\dagger)_{ik} U_{kj} \\
        &= (\bar{\vec{u_i}})_k (\vec{u_j})_k \\
        &= [\bar{\vec{u_i}} \vec{u_j}]_k \\
        &= \delta_{ij} \text{ by noticing complex inner product of orthogonal vectors}
    \end{align*}
    So $U^\dagger U = I$, and therefore $U$ is unitary.
\end{proof}
We can also now get a very important theorem that most of this part of the course has been leading up to:
\begin{theorem}
    An $n \times n$ hermitian matrix has $n$ orthogonal eigenvectors and is therefore diagonalisable.
    \label{thmHermitianEVecsN}
\end{theorem}
\begin{proof}
    Let $\lambda_i, 1 \leq i \leq r$ be the distint eigenvalues of an $n \times n$ hermitian matrix $H$. Note that $r \leq n$, due to duplication.\par
    Let the set $B = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_r}\}$ be the set of orthogonal (by theorem~\ref{thmHermitianEVecs}) eigenvectors corresponding to $\lambda_i$.\par
    Let $B' = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_r}, \vec{w_1}, \vec{w_2}, \cdots, \vec{w_{n-r}}\}$ be a basis of $\C^n$.\par
    Using Gram-Schmidt, make this basis orthonormal:
    \begin{equation*}
        \tilde{B} = \{\vec{v_1}, \vec{v_2}, \cdots, \vec{v_r}, \vec{u_1}, \vec{u_2}, \cdots, \vec{u_{n-r}}\}
    \end{equation*}
    Now consider the matrix $P$ that transforms the standard basis to $\tilde{B}$. By proposition~\ref{propUnitaryTransform}, this is unitary: $P^{-1} = P^\dagger$.\par
    Now consider the matrix $H$ under the new basis:
    \begin{equation*}
        P^{-1} H P =
        \begin{pmatrix}
            \lambda_1 & 0 & \cdots & 0 & & & & \\
            0 & \lambda_2 & & \vdots & & & & \\
            \vdots & & \ddots & 0 & & \text{\huge{0}} & & \\
            0 & \cdots & 0 & \lambda_r & & & & \\
            & & & & C_{11} & C_{12} & \cdots & C_{1(n-r)} \\
            & & \text{\huge{0}} & & C_{21} & C_{22} & \cdots & C_{2(n-r)} \\
            & & & & \vdots & & \ddots & \vdots \\
            & & & & C_{(n-r)1} & C_{(n-r)2} & \cdots & C_{(n-r)(n-r)} 
        \end{pmatrix}
    \end{equation*}
\end{proof}
\end{document}