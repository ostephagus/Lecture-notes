\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions and Examples}
\begin{definition}{Bilinear form}
    A \underline{bilinear form} is a function $\varphi : U \times V \mapsto \F$ that is linear in both arguments. That is, for each fixed $u_0 \in U$ and $v_0 \in V$, the functions:
    \begin{align*}
        \varphi_{u_0} : V &\mapsto \F & \varphi^{v_0} : U &\mapsto \F \\
        v &\mapsto \varphi(u_0, v) & u &\mapsto \varphi(u, v_0)
    \end{align*}
    are linear.
\end{definition}
\begin{examples}[Examples of Bilinear functions]{
        
    }
    \item $U = V = \F^n$, $\varphi(\vec{x}, \vec{y}) = \sum_{i=1}^{n} x_i y_i$. In the case $\F = \R$, this is the inner product.
    \item $A \in M_{m \times n}(\F)$, the function:
        \begin{align*}
            \varphi : \F^m \times \F^n &\mapsto \F\\
            (\vec{x}, \vec{y}) &\mapsto \vec{x}^T A \vec{y}
        \end{align*}
    \item For $U = V = C[0, 1]$, the integral inner product given by:
        \begin{equation*}
            \varphi(f, g) = \int_{0}^{1} f(x)g(x) dx
        \end{equation*}
    \item The function:
        \begin{align*}
            \varphi : V \times V^* &\mapsto \F\\
            (v, \theta) &\mapsto \theta(v)
        \end{align*}
\end{examples}
\begin{definition}{Matrix of Bilinear Form}
    For $U, V$ finite-dimensional vector spaces over a field $\F$, with bases $B = \{b_1, \cdots, b_m\}$ and $C = \{c_1, \cdots, c_n\}$, and a bilinear form $\varphi$, the \underline{matrix of $\varphi$} with respect to $B$ and $C$ is:
    \begin{equation*}
        [\varphi]_{B, C} = \left(\varphi(b_i, c_j)\right)_{i, j} \in M_{m \times n}(\F)
    \end{equation*}
\end{definition}
\begin{proposition}
    The matrix $[\varphi]_{B, C}$ as defined in the above definition satisfies:
    \begin{equation}
        \left([u]_B\right)^T [\varphi]_{B, C} [v]_C = \varphi(u, v)
        \label{eqnBFMatProduct}
    \end{equation}
    for all $u \in U, v \in V$. Further, $[\varphi]_{B, C}$ is the only matrix satisfying \eqnref{eqnBFMatProduct}.
    \label{propBFMatProducts}
\end{proposition}
\begin{proof}
    Fix any $u \in U, v \in V$. Find their linear combinations:
    \begin{equation*}
        u = \sum_{i=1}^{m} \lambda_i b_i,\qquad v = \sum_{j=1}^{n} \mu_i c_i
    \end{equation*}
    So that $[u]_B = \vec{\lambda}$, and $[v]_C = \vec{\mu}$.

    Then we can find $\varphi(u, v)$:
    \begin{align*}
        \varphi(u, v) &= \sum_{i=1}^{m}\sum_{j=1}^{n} \varphi(\lambda_i b_i, \mu_i c_i) \\
        &= \sum_{i=1}^{m}\sum_{j=1}^{n} \lambda_i \mu_i\varphi(b_i, c_i) \\
        &= \vec{\lambda}^T \left(\varphi(b_i, c_i)\right)_{ij} \vec{\mu}
    \end{align*}
    as required. If now $A \in M_{m \times n}(\F)$ satisfies \eqnref{eqnBFMatProduct}, for all $u \in U, v \in V$:
    \begin{align*}
        \varphi(b_i, c_j)&= \left([b_i]_B\right)^T A [c_j]_C \\
        &= \vec{e_i}^T A \vec{e_j} \\
        &= a_{ij}
    \end{align*}
    Therefore $A$ and $[\varphi]_{B, C}$ are equal on every component, so they are equal.
\end{proof}
\begin{corollary}
    If $B, B'$ are bases for $U$ and $C, C'$ are bases for $V$, then the change of basis formula for the matrix of $\varphi$ is:
    \begin{equation*}
        [\varphi]_{B', C'} = \left([\Id_U]_B^{B'}\right)^T [\varphi]_{B, C} [\Id_V]_C^{C'}
    \end{equation*}
    \label{corBFChangeBasis}
\end{corollary}
\begin{proof}
    For each $u \in U, v \in V$,
    \begin{align*}
        \left([u]_{B'}\right)^T&\left([\Id_U]_B^{B'}\right)^T [\varphi]_{B, C} [\Id_V]_C^{C'}[v]_{C'} \\
        &= \left([\Id_U]_B^{B'} [v]_B'\right)^T [\varphi]_{B, C} [v]_C \\
        &= \left([v]_B\right)^T [\varphi]_{B, C} [v]_C \\
        &= \phi(u, v)
    \end{align*}
    Then by \thmref{propBFMatProducts}, the matrix representing $\varphi$ in any basis is unique, so we have the required formula.
\end{proof}
\begin{definition}{Rank of a bilinear form}
    For a bilinear form $\varphi : U \times V \mapsto \F$ with $U, V$ finite-dimensional, the \underline{rank} of $\varphi$ is the rank of the matrix $[\varphi]_{B, C}$ with respect to any bases $B, C$.
\end{definition}
The matrices representing $\varphi$ in different bases are equivalent, and since equivalent matrices have the same rank we have that the rank of $\varphi$ is well-defined (same in all bases).

For $U = V$, and $B, B'$ bases for $U$, we have that there exists $P \in GL_{\dim(V)}(\F)$ such that:
\begin{equation*}
    [\varphi]_{B', B'} = P^T [\varphi]_{B, B} P
\end{equation*}
We can more concretely define this relation:
\begin{definition}{Congruent matrices}
    For matrices $A, A' \in M_{n \times n}(\F)$, these are \underline{congruent} if there exists an invertible matrix $P$ such that $A' = P^T A P$.
\end{definition}
\begin{proposition}
    Congruence of matrices is an equivalence relation over $M_{n \times n}(\F)$.
    \label{propCongruenceEquiv}
\end{proposition}
\begin{proof}
    Clearly for any matrix $A$ we have $A$ is congruent to itself by setting $P = I$.

    Further, if $A$ is congruent to $B$ then $B$ is congruent to $A$ because $B = (P^{-1})^T A P^{-1}$.

    Now suppose $A = P^T B P, B = Q^T C Q$. Then we also know that $A$ is congruent to $C$ because:
    \begin{equation*}
        A = (QP)^T C (QP)
    \end{equation*}
    and the product of invertible matrices is still invertible.
\end{proof}
\begin{definition}{Left and right linear map}
    For a bilinear form $\varphi : U \times V \mapsto \F$, define the \underline{left linear map} of $\varphi$ to be:
    \begin{align*}
        \varphi_L : U &\mapsto V^*\\
        u &\mapsto \varphi_L(u) \text{ such that } (\varphi_L(u))(v) = \varphi(u, v).
    \end{align*}
    Similarly define the \underline{right linear map} of $\varphi$ to be:
    \begin{align*}
        \varphi_R : V &\mapsto U^*\\
        v &\mapsto \varphi_R(v) \text{ such that } (\varphi_R(v))(u) = \varphi(u, v)
    \end{align*}
\end{definition}
\begin{lemma}
    $\varphi_L : U \mapsto V^*$ and $\varphi_R : V \mapsto U^*$ are linear.
    \label{lemLRBFLinear}
\end{lemma}
\begin{proof}
    This follows immediately from the linearity of $\varphi$ in each argument.
\end{proof}
\begin{definition}{Left and right kernels}
    Let $\varphi : U \times V \mapsto \F$ be a bilinear form. The set $\ker{\varphi_L}$ is the \underline{left kernel} of $\varphi$, which is a subspace of $U$. 
    The set $\ker{\varphi_R}$ is the \underline{right kernel} of $\varphi$, which is a subspace of $V$.
\end{definition}
\begin{proposition}
    Let $U$ and $V$ be finite-dimensional vector spaces over $\F$. Let $B$ and $C$ be bases for the respective spaces, with dual bases $B^*, C^*$. Then:
    \begin{align*}
        [\varphi_L]_{C^*}^B &= \left([\varphi]_{B, C}\right)^T \\
        [\varphi_R]_{B^*}^C &= [\varphi]_{B, C}
    \end{align*}
    \label{propLRMapsMatrices}
\end{proposition}
\begin{proof}
    Set $B = \{b_1, \cdots, b_m\}$ and $C = \{c_1, \cdots, c_n\}$. Let $A = [\varphi]_{B, C}$ and consider:
    \begin{align*}
        [\varphi(b_i)](c_i)&= \varphi(b_i, c_i) = A_{ij} \\
        &= \left(\sum_{l=1}^{n} A_{il} c_l^*\right) c_j \\
    \end{align*}
    and so:
    \begin{equation*}
        \varphi_L(b_i) = \sum_{l=1}^n A_{li}^T c_l^*
    \end{equation*}
    and so $[\varphi_L]_{C^*}{B} = A^T$.

    The proof is similar for the right linear map.
\end{proof}
\begin{corollary}
    $rk(\varphi_L) = rk(\varphi) = rk(\varphi_R)$.
    \label{corRankLRMap}
\end{corollary}
\begin{definition}{Perpendiculars}
    For vector spaces $U$ and $V$ with subsets $S \subseteq U$, $T \subseteq V$, the \underline{perpendicular} of $S$ and $V$ are given by:
    \begin{align*}
        S^\perp &= \subsetselect{v \in V}{\forall s\in S, \varphi(s, v) = 0} \\
        \lperp{T} &= \subsetselect{u \in U}{\forall t \in T, \varphi(t, u) = 0}
    \end{align*}
\end{definition}
\begin{remarks}
    \item $S^\perp \leq V$ and $\lperp{T} \leq U$ by the subspace test.
    \item If $S_1 \subseteq S_2 \subseteq U$, then $S_2^\perp \leq S_1^\perp \leq V$. Similar for subsets of $V$.
    \item $U^\perp = \ker(\varphi_R)$, $\lperp{V} = \ker(\varphi_L)$.
\end{remarks}
\begin{definition}{Degeneracy}
    A bilinear form $\varphi$ is \underline{degenerate} if one of the perpendiculars $U^\perp$ or $\lperp{V}$ are nontrivial.
\end{definition}
\begin{proposition}
    For $U$, $V$ finite-dimensional, $B$, $C$ respective bases, $\varphi$ is non-degenerate if and only if:
    \begin{itemize}
        \item $U$ and $V$ have the same dimension, and
        \item $[\varphi]_{B, C}$ is invertible.
    \end{itemize}
    \label{propDegeneracyConditions}
\end{proposition}
\begin{proof}
    We provide the following chain of equivalences:
    \begin{align*}
        \text{Non-}&\text{degeneracy} \iff \ker{\varphi_L}, \ker{\varphi_R} \text{ are trivial} \\
        &\iff \dim(U) = rk(\varphi_L) = rk(\varphi) = rk(\varphi_R) = \dim(V) \\
        &\iff \dim(U) = \dim(V) \text{ and } [\varphi]_{B, C} \text{ invertible.}
    \end{align*}
\end{proof}
\section{Symmetric Bilinear Forms}
We now consider bilinear forms where both arguments are in the same vector space.
\begin{definition}{Symmetric bilinear form}
    For a vector space $V$ over $\F$, a bilinear form $\varphi : V \times V \mapsto \F$ is \underline{symmetric} if for all $v_1, v_2 \in V$,
    \begin{equation*}
        \varphi(v_1, v_2) = \varphi(v_2, v_1)
    \end{equation*}
\end{definition}
\begin{definition}{Symmetric matrix}
    A matrix $A \in M_{n \times n}(\F)$ is \underline{symmetric} if $A = A^T$.
\end{definition}
\begin{proposition}
    In a finite-dimensional vector space $V$, a bilinear form $\varphi$ is symmetric if and only if in a basis $B$, $[\varphi]_{B, B}$ is a symmetric matrix.
    \label{propSymMatrixIffBF}
\end{proposition}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Suppose that $\varphi$ is symmetric}
        Then, in any basis $B$ of $V$, and for vectors $u, v \in V$,
        \begin{align*}
            \varphi(u, v) &= \varphi(v, u) \\
            \left([u]_B\right)^T [\varphi]_B^B [v]_B &= \left([v]_B\right)^T [\varphi]_{B, B} [u]_B \\
            &= \left(\left([u]_B\right)^T \left([\varphi]_{B, B}\right)^T [v]_B\right)^T \\
            &= \left([u]_B\right)^T \left([\varphi]_{B, B}\right)^T [v]_B \\
        \end{align*}
        because scalars are invariant under transpose.

        Since this is the case for all $u$ and $v$, $[\varphi]_{B, B}$ must be equal to its transpose.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $\varphi$ can be represented by a symmetric matrix}
        Consider the basis $B$ in which the matrix of $\varphi$ is symmetric. Then for any vectors $u, v \in V$,
        \begin{align*}
            \varphi(u, v) &=\left([u]_B\right)^T [\varphi]_B^B [v]_B  \\
            &= \left(\left([u]_B\right)^T [\varphi]_B^B [v]_B\right)^T \text{ by same logic as above} \\
            &= \left([v]_B\right)^T \left([\varphi]_B^B\right)^T [u]_B \\
            &= \left([v]_B\right)^T [\varphi]_B^B [u]_B \text{ by symmetric matrix} \\
            &= \varphi(v, u)
        \end{align*}
    \end{proofdirection}
\end{proof}
\begin{proposition}
    Let $V$ be a vector space over $\F$, and let $\varphi$ be a symmetric bilinear form.
    For any subset $S \subseteq V$, $\lperp{S} = S^\perp$.

    In particular, the left and right kernels are equal.
    \label{propBFSymPerpendicular}
\end{proposition}
The \underline{kernel} of a symmetric bilinear form is the left kernel or the right kernel (since they are equal).
\begin{definition}{Quadratic form}
    A \underline{quadratic form} on a vector space $V$ is a function $Q : V \mapsto \F$ such that there exists a bilinear form $\varphi : V \times V \mapsto \F$ with the property that $\varphi(v, v) = Q(v)$ for all $v \in V$.
\end{definition}
\begin{example}
    Consider $V = \R^2$. For $A \in M_{2 \times 2}(\R)$, define the quadratic form:
    \begin{equation*}
        Q_A(\vec{x}) = \vec{x}^T A \vec{x}
    \end{equation*}
    Then we consider a corresponding bilinear form:
    \begin{equation*}
        \varphi(\vec{x}, \vec{y}) = \vec{x}^T A \vec{y}
    \end{equation*}
    Then note that $Q_A = Q_{\frac12 (A + A^T)}$.
\end{example}
\begin{proposition}[Polarisation Identity]
    Suppose that, in $\F$, $1 + 1 \neq 0$. This excludes, for example, $\F = \Z / 2\Z$.

    Let $Q : V \mapsto \F$ be a quadratic form. Then there exists a unique symmetric bilinear form:
    \begin{equation}
        \begin{split}
            \psi : V \times V &\mapsto \F \\
            \text{such that } Q(v) &= \psi(v, v)~\forall v \in V.
        \end{split}
        \label{eqnBFSymDef}
    \end{equation}

    In fact, $\psi$ is given by:
    \begin{equation}
        \psi(u, v) = \frac12 \left(Q(u + v) - Q(u) - Q(v)\right)
        \label{eqnPolarisationId}
    \end{equation}
    \label{propPolarisationId}
\end{proposition}
\begin{proof}
    \begin{subproof}{$\psi$ exists.}
        Consider the bilinear form $\varphi$ from the definition of a quadratic form, so $\varphi(v, v) = Q(v)$ for all $v \in V$. Then define $\psi$ by symmetrising:
        \begin{equation*}
            \psi(u, v) = \frac12 (\varphi(u, v) + \varphi(v, u))
        \end{equation*}
    \end{subproof}
    \begin{subproof}{$\psi$ is unique.}
        Suppose that $\psi$ is a symmetric bilinear form satisfying \eqnref{eqnBFSymDef}. Then:
        \begin{align*}
            Q(u + v) &= \psi(u + v, u + v) \\
            &= \psi(u, u) + \psi(u, v) + \psi(v, u) + \psi(v, v) \\
            &= Q(u) + 2\psi(u, v) + Q(v) \text{ by symmetry} \\
        \end{align*}
        Then since we can divide by $2$, we can rearrange to get \eqnref{eqnPolarisationId}. That is, all symmetric bilinear forms satisfying \eqnref{eqnBFSymDef} must in fact be defined by \eqnref{eqnPolarisationId}.
    \end{subproof}
\end{proof}
\begin{theorem}
    Suppose again that, in $\F$, $1 + 1 \neq 0$.

    Let $V$ be a finite-dimensional vector space with $\varphi$ a symmetric bilinear form. Then there exists a basis $B$ for $V$ that $[\varphi]_{B, B}$ is a diagonal matrix.
    \label{thmBFSymImpliesDiag}
\end{theorem}
\begin{remark}
    The assumption that $\varphi$ is symmetric is necessary here. If instead there existed a basis such that the diagonal matrix $D$ represented a non-symmetric bilinear form, then because every diagonal matrix is symmetric we would get that $\varphi$ was symmetric by \thmref{propSymMatrixIffBF}.
\end{remark}
\begin{proof}
    Let $n = \dim(V)$.
    \induction{$n = 1$}{
        A 1-dimensional matrix is always diagonal.
    }{$n = k$}{
        Assume that, in a $k$-dimensional vector space, for any symmetric bilinear form there exists a basis in which its matrix is diagonal.
    }{$n = k + 1$}{
        If, for all $v \in V$, $Q(v) = 0$, then $[\varphi]_B^B = 0$ in any basis, which is diagonal.
        If not, there exists a non-zero vector $v_1 \in V$ such that $\varphi(v_1, v_1) \neq 0$.

        Define $U = \spn{v_1}^\perp = \ker(\varphi_L(v_1))$. Note that $v_1 \notin U$.

        By \thmref{thmRankNullity}, the dimension of $U$ is $k$, and $V = \spn{v_1} \oplus U$. Apply the inductive hypothesis, since $\varphi |_U : U \times U \mapsto \F$ is a symmetric bilinear form. Then we have a basis $B'$ for $U$ in which $[\varphi|_U]_{B, B}$ is diagonal.

        Set $B = \{v_1\} \cup B'$, so $v_1$ is now the first vector in the basis. Then, since all elements of $U$ are in the perpendicular to $v_1$,
        \begin{equation*}
            [\varphi]_{B, B} =
            \begin{pmatrix}
                \varphi(v_1, v_1) &  & 0 \\
                 & \ddots &  \\
                0 &  & \varphi(v_n, v_n)
            \end{pmatrix}
        \end{equation*}
    }
\end{proof}
\begin{corollary}
    For fields with $1 + 1 \neq 0$, every symmetric matrix is congruent to a diagonal matrix.
    \label{corSymImpliesDiag}
\end{corollary}
\begin{proof}
    A symmetric matrix represents a symmetric bilinear form, which is represented by a diagonal matrix in a different basis. Since the change-of-basis formula for bilinear forms is a congruence relation, we have that any symmetric matrix is congruent to a diagonal one.
\end{proof}
\begin{example}
    In practice, we generally do not have to use this inductive algorithm to find a basis in which $[\varphi]$ is diagonal.

    Consider the Quadratic form:
    \begin{align*}
        Q(\vec{x}) &= x_1^2 - 4x_2 x_3 + x_3^2 \\
        &= x_1^2 + (x_3 - 2x_2)^2 - (2x_2)^2 \\
        &= z_1^2 + z_2^2 - z_3^2
    \end{align*}
    where we define $\vec{z}$:
    \begin{equation*}
        \vec{z} =
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & -2 & 1 \\
            0 & 2 & 0
        \end{pmatrix}
        \vec{x}
    \end{equation*}
    Let this matrix be $P^{-1}$, $\vec{z} = P^{-1} \vec{x}$.

    Then let $Q(\vec{x}) = \varphi(\vec{x}, \vec{x})$, where:
    \begin{equation*}
        \varphi(\vec{x}, \vec{y}) = x_1 y_1 - 2(x_2 y_3 + x_3 y_2) + x_3 y_3
    \end{equation*}
    With respect to the standard basis, $\varphi$ has the matrix:
    \begin{equation*}
        [\varphi]_{E, E} =
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 0 & -2 \\
            0 & -2 & 1
        \end{pmatrix}
    \end{equation*}
    and $P^T A P$ is:
    \begin{equation*}
        [\varphi]_{B, B} =
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & -1
        \end{pmatrix}
    \end{equation*}
\end{example}
\begin{corollary}
    Suppose that $V$ is a finite-dimensional vector space over $\F$, and $\varphi$ is a symmetric bilinear form. Then for the following fields, there exists a basis $B = \{v_1, \cdots, v_n\}$ such that:
    \begin{enumerate}
        \item In the case $\F = \C$,
            \begin{equation*}
                [\varphi]_{B, B} = \begin{pmatrix}
                    I_r & 0 \\
                    0 & 0
                \end{pmatrix}
                \text{ where } r = rk(\varphi)
            \end{equation*}
        \item In the case $\F = \R$,
            \begin{equation*}
                [\varphi]_{B,B} =
                \begin{pmatrix}
                    I_p & 0 & 0 \\
                    0 & -I_q & 0 \\
                    0 & 0 & 0
                \end{pmatrix}
                \text{ where } p + q = rk(\varphi)
            \end{equation*}
    \end{enumerate}
    \label{corBFNiceDiag}
\end{corollary}
\begin{proof}
    Choose a basis $C = \{c_1, \cdots, c_n\}$ such that $[\varphi]_C^C$ is diagonal, by \thmref{thmBFSymImpliesDiag}. After possibly re-labelling, assume that the diagonal entries of $[\varphi]_{C, C}$ are:
    \begin{equation*}
        \begin{cases}
            a_i & i \leq r \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    If $\F = \R$, also re-label such that the $p$ positive diagonal entries come before the $q = r - p$ negative entries.

    \begin{case}{$\F = \C$}
        Define:
        \begin{equation*}
            v_i =
            \begin{cases}
                \frac{1}{\sqrt{a_i}}c_i & i \leq r \\
                c_i & \text{otherwise}
            \end{cases}
        \end{equation*}
        Then in this new basis all diagonal elements are 1.
    \end{case}
    \begin{case}{$\F = \R$}
        Define:
        \begin{equation*}
            v_i =
            \begin{cases}
                \frac{1}{\sqrt{a_i}}c_i & i \leq p \\
                \frac{1}{\sqrt{|a_i|}}c_i & p < i \leq r \\
                c_i & \text{otherwise}
            \end{cases}
        \end{equation*}
        Then in this new basis all diagonal elements are $1$ or $-1$, in the required form.
    \end{case}
\end{proof}
\subsection{Definiteness}
\begin{definition}{Definiteness}
    Let $V$ be a real vector space, and $\varphi : V \times V \mapsto \R$.
    \begin{enumerate}
        \item $\varphi$ is \underline{positive-definite} if $\varphi(v, v) > 0$ for all non-zero $v \in V$.
        \item $\varphi$ is \underline{positive semi-definite} if $\varphi(v, v) \geq 0$ for all non-zero $v \in V$.
        \item $\varphi$ is \underline{negative definite} if $\varphi(v, v) < 0$ for all non-zero $v \in V$.
        \item $\varphi$ is \underline{negative semi-definite} if $\varphi(v, v) \leq 0$ for all non-zero $v \in V$.
        \item $\varphi$ is \underline{indefinite} otherwise.
    \end{enumerate}
\end{definition}
We have similar definitions for quadratic forms.

In the case $V$ is finite-dimensional, then we can use the representation given by \thmref{corBFNiceDiag} to characterise these definitions:
\begin{itemize}
    \item If $q = 0$, then the bilinear form is positive semi-definite.
    \item If further $p = r$, then the bilinear form is positive definite.
    \item The other characterisations are similar.
\end{itemize}
\begin{lemma}
    Let $U, W$ be subspaces of a vector space $V$.

    If $\varphi |_U$ is positive definite and $\varphi |_W$ is negative semi-definite, then $U \cap W = \{\zv\}$.

    Similarly, if $\varphi |_U$ is negative definite and $\varphi |_W$ is positive semi-definite, then $U \cap W = \{\zv\}$.
    \label{lemBFRestrictDefness}
\end{lemma}
\begin{proof}
    For all $v \in U \cap W$, we must have that, if $v \neq \zv$, $\varphi(v, v) > 0$ and $\varphi(v, v) \leq 0$. \contradiction

    The proof is similar for the second claim.
\end{proof}
\begin{definition}{Signature}
    The \underline{signature} of a real symmetric bilinear form, $\sigma(\varphi)$, is $p - q$ in the representation given by \thmref{corBFNiceDiag}.
\end{definition}
We can show that this is well-defined:
\begin{theorem}[Sylvester's Law of Inertia]
    For $V$ a finite-dimensional vector space and $\varphi$ a symmetric bilinear form, if $B$ and $B'$ are bases for $V$ such that:
    \begin{equation*}
        [\varphi]_{B, B} =
        \begin{pmatrix}
            I_p & 0 & 0 \\
            0 & -I_q & 0 \\
            0 & 0 & 0
        \end{pmatrix}
        , \qquad [\varphi]_{B', B'} =
        \begin{pmatrix}
            I_{p'} & 0 & 0 \\
            0 & -I_{q'} & 0 \\
            0 & 0 & 0
        \end{pmatrix}
    \end{equation*}
    then $p = p', q = q'$. That is, the above representation is unique.
    \label{thmSylvesterInertia}
\end{theorem}
\begin{proof}
    \begin{subproof}{$p$ and $q$ are independent of the basis chosen.}
        We will show that $p$ is the maximal dimension of a subspace $U$ such that $\varphi |_U$ is positive-definite, and same for $q$ with negative definite.

        Let $B = \{v_1, \cdots, v_n\}$, and we know that in this basis:
        \begin{equation*}
            [\varphi]_{B, B} =
            \begin{pmatrix}
                I_p & 0 & 0 \\
                0 & -I_q & 0 \\
                0 & 0 & 0
            \end{pmatrix}
        \end{equation*}
        Now define $U = \spn{v_1, \cdots, v_p}$ and $W = \spn{v_{p+1}, \cdots, v_q}$. Then we know that $\varphi |_U$ is positive-definite, and $\varphi |_W$ is negative definite. To show that these are the largest such, suppose that $U'$ is another subspace such that $\varphi |_{U'}$ is positive definite, and with dimension $p' > p$.

        We must have that $U, U', W$ are all subsets of the space $U + W$ where $\varphi$ is not trivially zero. By the sum intersection formula,
        \begin{align*}
            \dim(U' + W)&= \dim(U') + \dim(W) - \dim(U' \cap W) \\
            &= p' + q - \dim(U' \cap W) \leq p + q
        \end{align*}
        This implies that $\dim(U' \cap W) \geq 1$, which contradicts \thmref{lemBFRestrictDefness}.

        The case for $q$ is similar.
    \end{subproof}
    Then we have that $p$ and $q$ are invariant under change of basis.
\end{proof}
\begin{definition}{Totally isotropic subspace}
    A subspace $T \leq V$ is \underline{totally isotropic} for a bilinear form $\varphi : V \times V \mapsto \F$ if, for all $u, v \in T$, $\varphi(u, v) = 0$.
\end{definition}
\begin{remark}
    A totally isotropic subspace $T$ must intersect trivially with any subspace over which $\varphi$ is positive definite or negative definite.
\end{remark}
\begin{proposition}
    Let $V$ be an $n$-dimensional vector space over $\F$. The maximal dimension of a totally isotropic subspace for a bilinear form $\varphi$ is $n - \max\{p, q\}$.
    \label{propTISMaxDim}
\end{proposition}
\begin{proof}
    Let $B = \{v_1, \cdots, v_n\}$ be a basis in which:
    \begin{equation*}
        [\varphi]_{B, B} =
        \begin{pmatrix}
            I_p & 0 & 0 \\
            0 & -I_q & 0 \\
            0 & 0 & 0
        \end{pmatrix}
    \end{equation*}
    Let $p \geq q$. The argument for the reverse is exactly symmetric, so we can do this without loss of generality.

    Then let $T = \spn{v_1 + v_{p + 1}, v_2 + v_{p + 2}, \cdots, v_q + v_{p + q}, v_{p + 1 + 1}, \cdots, v_n}$.
    Then this is a totally isotropic subspace of dimension $n - p$. Suppose $T'$ is another totally isotropic such that $\dim(T') > n - p$. Now set $U = \spn{v_1, \cdots, v_p}$. Now $\varphi |_U$ is positive definite, but by the sum-intersection formula,
    \begin{align*}
        \dim(U + T') &= \dim(U) + \dim(T') - \dim(U \cap T') \leq n \\
        &= p + \dim(T') - \dim(U \cap T') \\
        \implies &\dim(T') - \dim(U \cap T') \leq n - p
    \end{align*}
    But since $\dim(T') > n - p$, we have that $\dim(U \cap T') \geq 1$, which is a contradiction of the remark above.
\end{proof}
\section{Sesquilinear and Hermitian Forms}
For this section, take $\F = \C$.
\begin{example}
    In the previous chapter, we defined the real dot product,
    \begin{equation*}
        \sum_{i = 1}^n x_i y_i
    \end{equation*}
    We also know about the complex dot product,
    \begin{equation*}
        \sum_{i=1}^{n} x_i \overline{y_i},
    \end{equation*}
    but we note that this is not linear in the second argument. Instead, we have \textit{conjugate linearity} which we will make rigorous in this section.
\end{example}
\begin{definition}{Sesquilinear form}
    A \underline{sesquilinear form} on complex vector spaces $V$ and $W$ is a map $V \times W \mapsto \C$ such that, for any $v, v_1, v_2 \in V$, $w, w_1, w_2 \in W$, and $\lambda, \mu \in \C$,
    \begin{enumerate}
        \item $\varphi(\lambda v_1 + v_2, w) = \lambda \varphi(v_1, w) + \varphi(v_2, w)$
        \item $\varphi(v, \mu w_1 + w_2) = \overline{\mu} \varphi(v, w_1) + \varphi(v, w_2)$
    \end{enumerate}
\end{definition}
\begin{remark}
    Sesquilinear forms can also be defined for fields that admit an operation akin to complex conjugation, but we will assume we are working in $\C$ when we discuss sesquilinear forms.
\end{remark}
Many of our results carry over from bilinear forms.

\begin{definition}{Matrix of sesquilinear form}
    Suppose $V, W$ are finite-dimensional vector spaces and $\varphi : V \times W \mapsto \C$ is a sesquilinear form. Given bases $B$ and $C$ for $V$ and $W$, the \underline{matrix of $\varphi$} is the matrix with elements:
    \begin{equation*}
        ([\varphi]_{B, C})_{i, j} = \varphi(b_i, c_j)
    \end{equation*}
\end{definition}
\begin{proposition}
    For vector spaces $V, W$ with bases $B, C$, $[\varphi]_{B, C}$ is the unique matrix satisfying:
    \begin{equation}
        \varphi(v, w) = \left([v]_B\right)^T [\varphi]_{B, C} \overline{[w]_C}
        \label{eqnCFMatProduct}
    \end{equation}
    for all $v \in V, w \in W$.
    \label{propCFMatProducts}
\end{proposition}
\begin{proof}
    The proof is similar to that of \thmref{propBFMatProducts}
\end{proof}
\begin{corollary}
    Let $V, W$ be vector spaces with bases $B, C$. If $B', C'$ are different bases for $V, W$ respectively, then the change-of-basis formula for the matrix of $\varphi$ is:
    \begin{equation*}
        [\varphi]_{B', C'} = \left([\Id_V]_B^{B'}\right)^T [\varphi]_{B, C} \overline{[\Id_W]_{C}^{C'}}
    \end{equation*}
    \label{corCFChangeBasis}
\end{corollary}
As we found that symmetric forms were especially nice for the real case, we find that the appropriate symmetry condition is conjugate symmetry.
\begin{definition}{Hermitian form}
    A sesquilinear form $\varphi : V \times V \mapsto \C$ is a \underline{Hermitian form} if:
    \begin{equation*}
        \varphi(u, v) = \overline{\varphi(v, u)}
    \end{equation*}
\end{definition}
\begin{remarks}
    \item For $\varphi$ Hermitian, $\varphi(v, v) \in \R$ for all vectors $v$.
    \item $\varphi(\lambda v, \lambda v) = |\lambda|^2$, so it makes sense to assign the same definitions of definiteness to Hermitian forms.
    \item $\varphi(v, u) = 0$ if and only if $\varphi(u, v) = 0$, so the left and right perpendicular are the same.
\end{remarks}
\begin{definition}{Hermitian matrix}
    $A \in M_{n \times n}(\C)$ is \underline{Hermitian} if:
    $A = A^\dagger = \overline{A^T}$.
\end{definition}
\begin{lemma}
    For a finite-dimensional vector space $V$, a sesquilinear form $\varphi : V \times V \mapsto \C$ is Hermitian if and only if there exists a basis such that the matrix of $\varphi$ is Hermitian.
    \label{lemSFHermitIffMat}
\end{lemma}
\begin{proof}
    Let $B = \{v_1, \cdots, v_n\}$, and let $A = [\varphi]_{B, B}$.
    \begin{proofdirection}{$\Rightarrow$}{Suppose that $\varphi$ is Hermitian}
        \begin{align*}
            a_{ij} &= \varphi(b_i, b_j) \\
            &= \overline{\varphi(b_j, b_i)} \\
            &= \overline{a_{ji}}
        \end{align*}
        \begin{proofdirection}{$\Leftarrow$}{Assume that $A$ is Hermitian}
            For vectors $u, w \in V$,
            \begin{align*}
                \varphi(u, w) &= \left([v]_B\right)^T A \overline{[w]_B} \\
                &= \left(\left([v]_B\right)^T A \overline{[w]_B}\right)^T \text{ because this a scalar} \\
                &= \left([w]_B\right)^\dagger A^T [v]_B \\
                &= \left([w]_B\right)^\dagger \overline{A} [v]_B \\
                &= \overline{\left([w]_B\right)^T A \overline{[v]_B}} \\
                &= \overline{\varphi(w, u)}
            \end{align*}
        \end{proofdirection}
    \end{proofdirection}
\end{proof}
We now provide an equivalent Polarisation Identity:
\begin{proposition}[Sesquilinear Polarisation Identity]
    Let $V$ be a complex vector space and $\varphi$ a Hermitian form over $V$.

    Then $\varphi$ is uniquely determined by:
    \begin{align*}
        Q : V &\mapsto \R\\
        v &\mapsto \varphi(v, v)
    \end{align*}
    by the expression:
    \begin{equation}
        \varphi(u, v) = \frac14 \left(Q(u + v) - Q(u - v) + iQ(u + iv) - iQ(u - iv)\right)
        \label{eqnPolarisationIdC}
    \end{equation}
    \label{propPolarisationIdC}
\end{proposition}
\begin{proof}
    The proof is just by multiplying out.
\end{proof}
\begin{theorem}
    Let $V$ be finite-dimensional and $\varphi$ a Hermitian form. Then there exists a basis $B$ for $V$ such that:
    \begin{equation*}
        [\varphi]_{B, B} =
        \begin{pmatrix}
            I_p & 0 & 0 \\
            0 & I_q & 0 \\
            0 & 0 & 0
        \end{pmatrix}
    \end{equation*}
    and this is unique.
    \label{thmSFNiceMatrix}
\end{theorem}
\begin{proof}
    The proof is similar to the case of a real symmetric bilinear form.

    We first show that this matrix exists:
    \induction{$\dim(V) = 1$}{
        A 1-dimensional matrix is always diagonal.
    }{$\dim(V) = k$}{
        Assume that a bilinear form of a $k$-dimensional can be expressed like above.
    }{$\dim(V) = k + 1$}{
        If $\varphi \equiv 0$, then we are done. Otherwise, there exists a vector $v_1$ such that $\varphi(v_1, v_1) \neq 0$ by \thmref{propPolarisationIdC}. Let $U = \spn{v_1}^\perp$. Like in the proof of \thmref{thmBFSymImpliesDiag}, we find that $V = U \oplus \spn{v_1}$ and apply the inductive hypothesis to $\varphi |_U$ to get that $[\varphi]_{B, B}$ is diagonal.
    }
    Then, once we have a basis in which $[\varphi]_{B, B}$ is diagonal, we can rescale the basis vectors to get the required form.

    As in the proof of \thmref{thmSylvesterInertia}, we can then characterise $p$ and $q$ based on the definiteness of the largest possible subspace, and so they are invariant under change of basis. That is, the representation is unique.
\end{proof}
\end{document}