\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Spanning Sets}
\begin{definition}{Span}
    Let $V$ be a vector space over $\F$. The \underline{span} of a subset $S \subseteq V$ is the set:
    \begin{equation*}
        \spn{S} = \left\{\finitesum_{s \in S} \lambda_s s\right\}
    \end{equation*}
\end{definition}
Here we have introduced notation $\finitesum$ for a sum over finitely many terms. In this case, that means we require all but finitely many $\lambda_s$ non-zero.

We call such a sum of not-infinitely-many scaled elements of $S$ a \underline{linear} \underline{combination}.
\begin{definition}{Spanning set}
    A subset $S$ of a vector space $V$ is a \underline{spanning set} for $V$ if $\spn{S} = V$. We say $S$ \underline{spans} $V$.
\end{definition}
$V$ is \underline{finite-dimensional} if it has a finite spanning set.
\begin{remarks}
    \item For $S \subseteq V$, $\spn{S} \leq V$. Conversely, if $W \leq V$ and $S \subseteq W$ then $\spn{S} \leq W$.
    \item IF $S, T \subseteq V$ and $S$ spans $V$, if $S \subseteq \spn{T}$ then $T$ spans $V$.
    \item By convention, $\spn{\emptyset} = \{\zv\}$.
    \item $\spn{S \cup T} = \spn{S} + \spn{T}$.
\end{remarks}
\begin{example}
    Consider $V = \R^3$. Define:
    \begin{align*}
        S &= \left\{\begin{pmatrix} 1 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}1 \\ 1 \\ 2\end{pmatrix}\right\} \\
        T &= \left\{\begin{pmatrix} 2 \\ 1 \\ 2\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ 2\end{pmatrix}, \begin{pmatrix}-1 \\ 2 \\ 4\end{pmatrix}\right\} \\
    \end{align*}
    Then we see that $\spn{S} = \spn{T}$
    \label{ex3DSpanningSets}
\end{example}
\begin{example}
    Let $V = \R^{\N}$, the set of real sequences. Let $T = \subsetselect{\delta_n}{n \in \N}$ where $delta_n(m) = \delta_{mn}$. Then the span of $T$ is the set of sequences that are eventually all $0$, the set of sequences with finitely many non-zero terms.

    The problem here is that we only allow a linear combination of finitely many elements of $T$, so $T$ does not span $V$.
\end{example}
\section{Linear Independence}
\begin{definition}{Linear independence}
    A subset $S$ of a vector space $V$ is \underline{linearly independent} if, for all linear combinations
    \begin{equation*}
        \sum_{s \in S} \lambda_s s
    \end{equation*}
    of $S$, if the linear combination is zero then all the coefficients $\lambda_s$ must be zero.
\end{definition}
\begin{remarks}
    \item The opposite of linear independence is \underline{linear dependence}
    \item If $\zv \in S$ then $S$ is linearly dependent
    \item If $S = \{v_1, \cdots, v_n\}$ is finite with size $n$, then $S$ is linearly independent if and only if:
        \begin{equation*}
            \sum_{i = 1}^n \lambda_i v_i = \zv \implies \lambda_i = 0~\forall i \in \{1, \cdots, n\}
        \end{equation*}
    \item If $S$ is infinite then $S$ is linearly independent if and only if every finite subset of $S$ is linearly independent.
    \item Every subset of a linearly independent set is linearly independent
\end{remarks}
\begin{example}
    In example~\ref{ex3DSpanningSets}, we can find that $T$ is not linearly independent:
    \begin{equation*}
        1 \times \begin{pmatrix} 2 \\ 1 \\ 2\end{pmatrix} - 5 \times \begin{pmatrix}0 \\ 1 \\ 2\end{pmatrix} + 2 \times \begin{pmatrix}-1 \\ 2 \\ 4\end{pmatrix} = \zv
    \end{equation*}
    But we note that $S$ is linearly independent.
\end{example}
\section{Basis of a Vector Space}
\begin{definition}{Basis}
    A subset $S$ of a vector space $V$ is a \underline{basis} of $V$ if $S$ is both linearly independent and a spanning set for $V$.
\end{definition}
\begin{examples}{}
    \item Let $e_i \in \F^n$ be the vector with its $i$th entry 1, and the rest $0$. Then $\subsetselect{e_i}{1 \leq i \leq n}$ is the \underline{standard basis} for $\F^n$.
    \item Let $P(\R)$ be the set of polynomial functions $\R \mapsto \R$. Let $p_n$ be given by $p_n(x) = x^n$. Then $\subsetselect{p_n}{n \in \N \cup \{0\}}$ is a basis for $P(\R)$.
\end{examples}
\begin{proposition}[Finite-dimensionality in terms of bases]
    Let $S \subseteq V$ be a finite spanning set of $V$. Then there exists $S' \subseteq V$ which is a finite basis for $V$.
    \label{propFinDimByBasis}
\end{proposition}
\begin{proof}
    If $S$ is linearly independent then done. If not, write $S = \{v_1, \cdots, v_n\}$ and initially set $S' = S$. Then there exist scalars $\lambda_1, \cdots, \lambda_n \in \F$ such that the linear combination:
    \begin{equation*}
        \sum_{i = 1}^n \lambda_i v_i = 0
    \end{equation*}
    with at least one coefficient non-zero.

    By possibly re-labelling, assume that $\lambda_n \neq 0$. Then:
    \begin{equation*}
        v_n = -\frac{1}{\lambda_n} \sum_{i = 1}^{n - 1} \lambda_i v_i
    \end{equation*}
    so $v_n \in \spn{S \backslash \{v_n\}}$, and so we can remove $v_n$ from $S'$. If $S'$ is linearly independent, then done. If not, repeat the process.

    This process of removing vectors must terminate because $S$ and $S'$ are finite.
\end{proof}
\begin{corollary}
    Every finite-dimensional vector space has a finite basis.
    \label{corFinDimByBasis}
\end{corollary}
\begin{proof}
    The definition we used of finite-dimensionality is that the vector space has a finite spanning set. Therefore we can create a finite basis using proposition~\ref{propFinDimByBasis}.
\end{proof}
\begin{theorem}[Steinitz Exchange Lemma]
    Let $S, T$ be finite subsets of a vector space $V$ with $S$ linearly independent and $T$ a spanning set for $V$. Then:
    \begin{enumerate}
        \item $|S| \leq |T|$;
        \item There exists $T' \subseteq T$ such that $|T'| = |T| - |S|$ and $S \cup T'$ spans $V$.
    \end{enumerate}
    \label{thmSteinitzExchange}
\end{theorem}
\begin{proof}
    Let $S = \{u_1, u_2, \cdots, u_m\}$ and $T = \{v_1, v_2, \cdots, v_n\}$.

    If $S$ is empty then done, so assume $S$ non-empty.

    We now construct new spanning sets $S_l$ containing elements of $S$.
    
    \induction{$l = 1$}{
        Write $u_1$ as a linear combination of vectors in $V$:
        \begin{equation*}
            u_1 = \sum_{i=1}^{n} \lambda_i v_i
        \end{equation*}
        because $T$ is spanning. By possibly re-numbering, assume $\lambda_1 \neq 0$, so:
        \begin{equation*}
            v_1 = \frac{1}{\lambda_1}\left(u_1 - \sum_{i=2}^{n}\lambda_i v_i\right)
        \end{equation*}
        and so define the set $U_1 = \{u_1, v_2, \cdots, v_n\}$ which spans $V$.
    }{$l = k < n$}{
        Assume that there exists a spanning set $U_k$ with $k$ vectors from $U$ and the rest from $V$.
    }{$l = k + 1$}{
        Write $u_{k+1}$ using the spanning set $U_k$:
        \begin{equation*}
            u_{k+1} = \sum_{j=1}^k \mu_j u_j + \sum_{i = k + 1}^n \lambda_i v_i
        \end{equation*}
        Note that if all $\lambda_i$ were non-zero, then we would have $u_{k+1}$ as a linear combination of other elements of $S$ which contradicts linear independence. Therefore, we must have at least one element $\lambda_i \neq 0$. After possibly re-labelling, let this be $\lambda_{k+1}$:
        \begin{equation*}
            v_{k+1} = \frac{1}{\lambda_{k+1}} \left(u_{k+1} - \sum_{j=1}^{k}\mu_i u_i - \sum_{j=k+2}^{n}\lambda_i v_i\right)
        \end{equation*}
        and so the new set $U_{k+1}$ spans $V$.
    }
    The above process can only terminate when we run out of elements of $S$, when $l = m$. We therefore conclude that $m < n$ and the new set $T' = \{v_{m+1}, \cdots, v_n\}$ is such that $S \cup T'$ spans $V$.
\end{proof}
\begin{corollary}
    Let $V$ be finite-dimensional. Then every basis is finite and each has the same size.
    \label{corBasesSameSize}
\end{corollary}
\begin{proof}
    Given that $V$ is finite-dimensional it has a finite basis $B$. Suppose, for contradiction, that $B'$ is an infinite basis. Then $B'$ must be linearly independent and so must any subset. Let $B''$ be a finite subset of $B'$ which has $|B| + 1$ elements. Then $B''$ is linearly independent. Applying theorem~\ref{thmSteinitzExchange} part 1 with $S = B''$, $T = B$ gives a contradiction.

    To get that bases have the same size, consider two bases $B$ and $B'$ of $V$. Then applying theorem~\ref{thmSteinitzExchange} part 1 first with $S = B, T = B'$ and second with $S = B', T = B$ gives that the sizes are bounded as:
    \begin{equation*}
        |B| \leq |B'| \leq |B| \implies |B| = |B'|
    \end{equation*}
\end{proof}
\begin{definition}{Dimension}
    The \underline{dimension} of a finite-dimensional vector space $V$, $\dim{V}$ is the size of any basis.
\end{definition}
\begin{corollary}
    Let $V$ be finite-dimensional. Let $S$ and $T$ be subsets of $V$, with $S$ be linearly independent and $T$ a spanning set for $V$. Then $|S| \leq \dim{V}$ and $|T| \geq \dim{V}$.

    Equality holds in the corresponding inequality if and only if $S$ or $T$ is in fact a basis.
    \label{corSpanLISizes}
\end{corollary}
\begin{proof}
    Let $B$ be a basis of $V$. Applying theorem~\ref{thmSteinitzExchange} part 1 with $S$ and $B$ gives the first inequality, and applying it again with $B$ and $V$ gives the second.

    If equality holds for $S$, use theorem~\ref{thmSteinitzExchange} part 2 to find $B' \subseteq B$ with $|B'| = |B| - |S| = 0$, so $B' = \emptyset$, and $S \cup B' = S$ spans $V$, so $S$ also spans $V$ and so $S$ is a basis.

    The method is very similar to show the second equality condition.
\end{proof}
\begin{remark}
    In general, we cannot say that a vector space has a basis. This holds for finite-dimensional vector spaces, but for many infinite-dimensional vector spaces we require the axiom of choice. As we have seen, sometimes we can write down a basis for an infinite-dimensional vector space such as $P(\R)$, but we cannot rely on this fact for a general infinite-dimensional vector space.
\end{remark}
\begin{corollary}
    Let $V$ be a finite-dimensional vector space, $U \leq V$. Then $U$ is also finite with dimension at most $\dim(V)$. $\dim(U) = \dim(V)$ if and only if $U = V$.
    \label{corSubspaceDim}
\end{corollary}
\begin{proof}
    If $U = \{\zv_V\}$, then we are done. Therefore assume not. We now want to construct a basis for $U$.

    \induction{$n = 1$}{
        Choose a non-zero vector $u_1 \in U$. Then $U_1 = \{u_1\}$ is a linearly independent set in $U$. 
    }{$n = k$}{
        Assume that there exists a set $U_k$ containing $k$ vectors that are linearly independent.
    }
    {$n = k + 1$}{
        If the set $U_k$ spans $U$ then we are done. If not, choose a vector $u_{k+1}$ that is not in the span of $U$. If we do not have linear independence,
        \begin{equation*}
            \sum_{i = 1}^{k+1} \lambda_iu_{i} = 0
        \end{equation*}
        and note that $\lambda_{k+1} \neq 0$ because the set $U_k$ is linearly independent so cannot have a non-trivial linear combination of $\zv$.
        Rearranging:
        \begin{equation*}
            u_{k + 1} = -\frac{1}{\lambda_{k+1}} \sum_{i=1}^{k} u_k
        \end{equation*}
        which is in the span of $U_k$.~\contradiction
    }

    This forms $U_m$ with $m$ vectors that is a basis for $U$.

    Then this must terminate after at most $\dim(V)$ steps by corollary~\ref{corSpanLISizes}. If in fact $\dim(U) = \dim(V)$, apply corollary~\ref{corSpanLISizes} with $S$ being any basis for $U$.
\end{proof}
\begin{proposition}[Basis Extension Principle]
    Let $V$ be finite-dimensional, and $U \leq V$. For any basis $B_U$ of $U$, there exists a basis $B_V$ for $V$ such that $B_U \subseteq B_V$.
    \label{propBasisExtension}
\end{proposition}
\begin{proof}
    Apply theorem~\ref{thmSteinitzExchange} part 2 with $S = B_U$, $T$ being any basis for $V$.

    Therefore we have $T' \subseteq T$ such that:
    \begin{align*}
        |T'| &= |T| - |S| \\
        &= \dim(V) - \dim(U)
    \end{align*}
    and $B_U \cup T'$ spans $V$. Therefore let $B_V = B_U \cup T'$. The size of a union is at most the sum of the sizes: $|B_V| \leq |B_U| + |T'| = \dim(V)$. Therefore by corollary~\ref{corBasesSameSize} we have that $B_V$ is a basis.
\end{proof}
\section{Bases and Linear Maps}
Consider $V$ a finite-dimensional vector space over $\F$, and let $W$ be a vector space over $\F$. Let $\alpha : V \mapsto W$ be a linear map. Then we can use ideas of bases and dimension to learn more about the kernel and image.
\begin{definition}{Nullity}
    The \underline{nullity} of a linear map $\alpha$, $n(\alpha)$, is defined to be the dimension of the kernel. 
\end{definition}
\begin{definition}{Rank}
    The \underline{rank} of a linear map $\alpha$, $rk(\alpha)$, is defined to be the dimension of the image.
\end{definition}
\begin{lemma}
    Let $V$ be finite-dimensional and $U \leq V$. Then:
    \begin{equation}
        \dim(V / U) = \dim(V) - \dim(U)
        \label{eqnQuotientSizes}
    \end{equation}
    \label{lemQuotientSizes}
\end{lemma}
\begin{proof}
    Let $B_U = \{u_1, \cdots, u_m\}$ be a basis for $U$. Extend to a basis $B_V = \{u_1, \cdots, u_m, v_{m+1}, \cdots, v_n\}$ of $V$. Define a subset of $V / U$:
    \begin{equation*}
        B_{V / U} = \subsetselect{v_i + U}{m+1 \leq i \leq n}
    \end{equation*}
    and note that it has size $n - m$.

    For spanning, let $v \in V$ and write:
    \begin{equation*}
        v = \sum_{i} \lambda_i v_i + \sum_{j} \mu_j u_j
    \end{equation*}
    Then $v + U = \sum_i \lambda_i(v_i + U) \in \spn{B_{V / U}}$.

    For linear independence, suppose there exists a non-trivial linear combination of $\zv$:
    \begin{align*}
        \zv + U &= \sum_i \lambda_i(v_i + U) \\
        &= \left(\sum_i \lambda_i v_i\right) + U \\
        \therefore &\sum_i \lambda_i v_i \in U \\
        \therefore &\sum_i \lambda_i v_i = \sum_j \mu_j v_j
    \end{align*}
    but since $B_V$ is linear independent, all $\lambda_i$ and $\mu_j$ must be zero. We also get uniqueness, since if $v_i + U = v_j + U$ then $v_i - v_j \in U$ which is a contradiction.
\end{proof}
\begin{theorem}[Rank-Nullity Theorem]
    Let $V, W$ be vector spaces over $\F$ and let $V$ be finite-dimensional. Let $\alpha : V \mapsto W$ be a linear map. Then:
    \begin{equation}
        \dim(V) = n(\alpha) + rk(\alpha)
        \label{eqnRankNullity}
    \end{equation}
    \label{thmRankNullity}
\end{theorem}
\begin{proof}
    By the First Isomorphism Theorem,
    \begin{equation*}
        V / \ker{\alpha} \isom \im(\alpha)
    \end{equation*}
    applying lemma~\ref{lemQuotientSizes} gives the result immediately.
\end{proof}
\begin{remark}
    The proof can be done directly without using a lemma, but the ideas are the same.
\end{remark}
\begin{corollary}[Linear pigeon-hole principle]
    Consider the same assumptions as the previous theorem. Assume also that $V$ and $W$ both have dimension $n$. Then the following are equivalent:
    \begin{enumerate}
        \item $\alpha$ is injective
        \item $\alpha$ is surjective
        \item $\alpha$ is an isomorphism
    \end{enumerate}
    \label{corLinPidge}
\end{corollary}
\begin{proof}
    By definition statement 3 implies 1 and 2. Statements 1 and 2 together imply statement 3, so it is sufficient to prove that statements 1 and 2 are equivalent.

    \begin{subproof}{Statement 1 implies statement 2}
        If $\alpha$ is injective then $\ker{\alpha} = \{\zv\}$. That is, $n(\alpha) = 0$. Therefore by theorem~\ref{thmRankNullity}, $rk(\alpha) = n-0 = n$, so $\alpha$ is surjective.
    \end{subproof}
    \begin{subproof}{Statement 2 implies statement 1}
        If $\alpha$ is surjective then $\im{\alpha} = W$. That is, $rk(\alpha) = n$. Therefore by theorem~\ref{thmRankNullity}, $n(\alpha) = n-n = 0$, so $\alpha$ is injective.
    \end{subproof}
\end{proof}
\begin{proposition}
    Let $V$ be a vector space (that can be infinite-dimensional), and suppose we have a basis $B_V$. For any vector space $W$ with a function $f : B_V \mapsto W$, there exists a corresponding unique linear map:
    \begin{equation*}
        F : V \mapsto W
    \end{equation*}
    such that $F(b) = f(b)$ for all basis vectors $b$.
    \label{propLinMapOfBasis}
\end{proposition}
\begin{proof}
    For each $v$ in $V$, write $v$ as a finite sum $\sum_{b} \lambda_b b$. Let $F(v)$ be the finite sum $\sum_b \lambda_b f(b)$.

    \begin{subproof}{$F$ is well-defined}
        As $B$ is a basis, the $\lambda_b$ are uniquely determined by $v$.
    \end{subproof}
    \begin{subproof}{$F$ is linear}
        Let $u, v \in V$. Find their finite linear combinations of basis vectors:
        \begin{equation*}
            u = \sum_b \mu_b b,~~v = \sum_b \lambda_b b
        \end{equation*}
        Then:
        \begin{align*}
            F(u + \lambda v) &= F\left(\sum_b (\mu_b + \lambda\lambda_b)b\right) \\
            &= \sum_b (\mu_b + \lambda \lambda_b) f(b) \\
            &= \sum_b \mu_b f(b) + \lambda\sum_b\lambda_b f(b) \\
            &= F(u) + \lambda F(v)
        \end{align*}
    \end{subproof}
    \begin{subproof}{$F$ is unique}
        Suppose $\bar{F}$ is another linear map that extends the function $f$.
        \begin{align*}
            \bar{F} \left(\sum_b \lambda_b b\right) &= \sum_b\lambda_b \bar{F}(b) \\
            &= \sum_b \lambda_b f(b) \\
            &= F\left(\sum_b \lambda_b b\right)
        \end{align*}
    \end{subproof}
\end{proof}
\begin{corollary}
    Let $\dim(V) = n$, and $B$ a basis for $V$ with elements $\{b_i\}_{i=1}^n$. Then there is an isomorphism:
    \begin{align*}
        F_B : V &\mapsto \F^n\\
        \sum_{i=1}^n \lambda_i b_i &\mapsto \begin{pmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{pmatrix}
    \end{align*}
    \label{corCoordinateVector}
\end{corollary}
\begin{proof}
    Let $E = \{e_1, e_2, \cdots, e_n\}$ be the standard basis for $\F^n$. Define $f : B \mapsto W$ by $f(v_i) = e_i$. Then let $F_B$ be the unique linear extension of $f$ to $V$, as in proposition~\ref{propLinMapOfBasis}.

    Then $f$ defines a bijection from $B$ to $E$. Let $\overline{F_B}$ be the linear extension of $f^{-1} : E \mapsto B$ to $W$.

    Then $\overline{F_B} \circ F_B$ is a linear extension of $Id_B$, but $Id_V$ is also a linear extension of $Id_B$, and since by proposition~\ref{propLinMapOfBasis} these are unique, $\overline{F_B} = F_B^{-1}$.

    Therefore $F_B$ is an isomorphism.
\end{proof}
This isomorphism gives the following definition
\begin{definition}{Coordinate vector}
    Given a finite vector space $V$ with a basis $B = \{b_1, \cdots, b_n\}$, the\\\underline{coordinate vector} of a vector $v \in V$ is a vector in $\F^{\dim(V)}$ given by:
    \begin{equation*}
        \sum_{i=1}^n \lambda_i b_i \mapsto \begin{pmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{pmatrix}
    \end{equation*}
\end{definition}
\begin{corollary}
    Let $V$ and $W$ be finite-dimensional vector spaces over a field $\F$. Then $V \isom W$ if and only if $\dim(V) = \dim(W)$.
    \label{corIsomIffSameDim}
\end{corollary}
\begin{proof}
    If the dimensions are both equal to $n$, use corollary~\ref{corCoordinateVector}:
    \begin{equation*}
        V \isom \F^n \isom W
    \end{equation*}
    If instead there exists an isomorphism, consider the image of the basis of $V$ under this isomorphism to get a new basis of the same size in $W$.
\end{proof}
\section{Direct Sums}
\begin{definition}{(External) Direct sum}
    The \underline{(external) direct sum} of vector spaces $V$ and $W$ over a field $\F$, denoted $V \oplus W$, is the vector space over $\F$ with underlying set $V \times W$ and operations:
    \begin{itemize}
        \item $(v_1, w_1) + (v_2, w_2) = (v_1 + v_2, w_1 + w_2)$
        \item $\lambda(v, w) = (\lambda v, \lambda w)$.
    \end{itemize}
\end{definition}
Similarly, we can chain this operation to define the external direct sum for a finite number of vector spaces.

\begin{lemma}
    For $V$ and $W$ vector spaces over $\F$,
    \begin{equation}
        \dim(V \oplus W) = \dim(V) + \dim(W)
        \label{eqnDSDim}
    \end{equation}
    \label{lemDSDim}
\end{lemma}
\begin{proof}[by finding a basis for the direct sum]
    Find bases $B$ and $C$ for $V$ and $W$. Define:
    \begin{equation*}
        D = (B \times \{\zv_W\}) \cup (\{\zv_V\} \times C)
    \end{equation*}
    Then we can check that this is a basis and note that it has size $|B| + |C|$.
\end{proof}
\begin{proof}[By isomorphism]
    Suppose $V \isom \F^n$ and $W \isom \F^m$. Then we want to construct an isomorphism $V \oplus W \isom \F^{n+m}$. Consider a basis $B$ for $V$ and $C$ for $W$, and construct the isomorphism as follows:
    \begin{align*}
        \phi : V \oplus W &\mapsto \F^{n+m}\\
        \left(\begin{pmatrix} v_1 \\ \vdots \\ v_n\end{pmatrix}, \begin{pmatrix} w_1 \\ \vdots \\ w_m\end{pmatrix}\right) &\mapsto \begin{pmatrix} v_1 \\ \vdots \\ v_n \\ w_1 \\ \vdots \\ w_m\end{pmatrix}
    \end{align*}
\end{proof}
\begin{proposition}
    Let $V$ be a vector space over $\F$. Let $U, W \leq V$. Then there exists a surjective linear map:
    \begin{align*}
        \phi : U \oplus W &\mapsto U + W\\
        (u, w) &\mapsto u + w
    \end{align*}
    with $\ker(\phi) = U \cap W$.
    \label{propMapDSToSum}
\end{proposition}
\begin{proof}
    Linearity is clear. By considering any element of $U + W$, we must be able to represent it as $u + w$ for some $u \in U, w \in W$ by definition, and then $\phi(u, w)$ is the required element which gives surjectivity.

    Note for $(u, w) \in U \oplus W$, $(u, w) \in \ker(\phi)$ if and only if $w = -u$. Therefore, $\ker(\phi) = \subsetselect{(x, -x)}{x \in U \cap W}$. Then we can define:
    \begin{align*}
        \psi : U \cap W &\mapsto \ker(\phi)\\
        x &\mapsto (x, -x)
    \end{align*}
    which is an isomorphism.
\end{proof}
\begin{corollary}[Sum-Intersection Formula]
    If $V$ is a finite-dimensional vector space over $\F$ and $U, W \leq V$,
    \begin{equation*}
        \dim(U + W) = \dim(U) + \dim(W) - \dim(U \cap W)
    \end{equation*}
    \label{corSumIntersection}
\end{corollary}
\begin{proof}
    Apply theorem~\ref{thmRankNullity} to the linear map from proposition~\ref{propMapDSToSum}:
    \begin{align}
        \dim(U) + \dim(W) &= \dim(U \oplus W) \\
        &= \dim(\ker(\phi)) + \dim(\im(\phi)) \\
        &= \dim(U \cap W) + \dim(U + W)
    \end{align}
\end{proof}
We can construct an explicit basis for this operation:

Given a basis $B$ for $U \cap W$, extend $B$ to a basis $B_U$ for $U$ and $B_W$ for $W$. Then consider the set $B_U \cap B_W$. Therefore this spans $U + W$ and:
\begin{equation}
    |B_U \cap B_W| \leq |B_U| + |B_W| - |B|
    \label{eqnBasisDSIneq}
\end{equation}
but by the Sum-Intersection formula, the RHS of equation~\ref{eqnBasisDSIneq} is equal to $\dim(U + W)$. Therefore $B_U \cap B_W$ is linearly independent, so is a basis for $U + W$.
\begin{definition}{(Internal) Direct sum}
    Suppose $U, W \leq V$ satisfying:
    \begin{enumerate}
        \item $U + W = V$,
        \item $U \cap W = \{\zv_V\}$
    \end{enumerate} 
    then $U \oplus W$ is the \underline{(internal) direct sum} of $V$.
\end{definition}
We see that this is compatible with our previous notion by considering the map $\phi$ in proposition~\ref{propMapDSToSum}, and noting that the assumption $U \cap V = \{\zv_V\}$ implies that the kernel is trivial, so $\phi$ is an isomorphism.
\begin{definition}{Direct complement}
    For $U \leq V$, a \underline{direct complement} to $U$ in $V$ is a subspace $W \leq V$ such that:
    \begin{equation*}
        V = U \oplus W
    \end{equation*}
\end{definition}
\begin{proposition}
    If $V$ is finite-dimensional, any subspace $U$ has a direct complement $W$.
    \label{propDCExists}
\end{proposition}
\begin{proof}
    Let $U \leq V$. Let $B_U$ be a basis for the subspace, and extend $B_U$ to a basis $B_V$ for $V$. Set $W = \spn{B_V \backslash B_U}$.

    Then:
    \begin{align*}
        V &= \spn{B_V} = \spn{B_U \cap (B_V \backslash B_U)} \\
        &= \spn{B_U} + \spn{B_V \backslash B_U} \\
        &= U + W
    \end{align*}
    Moreover, since $B_V$ is linearly independent,
    \begin{equation*}
        U \cap W = \spn{B_U} \cap \spn{B_V \backslash B_U} = \{\zv_V\}
    \end{equation*}
\end{proof}
We can also apply this to $n$ subspaces $U_i, 1 \leq i \leq n$. We say that $V$ is the (internal) direct sum of the $U_i$, and write:
\begin{equation*}
    V = U_1 \oplus U_2 \oplus \cdots \oplus U_n
\end{equation*}
If the map $\phi : U_1 \oplus \cdots \oplus U_n \mapsto V$ is an isomorphism.
\end{document}