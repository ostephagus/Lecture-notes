\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Defining a Field}
In this section we will quickly run through the important properties of a field. Proofs are very quick and so are not explicitly given.
\subsection{Definition and Examples}
\begin{definition}{Field}
    A \underline{field} is a tuple $(\F, +, \cdot, 0, 1)$ where $\F$ is a set, $+$ (addition) and $\cdot$ (multiplication) are binary operations on $\F$, and $0, 1 \in F$ such that:
    \begin{enumerate}
        \item $(\F, +, 0)$ is an Abelian group;
        \item $(\F \backslash \{0\}, \cdot, 1)$ is an Abelian group
        \item For all $a, b, c \in \F$, $a \cdot (b + c) = a \cdot b + a \cdot c$.
        \item $0 \neq 1$.
    \end{enumerate}
\end{definition}
For $a \in \F$ we write $-a$ for its additive inverse, and $a^{-1}$ for its multiplicative inverse. We can easily prove that for every $a \in \F, 0 \cdot a = 0$. Therefore, if axiom 4 were false, we would have that $\F = \{0\}$.
\begin{examples}
    \item $\R, \C$ and $\Q$ are all examples of fields with the standard multiplication and addition operations.
    \item If $p$ is prime, the $\Z / p\Z$ is a field (see IA Numbers and Sets, and B\'ezout's Theorem).
\end{examples}
\begin{examples}[Non-examples of fields]
    \item The integers $\Z$ do not form a field because we lack multiplicative inverses.
    \item If $n \in \N$ is not prime then $\Z / n\Z$ is not a field. Also, if $n = 1$ then we do not have axiom 4.
\end{examples}
\subsection{Classifying Fields}
\begin{definition}{Finite field}
    A field is \underline{finite} (or infinite) if the corresponding set is finite (or infinite).
\end{definition}

We cannot divide by zero in a field, because this would contradict the axiom of total collapse. This means we must be careful when taking multiplicative inverses. For instance, $(1 + 1)^{-1}$ is not defined on $\Z / 2\Z$ because $1 + 1 = 0$.
\begin{definition}{Algebraic closure}
    A field $\F$ is \underline{algebraically closed} if any polynomial equation $f(X) = 0$, with coefficients in $\F$, has a solution in $\F$.
\end{definition}
\begin{example}
    $\C$ is the most obvious algebraically closed field by the Fundamental Theorem of Algebra.
\end{example}
\begin{examples}[Non-examples of algebraic closure]
    \item $\R$ is not algebraically closed, $x^2 + 1 = 0$ has no solutions in $\R$.
    \item For any prime $p$, $\Z / p\Z$ is not algebraically closed. Indeed, we can show that there exist quadratic equations for each such field that have no solutions in that field.
\end{examples}
\section{Defining a Vector Space}
\begin{definition}{Vector space}
    A \underline{vector space} over a field $\F$ is an Abelian group $(V, +, 0)$ equipped with a function:
    \begin{align*}
        \F \times V &\mapsto V
        (\lambda, v) &\mapsto \lambda v
    \end{align*}
    called scalar multiplication.

    We also require that, for all $v, w \in V$ and $\lambda, \mu \in \F$,
    \begin{enumerate}
        \item $(\lambda + \mu)v = \lambda v + \mu v$
        \item $\lambda (v + w) = \lambda v + \lambda w$
        \item $\lambda (u v) = (\lambda u)v$
        \item $1 \cdot v = v$.
    \end{enumerate}
    Elements of the field $\F$ are called \underline{scalars}, and elements of $V$ are called \underline{vectors}.
\end{definition}
We will not make bold elements of $V$, which is different to IA courses, except the vector $\zv$ which may otherwise be confused with the additive identity of $\F$, $0$.

\begin{example}
    We have often considered column vectors, $V = \F^n$, with the familiar ideas of vector addition and scalar multiplication.

    $\C^n$ is a complex vector space, but it is also a real vector space because we can simply take the field as $\R$ not $\C$.
\end{example}
\begin{example}
    We can also consider the vector space of matrices over a field $\F$ with $n$ columns and $m$ rows.
\end{example}
\begin{example}
    For any non-empty set $X$, we can consider $F^X$ as the set of function on $X$:
    $\F^X = \subsetselect{f : X \mapsto \F}{\text{f is a function}}$. Then we define addition of vectors:
    \begin{equation*}
        f+g := (f + g)(x) = f(x) + g(x)
    \end{equation*}
    and scalar multiplication:
    \begin{equation*}
        \lambda f := (\lambda f)(x) = \lambda (f(x))
    \end{equation*}
\end{example}
\begin{propositions}{
        Let $V$ be a vector space over a field $\F$, let $v \in V$.
    }
    \item $0v = \zv$ \label{propZeroVector}
    \item If $(-1) \in \F$ is the additive inverse of the multiplicative identity $1$, $(-1)v = -v$, the additive inverse of $v$. \label{propNegativeVector}
    \label{propsSpecialVectors}
\end{propositions}
\section{Vector Subspaces}
\begin{definition}{Subspace}
    A \underline{subspace} of a vector space $V$ over $\F$ is a subset $U \subseteq V$ which is a vector space over $\F$ over the same operations as $V$.

    Equivalently, $(U, +)$ is a subgroup of $(V, +)$ and $\forall \lambda \in \F, u \in U, \lambda U \in U$.
\end{definition}
\begin{remark}
    This ensures the vector space axioms are already inherited.
\end{remark}
\begin{proposition}[Subspace test]
    Let $V$ be a vector space over $\F$. Let $U \subseteq V$. Then $U$ is a subspace of $V$ if and only if:
    \begin{enumerate}
        \item $U \neq \emptyset$
        \item $\forall u, w \in U,~\forall \lambda \in \F,~ u + \lambda w \in U$.
    \end{enumerate}
    \label{propSubspaceTest}
\end{proposition}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let $U$ be a subspace of $V$}
        Then we can use the axioms to show the 2 conditions.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose $U \subseteq V$ satisfies the two conditions.}
        Let $\lambda = (-1)$ in the second criterion, then for all $u, v \in U, u - w \in U$ and $U \neq \emptyset$. By the subgroup test from IA Groups, $(U, +)$ is a subgroup of $(V, +)$. Finally, take $u = \zv$ in the second condition. Then for all $w \in U, \lambda \in \F, \lambda w \in U$.
    \end{proofdirection}
\end{proof}
Notationally, for ``subspace of'' we write $U \leq V$.
\begin{example}
    Consider the set:
    \begin{equation*}
        \left\{\begin{pmatrix} x \\ y \\ z\end{pmatrix} \in \R^3~:~x + y + z = t\right\} \subseteq \R^3
    \end{equation*}
    where $t \in \R$ is fixed. Then this is a vector space if and only if $t = 0$.
\end{example}
\begin{example}
    $\R^{\R}$, the set of functions on the reals, is a valid vector space over $\R$. The set of such functions that are continuous, $C(\R)$, is a subspace.
    The set of infinitely differentiable functions, $C^{\infty}(\R)$ is a subspace of this, and the set of polynomial functions, $P(\R)$, is a subspace again.
\end{example}
\begin{lemma}
    For $U, W \leq V$,
    \begin{equation*}
        U \cap W \leq V
    \end{equation*}
    \label{lemIntersectSubspace}
\end{lemma}
\begin{proof}
    We need to check first that the set is non-empty. The easiest way to do this is to check it contains $\zv$. We have that, since $U$ and $W$ are independently subspaces of $V$ and so both contain $\zv$. Therefore, $\zv \in  U \cap W$.

    Now consider $x, y \in U \cap W$, $\lambda \in \F$.

    Since $U$ is a subspace of $V$, $x + \lambda y \in U$. The same applies for $W$, so $x + \lambda y \in U \cap W$. This holds for all such $x, y, \lambda$.

    Thus by proposition~\ref{propSubspaceTest}, $U \cap W \leq V$.
\end{proof}
\begin{warning}
    Union rarely preserves subspaces.
\end{warning}
Instead of taking a union, we can take a subspace sum:
\begin{definition}{Subspace sum}
    The \underline{subspace sum} of $U, W$ is:
    \begin{equation*}
        U + W = \subsetselect{u + w}{u \in U, w \in W}
    \end{equation*}
\end{definition}
\begin{lemma}
    If $U, W \leq V$ then $U + W \leq V$.
    \label{lemSubspaceSum}
\end{lemma}
\begin{proof}
    The proof is a simple application of the subspace test.
\end{proof}
\begin{remark}
    $U + W$ is the smallest subspace of $V$ containing both $U$ and $W$, since any other subspace of $V$ that contains $U$ and $W$ must contain $U + W$ by the axiom of addition.
\end{remark}
\section{Linear Maps}
\subsection{Definition and Examples}
\begin{definition}{Linear map}
    A \underline{linear map} from $V$ to $W$ is a function $\alpha : V \mapsto W$ such that, for all $u, v \in V, \lambda \in \F$:
    \begin{enumerate}
        \item $\alpha(u + v) = \alpha(u) + \alpha(v)$
        \item $\alpha(\lambda v) = \lambda \alpha(v)$.
    \end{enumerate}
\end{definition}
We can combine these conditions into one: $\alpha(u + \lambda v) = \alpha(u) + \lambda \alpha(v)$.

\begin{example}[Matrix multiplication is linear]
    Let $V = \F^n, W = \F^m$. Let $A \in M_{m \times n}$, and let $\alpha$ be given by $\alpha(v) = Av$.

    Then we can use the test above to show that $\alpha$ is linear.
\end{example}
\begin{example}[Differentiation/integration is linear]
    We know from analysis that differentiation is linear, in this case differentiation acts on the set of differentiable functions of $\R$ to the set of functions of $\R$.

    Integration acts on the set of integrable functions of $\R$.
\end{example}
\begin{example}[Functions are not necessarily linear]
    Let $X$ be a non-empty set and let $\alpha : \F^X \mapsto \F$ using a function $f$. Then we know that these functions are not necessarily linear, such as $f(x) = x^2$ on the real numbers.
\end{example}

\subsection{Isomorphisms}
\begin{definition}{Isomorphism}
    An \underline{isomorphism} of vector spaces $V$ and $W$ over a field $\F$ is a bijective linear map $\alpha : V \mapsto W$. If such an isomorphism exists, we say $V$ and $W$ are \underline{isomorphic} and write $V \isom W$.
\end{definition}
\begin{example}
    Consider $V = \F^4$, $W = M_{2 \times 2}(\F)$. Then consider the linear map:
    \begin{equation*}
        \alpha\left(\begin{pmatrix}a \\ b \\ c \\ d\end{pmatrix}\right) = \begin{pmatrix}a & b \\ c & d\end{pmatrix}
    \end{equation*}
    Injectivity and surjectivity are immediate, and we have linearity because the function is applied individually to each argument, which respects matrix and vector multiplication.
\end{example}
\begin{proposition}
    Let $V, W$ be vector spaces of a field $\F$. Then let $\alpha : V \mapsto W$ be an isomorphism.
    The map $\alpha^{-1} : W \mapsto V$ is an isomorphism.
    \label{propInverseIsomorphism}
\end{proposition}
\begin{proof}
    We have that $\alpha^{-1}$ is a bijection.

    Let $w_1, w_2 \in W, \lambda \in \F$.
    There exist $v_1$ and $v_2$ in $V$ such that, for $i = 1, 2, w_i = \alpha(v_i)$.
    \begin{align*}
        \alpha^{-1}(w_1 + \lambda w_2) &= \alpha^{-1}(\alpha(v_1) + \lambda \alpha(v_2)) \\
        &= \alpha^{-1} (\alpha(v_1 + \lambda v_2)) \\
        &= v_1 + \lambda v_2 \\
        &= \alpha^{-1}(w_1) + \lambda \alpha^{-1}(w_2)
    \end{align*}
\end{proof}
\subsection{Image and Kernel}
\begin{definition}{Kernel}
    Let $V, W$ be vector spaces over $\F$. Let $\alpha : V \mapsto W$ be a linear map. Then the \underline{kernel} of $\alpha$ is:
    \begin{equation*}
        \kera = \subsetselect{v \in V}{\alpha(v) = \zv_W} \subseteq V
    \end{equation*}
\end{definition}
\begin{definition}{Image}
    Let $V, W$ be vector spaces over $\F$. Let $\alpha : V \mapsto W$ be a linear map. Then the \underline{image} of $\alpha$ is:
    \begin{equation*}
        \im(\alpha) = \subsetselect{\alpha(v) \in W}{v \in V} \subseteq W
    \end{equation*}
\end{definition}
\begin{propositions}{
        Let $V$, $W$ be vector spaces over $\F$. Let $\alpha : V \mapsto W$ be a linear map.
        \label{propKernelImage}
    }
    \item $\ker{\alpha} \leq V$ \label{propKerSubspace}
    \item $\im{\alpha} \leq W$ \label{propKerImage}
    \item $\alpha$ is surjective if and only if $\im{\alpha} = W$ \label{propSurjectiveIffIm}
    \item $\alpha$ is injective if and only if $\ker{\alpha} = \{\zv_W\}$ \label{propInjectiveIffKer}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Kernel is a subspace of $V$:
            First note that the kernel is certainly a subset of $V$. First note that:
            \begin{align*}
                \alpha(\zv_V) &= \alpha(\zv_V + \zv_V) \\
                &= \alpha(\zv_V) + \alpha(\zv_V) \\
                \therefore \zv_W &= \alpha(\zv_V)
            \end{align*}
            This means that $\zv_V \in \kera$, so $\kera \neq \emptyset$.

            Now suppose $u, v \in \kera, \lambda \in \F$.
            \begin{align*}
                \alpha(u + \lambda v) &= \alpha(u) + \lambda \alpha(v) \\
                \therefore u + \lambda v &\in \kera
            \end{align*}
        \item Image is a subspace of $W$:
            The proof proceeds similarly to the above
        \item Surjectivity criterion:
            This is reached directly from the definition of surjectivity.
        \item Injectivity criterion:
            \begin{proofdirection}{$\Rightarrow$}{Suppose that $\alpha$ is injective}
                $\alpha(\zv_V) = \zv_W$, so $\zv_V \in \kera$. Let $v \in \kera$. Then $\alpha(v) = \zv_W = \alpha(\zv_V)$. Therefore, $v = \zv_V$, and the kernel is trivial.
            \end{proofdirection}            
            \begin{proofdirection}{$\Leftarrow$}{Suppose that $\kera = \{\zv_V\}$}
                Let $u, v \in V$ and suppose $\alpha(u) = \alpha(v)$.
                \begin{align*}
                    \alpha(u) - \alpha(v) &= \zv_W \\
                    \alpha(u - v) &= \zv_W \\
                    u - v &\in \kera \\
                    u - v &= \zv_V \\
                    u &= v
                \end{align*}
                So $\alpha$ is injective.
            \end{proofdirection}
    \end{enumerate}
\end{proof}
\section{Quotient Spaces}
Note that, since vector spaces are Abelian groups under addition of vectors, and under scalar multiplication, all subspaces are normal subgroups. We can therefore define an analogue to a quotient group for a vector space.
\begin{definition}{Left coset}
    Let $V$ be a vector space over a field $\F$, and let $W \leq V$. Define the \underline{left coset} of $W$ by an element of $V$ as:
    \begin{equation*}
        v + W = \subsetselect{v + w}{w \in W} 
    \end{equation*}
\end{definition}
Then the set of left cosets is:
\begin{equation*}
    V / W = \subsetselect{v + W}{v \in V}
\end{equation*}
Recall that for $u, v \in V$,
\begin{equation}
    u + W = v + W \Leftrightarrow u - v \in W
    \label{eqnCosetEquality}
\end{equation}
\begin{definition}{Quotient space}
    The \underline{quotient space} of a vector space $V$ by a subspace $W$ is the vector space $V / W$ with the associated operations:
    \begin{enumerate}
        \item $(u + W) + (v + W) = (u + v) + W$
        \item $\lambda(u + W) = (\lambda u) + W$.
    \end{enumerate}
\end{definition}
\begin{proposition}
    The quotient space $V / W$ is a vector space and its operations are well-defined.
    \label{propQuotientSpace}
\end{proposition}
\begin{proof}
    The proof is similar to that in IA Groups.
\end{proof}
\begin{proposition}[Quotient Map]
    There exists a well-defined function:
    \begin{align*}
        \pi_W : V &\mapsto V / W \\
        v &\mapsto v + W
    \end{align*}
    which is surjective and has kernel $\ker(\pi_W) = W$.
    \label{propQuotMap}
\end{proposition}
\begin{proof}
    \begin{subproof}{$\pi_W$ is surjective}
        Let $v + W \in V / W$. The $\pi_W (v) = v + W$.
    \end{subproof}
    \begin{subproof}{$\pi_W$ is linear}
        Let $u, v \in V, \lambda \in \F$.
        \begin{align*}
            \pi_W(u + \lambda v) &= (u + \lambda v) + W \\
            &= (u + W) + ((\lambda v) + W) \\
            &= (u + w) + \lambda (v + W) \\
            &= \pi_W (u) + \lambda \pi_W (v)
        \end{align*}
    \end{subproof}
    \begin{subproof}{The kernel of $\pi_W$ is $W$}
        Let $v \in \ker(\pi_W), \pi_W(v) = \zv_{V / W} = W$. Therefore by the criterion in equation~\ref{eqnCosetEquality}, $v - \zv_V \in W$, so $v \in W$. Clearly if $v \in W$ then $v \in \ker(\pi_W)$.
    \end{subproof}
\end{proof}
\begin{theorem}[First isomorphism theorem]
    Let $V, W$ be vector spaces over a field $\F$. Let $\alpha : V \mapsto W$ be a linear map.

    Then there exists an isomorphism $\bar\alpha : V / \kera \mapsto \im(\alpha)$ given by $\bar\alpha(v + \kera) = \alpha(v)$.
    \label{thmIsom1}
\end{theorem}
\begin{proof}
    \begin{subproof}{$\bar{\alpha}$ is well-defined and injective}
        Let $u, v \in V$.
        \begin{align*}
            &u + \kera = v + \kera \\
            &\equiv u - v \in \kera \\
            &\equiv \alpha(u - v) = \zv_W \\
            &\equiv \alpha(u) = \alpha(v) \\
            &\equiv \bar\alpha (u + \kera) = \bar\alpha(v + \kera)
        \end{align*}
        Then the forward direction implies that $\bar\alpha$ is well-defined, and the backward direction gives that $\bar\alpha$ is injective.
    \end{subproof}
    \begin{subproof}{$\bar\alpha$ is surjective}
        Let $w \in \im(\alpha)$, so $\exists v \in V$ such that $w = \alpha(v)$.

        Then $w = \bar\alpha(v + K)$.
    \end{subproof}
    \begin{subproof}{$\bar\alpha$ is linear}
        Let $u, v \in V, \lambda \in \F$.
        \begin{align*}
            \bar\alpha((u + \kera) &+ \lambda (v + \kera)) = \bar\alpha((u + \lambda v) + \kera) \\
            &= \alpha(u + \lambda v) \\
            &= \alpha(u) + \lambda \alpha(v) \\
            &= \bar\alpha(u + \kera) + \lambda\bar\alpha(v + \kera)
        \end{align*}
    \end{subproof}
\end{proof}
\end{document}