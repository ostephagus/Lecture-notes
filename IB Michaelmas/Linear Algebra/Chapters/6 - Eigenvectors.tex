\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Polynomials}
\begin{definition}{Polynomial}
    A \underline{polynomial} $f$ over a field $\F$ is a formal expression:
    \begin{equation*}
        f(t) = \sum_{i=0}^{n} a_i t^i
    \end{equation*}
    Here $n \in \N \cup \{0\}$ and $a_i \in \F$ are the \underline{coefficients}.
\end{definition}
Define $\F[t]$ to be the vector space over $\F$ of all polynomials. We have the basis $\{1 = t^0, t^1, t^2, \cdots\}$.

\begin{definition}{Degree}
    The \underline{degree} $\deg{f}$ of $f \in \F[t]$ is the largest $i$ such that $a_i \neq 0$. We have the convention that if $f$ is the \underline{zero polynomial}, $f(t) = 0$, $\deg(f) = -\infty$.
    We call this $a_i$ the \underline{leading coefficient}
\end{definition}
\begin{definition}{Monic polynomial}
    A polynomial $f \in \F[t]$ is \underline{monic} if its leading coefficient is $1$.
\end{definition}
We define addition and multiplication for any polynomials $f, g \in \F[t]$
\begin{align*}
    (f + g)(t) &= \sum_{i=0}^{\max\{\deg(f), \deg(g)\}} (a_i + b_i)t^i \\
    (fg)(t) &= \sum_{i=0}^{\deg(f) + \deg(g)} \left(\sum_{k=0}^{i} a_kb_{k-i}\right)t^i
\end{align*}
Note that the degree of $f + g$ is at most $\max\{\deg(f), \deg(g)\}$ (because the leading coefficient can cancel), but the degree of $fg$ is exactly $\deg(f) + \deg(g)$.

Write $f | g$ if there exists $h \in \F[t]$ satisfying $g = fh$.

For $\lambda \in \F$ we can \underline{evaluate} $f$ at $\lambda$ by:
\begin{equation*}
    f(\lambda) = \sum_{i=0}^{n} a_i \lambda^i
\end{equation*}
\begin{warning}
    We distinguish between $\F[t]$ and the space of polynomial maps $\F \mapsto \F$. If $\F$ is finite, then $\F[t]$ is not finite-dimensional but the space of polynomial maps on $\F$ is a subspace of $\F^{\F}$ which is finite-dimensional. This comes about because some polynomials agree on all points in $\F$ and so are not unique as functions (but have different coefficients). For example, in $\F = \Z / 5\Z$, the polynomials $f(t) = t$ and $f(t) = t^5$ encode the same underlying function but have different coefficients.
\end{warning}
\begin{proposition}[Euclid's Algorithm for Polynomials]
    For $f, g \in \F[t]$, $g \neq 0$, there exist $q, r \in \F[t]$ such that:
    \begin{equation*}
        f = qg + r
    \end{equation*}
    where $\deg(r) < \deg(q)$.
    \label{propEuclidAlgPoly}
\end{proposition}
\begin{proof}
    The proof is very similar to the integer case.

    Let $n = \deg(f), m = \deg(g)$. If $m > n$ set $r = f, q = 0$.

    If $n \geq m$ we can perform an induction by setting $\tilde{f}(t) = f(t) - \left(\frac{a_n}{b_m}\right)t^{n-m} g(t)$, which has degree less than $\deg(f)$.
\end{proof}
\begin{corollary}[B\'ezout's Lemma for Polynomials]
    If $f_1, \cdots, f_k$ are polynomials with no non-constant common divisor, then:
    \begin{equation*}
        \exists q_1, \cdots, q_n \in \F[t] \text{ s.t. } 1 = \sum_{i=1}^{k} f_i q_i
    \end{equation*}
    \label{corBezoutPolynom}
\end{corollary}
\begin{lemma}
    For $\lambda \in \F$, $f(\lambda) = 0$ if and only if $(t - \lambda) | f(t)$.
    \label{lemFactorTheorem}
\end{lemma}
The proof is by applying Euclid's Algorithm to $f(t)$ with $g(t) = t - \lambda$.

\begin{definition}{Root of a polynomial}
    A scalar $\lambda \in \F$ is a \underline{root} of a polynomial $f \in \F[t]$ if $f(\lambda) = 0$.
\end{definition}
\begin{definition}{Multiplicity}
    A root $\lambda \in \F$ for a polynomial $f$ has \underline{multiplicity} $e$ which is the largest integer satisfying $(t - \lambda)^e | f$.
\end{definition}
\begin{corollary}
    If $\deg(f) = n \geq 0$, then $f$ has at most $n$ roots (when considering multiplicity).
    \label{corPolyNRoots}
\end{corollary}
\begin{corollary}
    For monic polynomials $f$ and $g$, suppose that $\deg(f)$ and $\deg(g) \leq n$ and we can find a set of $n$ roots $\lambda_i$ such that, for each $i$, $f(\lambda_i) = g(\lambda_i) = 0$.

    Then $f = g$.
    \label{corPolyCompareRoots}
\end{corollary}
\begin{proof}
    The proof is by factoring both $f$ and $g$ into a product of linear factors $(x - \lambda_i)$, and noting that these factorisations are the same so the coefficients of $f$ and $g$ are equal.
\end{proof}
\begin{theorem}[Fundamental Theorem of Algebra]
    Every $f \in \C[t]$ of degree $n \geq 1$ has exactly $n$ roots, when considering multiplicity.
    \label{thmFundamentalAlgebra}
\end{theorem}
\begin{proof}
    The proof is seen in IB Complex Analysis.
\end{proof}
\section{An Introduction to Eigenspaces}
\begin{definition}{Diagonalisability}
    Let $V$ be a finite-dimensional vector space. Let $\alpha : V \mapsto V$. This is \underline{diagonalisable} if there exists a basis $B$ of $V$ such that $[\alpha]_B^B$ is a diagonal matrix.
\end{definition}
\begin{definition}{Triangularisability}
    Let $V$ be a finite-dimensional vector space. Let $\alpha : V \mapsto V$. This is \underline{triangularisable} if there exists a basis $B$ of $V$ such that $[\alpha]_B^B$ is a triangular matrix.
\end{definition}
\begin{remarks}
    \item $A \in M_{n \times n}(\F)$ is diagonalisable if $A$ is similar to a diagonal matrix.
    \item $A \in M_{n \times n}(\F)$ is triangularisable if $A$ is similar to a triangular matrix.
\end{remarks}
Diagonal and triangular matrices have nice properties with their determinants. It is easy to compute the determinant of an upper triangular matrix, since this is simply the product of the diagonal elements, and it is easy to understand the similarity classes of a diagonal matrix.

If $B = \{v_1, \cdots, v_n\}$ is a basis for a space $V$, and in this basis a map $\alpha : V \mapsto V$ is given by:
\begin{equation*}
    [\alpha]_B^B =
    \begin{pmatrix}
        \lambda_1 &  & 0 \\
         & \ddots &  \\
        0 &  & \lambda_n
    \end{pmatrix}
    = \diag(\lambda_1, \cdots, \lambda_n)
\end{equation*}
then $\alpha(v_i) = \lambda_i v_i$.
\begin{definition}{Eigenvalue}
    Let $V$ be a vector space over $\F$, and let $\alpha \in \L(V, V)$. $\lambda \in \F$ is an \underline{eigenvalue} of $\alpha$ if there exists a non-zero vector $v \in V$ such that:
    \begin{equation*}
        \alpha(v) = \lambda v
    \end{equation*}
    In this case, $v$ is an \underline{eigenvector} of $\alpha$.
\end{definition}
\begin{definition}{Eigenspace}
    Let $V$ be a vector space over $\F$. For an eigenvalue $\lambda$ of a linear map $\alpha : V \mapsto V$, the \underline{$\lambda$-eigenspace} is given by:
    \begin{equation*}
        V_{\lambda} = \subsetselect{v \in V}{\alpha(v) = \lambda v}
    \end{equation*}
    That is, all the eigenvectors with eigenvalue $\lambda$.
\end{definition}
\begin{propositions}{
        Let $V$ be a vector space over a field $\F$.
        \label{propsEVals}
    }
    \item $V_\lambda = \ker(\alpha - \lambda \Id_V) \leq V$ \label{propEValKernel}
    \item For $V$ finite-dimensional, $\alpha$ is diagonalisable if and only if $V$ has a basis of eigenvectors of $\alpha$. \label{propDiagIffBasis}
    \item If $v \in V_{\mu}$, for $\lambda \neq \mu$, then
        \begin{equation*}
            (\alpha - \lambda \Id_V) (v) = (\mu - \lambda)(v)
        \end{equation*}
        Thus $(\alpha - \lambda \Id_V)$ preserves $V_{\mu}$ and $(\alpha - \lambda \Id_V) \vline_{V_\mu}$ is invertible with inverse $v \mapsto (\mu - \lambda)^{-1} v$.
        \label{propEValDifferentKernel}
\end{propositions}
\begin{proof}
    The proofs of these statements are quite straightforward.
    \begin{enumerate}
        \item If $v \in V_{\lambda}$ then $\alpha(v) = \lambda v$. Equivalently, $(\alpha(v) - \lambda \Id_V(v)) = \zv$, so $v$ is in $\ker(\alpha - \Id_V)$. Similarly, if $v \in \ker(\alpha - \Id_V)$ then $\alpha(v) = \lambda v$ and $v \in V_{\lambda}$.
        \item This is by noting that the basis required by the definition of a diagonal matrix is in fact the eigenvector basis.
        \item This is immediate by working through the steps in the statement.
    \end{enumerate}
\end{proof}
\begin{lemma}
    Let $\lambda_1, \cdots, \lambda_k \in \F$ be a set of distinct eigenvalues over a vector space $V$, and $\zv \neq v_i \in V_{\lambda_i}$ be a set of corresponding eigenvectors. Then the set $\{v_1, \cdots, v_k\}$ is linearly independent.
    \label{lemEVecLI}
\end{lemma}
\begin{remark}
    The lemma is more succinctly stated: eigenvectors corresponding to distinct eigenvalues are linearly independent.
\end{remark}
\begin{proof}
    Suppose not. Let $\sum_{i=0}^{k} \mu_i v_i = \zv_V$ and assume that $\mu_i$ are not all zero. After possibly re-labelling, assume $\mu_1 \neq 0$. Define also:
    \begin{equation*}
        \beta = \prod_{i=2}^{k}(\alpha - \lambda_i \Id_V)
    \end{equation*}
    Then by proposition~\ref{propEValDifferentKernel},
    \begin{align*}
        \beta(v_j) &= \left(\prod_{i=2}^{k}(\lambda_j - \lambda_i)\right) v_j \\
        &\neq 0 \text{ if and only if } j = 1. \\
        \therefore \zv &= \beta\left(\sum_{i = 1}^k \mu_i v_i\right) \\
        &= \beta(\mu_1 v_1) \text{ by previous remark} \\
        &= \left(\mu_1 \prod_{i = 1}^k(\lambda_1 - \lambda_k)\right) v_1
    \end{align*}
    But, since the eigenvalues are all distinct, we must have $\mu_1 = 0$ \contradiction
\end{proof}
\begin{corollary}
    Let $V$ be a vector space over a field $\F$ with dimension $n$. Any linear map $\alpha \in \L(V, V)$ has at most $n$ eigenvalues.
    \label{corFinitelyManyEVals}
\end{corollary}
\begin{proof}
    Let $\alpha$ have $m$ eigenvalues $\lambda_1, \cdots, \lambda_m$. Then each eigenvalue has a corresponding eigenvector $v_i$, and by lemma~\ref{lemEVecLI}, these form a linearly independent set. By theorem~\ref{thmSteinitzExchange}, a linearly independent set of $V$ can have size at most $n$. Therefore, there can be at most $n$ eigenvectors and, because each eigenvalue has at least one eigenvector, at most $n$ eigenvalues.
\end{proof}
\begin{propositions}{
        Let $V$ be a finite-dimensional vector space over $\F$. Let $\alpha \in \L(V, V)$ with eigenvalues $\lambda_1, \cdots, \lambda_k \in F$.
        \label{propsEigenspaces}
    }
    \item $\spn{\bigcup_{i = 1}^k V_{\lambda_k}} = \bigoplus_{i = 1}^k V_{\lambda_k}$ \label{propSpanESpaces}
    \item $\alpha$ is diagonalisable if and only if $V = \bigoplus_{i = 1}^k V_{\lambda_k}$ \label{propESpaceWholeSpace}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Recall that the map in proposition~\ref{propMapDSToSum} is surjective, so $\spn{\bigcup_{i = 1}^k V_{\lambda_k}} \geq \bigoplus_{i = 1}^k V_{\lambda_k}$. If it is not also injective, there exists a vector in the kernel of this map. That is, there exists a linear combination:
            \begin{equation*}
                \sum_{i=1}^{k} v_j = \zv
            \end{equation*}
            but not all the $v_j$ are zero. This contradicts lemma~\ref{lemEVecLI}.
        \item
            \begin{proofdirection}{$\Rightarrow$}{Suppose that $\alpha$ is diagonalisable}
                By proposition~\ref{propDiagIffBasis}, we have a basis of eigenvectors. Then this means $V$ is a direct sum of the spaces spanned by each of these eigenvectors, which are the $V_{\lambda_i}$.
            \end{proofdirection}
            \begin{proofdirection}{$\Leftarrow$}{Suppose that $V$ is a direct sum}
                Because we are in finite-dimensional space, each $V_{\lambda_i}$ has a basis $B_i$. By Ex1Q8, the union $B = \bigcup_{i = 1}^k B_i$ is a basis for $V$. That is, $\alpha$ has a basis of eigenvectors, so $[\alpha]_B^B$ is diagonal.
            \end{proofdirection}
    \end{enumerate}
\end{proof}
\section{The Characteristic Polynomial}
Recall that for $V$ finite-dimensional, and $\beta \in \L(V, V)$, $\det(\beta) = 0 \iff \ker{\beta} \neq \{\zv_V\}$. Therefore, $\lambda \in \F$ is an eigenvalue of $\alpha \in \L(V, V)$ if and only if $\det(\alpha - \lambda \Id_V) = 0$.
\begin{definition}{Matrix characteristic polynomial}
    For $A \in M_{n \times n}(\F)$ the \underline{characteristic polynomial} of $A$ is:
    \begin{equation*}
        \chi_A(t) = \det(t I_n - A) \in \F[t]
    \end{equation*}
\end{definition}
\begin{definition}{Linear map characteristic polynomial}
    For a finite-dimensional vector space $V$ over a field $\F$, and $\alpha \in \L(V, V)$, the \underline{characteristic polynomial} of $\alpha$ is:
    \begin{equation*}
        \chi_\alpha(t) = \det(t \Id_V - \alpha)
    \end{equation*}
\end{definition}
We make concrete the remark we made earlier:
\begin{lemma}
    Let $V$ be a finite dimensional vector space over a field $\F$ and $\alpha \in \L(V, V)$.
    The roots of $\chi_\alpha$ are exactly the eigenvalues of $\alpha$.
    \label{lemCharPolyIsEvals}
\end{lemma}
\begin{remarks}
    \item $\chi_\alpha$ is monic. This is why we define it the ``other way round'' to in IA Vectors and Matrices, where we had that the leading coefficient was $(-1)^n$.
    \item Similar matrices have the same characteristic polynomial, because determinant is invariant on similarity classes (and $P^{-1} I P = I$).
    \item By the Leibniz formula applied to $tI - A$, we get that:
        \begin{equation*}
            \chi_\alpha(t) = t^n + C_{n-1} t^{n-1} + \cdots + C_1 t + C_0
        \end{equation*}
        These coefficients generally have interesting properties. For example, $C_{n-1} = -\tr(\alpha)$ and $C_0 = (-1)^n \det(A)$.
\end{remarks}
\begin{proposition}
    For $V$ a finite-dimensional vector space and $\alpha \in \L(V, V)$, if $\alpha$ is triangularisable then $\chi_\alpha(t)$ can be written as a product of linear factors.
    \label{propTriangularFactorise}
\end{proposition}
\begin{proof}
    Consider the basis $B$ in which $[\alpha]_B^B$ is triangular. Call this matrix $A$. By proposition~\ref{propTriangularDet}, $\det(t\Id_V - \alpha)$ is the product of the diagonal elements of $tI - A$, and this is precisely:
    \begin{equation*}
        \chi_\alpha(t) = \prod_{i=1}^{n} (t - a_{ii})
    \end{equation*}
\end{proof}
\begin{remark}
    If the characteristic polynomial does not split as linear factors, then the matrix is not triangularisable.
\end{remark}
\begin{example}
    Consider the rotation matrix:
    \begin{equation*}
        A =
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
    \end{equation*}
    We can compute the characteristic polynomial:
    \begin{equation*}
        \chi_A(t) = t^2 - 2\cos(\theta) t + 1
    \end{equation*}
    Then considering the discriminant, $4\cos^2(\theta) - 4$, this is negative for $\theta \in (0, \pi)$. So, in the space of \textit{real matrices}, this is not triangularisable.
\end{example}
\begin{theorem}
    Every $A \in M_{n \times n}(\C)$ is triangularisable.
    \label{thmComplexTriangular}
\end{theorem}
\begin{proof}
    By theorem~\ref{thmFundamentalAlgebra}, $\chi_A(t)$ has a root $\lambda \in C$.

    \induction{$n = 2$}{
        There exists a non-zero vector $v \in V$ such that $\alpha(v) = \lambda v$. Extend to a basis $B = \{v, w\}$ of $V$. With respect to this basis, $\alpha$ is given by:
        \begin{equation*}
            [\alpha]_B^B =
            \begin{pmatrix}
                \lambda & a_{12} \\
                0 & a_{22}
            \end{pmatrix}
        \end{equation*}
        This is upper-triangular.
    }{$n = k$}{
        Assume any matrix in $M_{k \times k}(\C)$ is triangularisable.
    }{$n = k + 1$}{
        Given the eigenvalue $\lambda$ there must exist a non-zero vector $v_1 \in V$ such that $\alpha(v) = \lambda v_1$. Then extend to a basis $B = \{v_1, \cdots, v_n\}$ of $V$. Under this basis, $\alpha$ is given by the block matrix:
        \begin{equation*}
            [\alpha]_B^B = A =
            \begin{pmatrix}
                \lambda & \star \\
                0 & C
            \end{pmatrix}
        \end{equation*}
        Now $C \in M_{k \times k}(\F)$, and so by induction there exists a change-of-basis matrix $\bar{P} \in GL_k(\C)$ such that $\bar{P} C \bar{P}^{-1}$ is upper-triangular. Extend $\bar{P}$ to a matrix $P$ by:
        \begin{equation*}
            P =
            \begin{pmatrix}
                1 & 0 \\
                0 & \bar{P}
            \end{pmatrix}
        \end{equation*}
        Then $PAP^{-1}$ is upper-triangular.
    }
\end{proof}
\begin{remark}
    The same proof can be used for any field $\F$, $A \in M_{n \times n}(\F)$ is triangularisable if and only if it is a product of linear factors.
\end{remark}
\begin{corollary}
    Every linear map in a finite-dimensional vector space over $\C$ is triangularisable.
    \label{corComplexLMTriangular}
\end{corollary}
\section{Annihilator Polynomials}
We define notation for applying a polynomial to a linear map. Let $p(t)$ be a polynomial,
\begin{equation*}
    p(t) = \sum_{i=0}^{n}\mu_i t^i
\end{equation*}
then we can apply this to a linear map $\alpha$, $p(\alpha)$, to get the linear map:
\begin{equation*}
    p(\alpha) = \sum_{i=0}^{n} \mu_i \alpha^i
\end{equation*}
We define a similar notion for a square matrix $A$, where we say $A^0 = I$.
\begin{remark}
    If $B$ is a basis for $V$ then $[p(\alpha)]_B^B = p([\alpha]_B^B)$.
\end{remark}
\begin{definition}{Annihilator polynomial}
    For $V$ a finite-dimensional vector space over $\F$ and $\alpha \in \L(V, V)$, $p \in \F[t]$ is an \underline{annihilator polynomial} if $p(\alpha)$ gives the zero map.
\end{definition}
The set of all such polynomials in $\F[t]$ is called the \underline{annihilator ideal} or just \underline{annihilator} of $\alpha$ and is denoted $I_\alpha$. For more about ideals, see the course IB Groups, Rings and Modules.

This section is now devoted to understanding some important members of this set.
\begin{example}[Examples of annihilator polynomials]
    Clearly the zero polynomial is inside $I_\alpha$.

    We can find a non-zero example by noting that $\dim(\L(V, V)) = n^2$, so the set $\{\Id_V, \alpha, \alpha^2, \cdots, \alpha^{n^2}\}$ is linearly dependent. Therefore, there exist coefficients $\mu_i$ not all zero such that:
    \begin{equation*}
        0 = \sum_{i=0}^{n^2} \mu_i \alpha^i = p(\alpha)
    \end{equation*}
    This is therefore another annihilator polynomial.
\end{example}
\begin{warning}
    This is not a standard definition. The idea of an annihilator used in this context is one from Ring Theory (see the course IB Groups, Rings and Modules), not the concept used in this course (although they are connected). This is my own definition to be able to talk about the set of these polynomials more succinctly.
\end{warning}
\begin{definition}{Minimal polynomial}
    The \underline{minimal polynomial} $m_\alpha$ of a linear map $\alpha \in \L(V, V)$ for a finite-dimensional vector space $V$ is the monic polynomial $p \in \F[t]$ with least degree that satisfies:
    \begin{equation*}
        p(\alpha) = 0
    \end{equation*}
\end{definition}
Then we want to show nice properties about this such as uniqueness, and we want to understand its roots.
\begin{lemma}
    For any $f \in I_\alpha$, $m_\alpha~\vline~f$.
    \label{lemMinPolyDivides}
\end{lemma}
\begin{proof}
    By proposition~\ref{propEuclidAlgPoly}, there exist $r, q \in \F[t]$ such that $\deg(r) < \deg(m_\alpha)$ and $f = q m_\alpha + r$. Then rearrange:
    \begin{equation*}
        r(\alpha) = f(\alpha) - q(\alpha) m_\alpha(\alpha) = 0
    \end{equation*}
    Therefore $r \in I_\alpha$ but by minimality of $\deg(m_\alpha)$, $r = 0$.
\end{proof}
\begin{theorem}
    The minimal polynomial, $m_\alpha$, is unique.
    \label{thmMinimalPolynomialUnique}
\end{theorem}
\begin{proof}
    Let $\overline{m_\alpha}$ be another non-zero element of minimal degree in $I_\alpha$, and let it be monic (by perhaps dividing by the leading coefficient). By lemma~\ref{lemMinPolyDivides} we have that $m_\alpha~\vline~\overline{m_\alpha}$, but since they have the same degree we must have that one is a multiple of the other. Because both are monic, in fact they must be equal.
\end{proof}
\begin{remark}
    Because $m_\alpha$ and $\overline{m_\alpha}$ were monic, this theorem tells us that $m_\alpha$ is unique in $I_\alpha$ up to multiplication by a constant.
\end{remark}
\begin{theorem}[Cayley-Hamilton Theorem]
    Let $V$ be a finite-dimensional vector space and let $\alpha \in \L(V, V)$. Let $\chi_\alpha$ be the characteristic polynomial for $\alpha$.
    \begin{equation*}
        \chi_\alpha(\alpha) = 0
    \end{equation*}
    \label{thmCayleyHamilton}
\end{theorem}
\begin{remark}
    Equivalently, $\chi_\alpha \in I_\alpha$.
\end{remark}
\begin{proof}[for $\F = \C$]
    By theorem~\ref{thmComplexTriangular}, there exists a basis $B = \{v_1, \cdots, v_n\}$ for $V$ such that:
    \begin{equation*}
        [\alpha]_B^B =
        \begin{pmatrix}
            \lambda_1 &  & \star \\
             & \ddots &  \\
            0 &  & \lambda_n
        \end{pmatrix}
    \end{equation*}
    Then the characteristic polynomial is a product of linear factors $\chi_\alpha(t) = \prod_{i=1}^{n} (t - \lambda_i)$.

    Define $U_j = \spn{v_1, \cdots, v_j}$ so that $U_0 = \{\zv\}$ and $U_n = V$. Then for $1 \leq j \leq n$,
    \begin{equation*}
        (\alpha - \lambda_j \Id_V)(U_j) \leq U_{j-1}
    \end{equation*}
    because $[\alpha]_B^B$ is upper triangular, and considering the matrix of the restricted map $(\alpha - \lambda_j \Id_V)\vline_{U_j}$, we see that the last row is all zero so its image is $U_{j-1}$.

    Therefore:
    \begin{align*}
        \chi_\alpha(\alpha) (V)&= (\alpha - \lambda_1 \Id_V) \circ \cdots \circ (\alpha - \lambda_n \Id_V)U_n \\
        &= U_0 = \{\zv\} \text{ by telescoping product.}
    \end{align*}
    Therefore, $\chi_\alpha(\alpha)$ must be the zero map, as required.
\end{proof}
\begin{remark}
    The only time we needed $\C$ was for triangularisability, so we have in fact proved that every triangularisable linear map satisfies its own characteristic polynomial.
\end{remark}
\begin{corollary}
    For $V$ finite-dimensional, $\alpha \in \L(V, V)$, $m_\alpha~\vline~\chi_\alpha$.
    \label{corMinPolyDividesCharPoly}
\end{corollary}
\begin{proof}
    This is a direct consequence of lemma~\ref{lemMinPolyDivides}, because we now have $\chi_\alpha \in I_\alpha$ by theorem~\ref{thmCayleyHamilton}.
\end{proof}
\begin{definition}{Algebraic multiplicity}
    Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. The  \underline{algebraic multiplicity} of an eigenvalue $\lambda \in \F$, $a_\lambda$, is the multiplicity of $\lambda$ as a root of $\chi_\alpha(t)$. That is, the largest power of $(t - \lambda)$ that divides $\chi_\alpha$.
\end{definition}
\begin{definition}{Geometric multiplicity}
    Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. The \underline{geometric multiplicity} of $\lambda$, $g_{\lambda}$, is the dimension of its eigenspace.
\end{definition}
\begin{proposition}[The other AM-GM inequality]
    For $V$ a finite-dimensional vector space over $\F$, $\alpha \in \L(V, V)$ and $\lambda \in \F$ an eigenvalue of $\alpha$,
    \begin{equation}
        g_\lambda \leq a_\lambda.
        \label{eqnAMGM}
    \end{equation}
    \label{propAMGM}
\end{proposition}
\begin{proof}
    let $\{v_1, \cdots, v_k\}$ be a basis for $V_\lambda$. Extend to a basis $B = \{v_1, \cdots, v_n\}$ of $V$. Then:
    \begin{equation*}
        [\alpha]_B^B =
        \begin{pmatrix}
            \lambda I_k & \star \\
            0 & C
        \end{pmatrix},~~C \in M_{(n-k) \times (n-k)}(\F).
    \end{equation*}
    Then we have that $\chi_\alpha(t) = (t - \lambda)^k \chi_C(t)$ where $\chi_C(t)$ may contain another factor of $(t - \lambda)^k$ so we have that $a_\lambda \geq k = g_\lambda$.
\end{proof}
\begin{propositions}{
        Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. Let the eigenvalues of $\alpha$ be $\lambda_1, \cdots, \lambda_k \in \F$.
        \label{propsAMGMIneqs}
    }
    \item $\sum_{i=1}^{k} g_{\lambda_i} \leq \dim(V)$ with equality if and only if $\alpha$ is diagonalisable. \label{propCompleteIffDiag}
    \item $\sum_{i = 1}^k a_{\lambda_i} \leq \dim(V)$ with equality if and only if $\alpha$ is triangularisable.
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item We have this result by applying propositions~\ref{propsEigenspaces}.
        \item We write:
            \begin{equation*}
                \chi_\alpha(t) = f(t) \prod_{i=1}^{k}(t - \lambda_i)^{a_{\lambda_i}}.
            \end{equation*}
            Now note that if $f(t)$ has a linear factor, then this is an eigenvalue of $\alpha$ which is not already counted which is a contradiction, so it must not have any linear factors. Moreover,
            \begin{align*}
                \dim(V) &= \deg(\chi_\alpha) \\
                &= \deg(f) + \sum_{i=1}^{k} a_{\lambda_i} \\
                &\leq \sum_{i=1}^{k} a_{\lambda_i}
            \end{align*}
            With equality if and only if in fact $f$ is constant, in which case $\chi_\alpha$ is a product of linear factors. By proposition~\ref{propTriangularFactorise} this is equivalent to triangularisability.
    \end{enumerate}
\end{proof}
\begin{proposition}
    Let $V$ be a finite-dimensional vector field over $\C$. A linear map $\alpha \in \L(V, V)$ is diagonalisable if and only if $a_\lambda = g_\lambda$ for every eigenvalue $\lambda$ of $\alpha$.
    \label{propDiagIfNotDegenerate}
\end{proposition}
\begin{proof}
    By theorem~\ref{thmComplexTriangular}, $\alpha$ is triangularisable and so $\chi_\alpha$ is a product of factors. By proposition~\ref{propAMGM},
    \begin{equation*}
        \sum_{i=1}^{k}g_{\lambda_i} \leq \sum_{i=1}^{k} a_{\lambda_i} = \dim(V)
    \end{equation*}
    So the only way we can get equality (note that diagonalisability is equivalent to the sum of geometric multiplicities being $\dim(V)$) is if in fact $\alpha_\lambda = g_\lambda$ for all eigenvalues $\lambda$.
\end{proof}
\begin{remark}
    For $p(t) = \sum_{i=1}^{n}\mu_i t^i$, $v \in V_\lambda$,
    \begin{equation*}
        (p(\alpha))(v) = \sum_{i=1}^n \mu_i \alpha^i(v) = \sum_{i=1}^{n} \mu_i \lambda^i v = p(\lambda) v
    \end{equation*}
\end{remark}
\begin{lemma}
    For $\lambda \in \F$,
    \begin{equation*}
        \lambda \text{ is a root of } m_\alpha \iff \lambda \text{ is a root of } \chi_\alpha
    \end{equation*}
    \label{lemMinPolyCharPolySameRoots}
\end{lemma}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let $\lambda$ be a root of $m_\alpha$}
        We have that $(t - \lambda)$ divides $m_\alpha$. By corollary~\ref{corMinPolyDividesCharPoly}, we must also have that $(t - \lambda)$ divides $\chi_\alpha$, or equivalently that $\lambda$ is a root of $\chi_\alpha$.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Let $\lambda$ be a root of $\chi_{\alpha}$}
        We have that $\lambda$ is an eigenvalue so $\alpha(v) = \lambda v$ for a non-zero eigenvector $v$.
        \begin{align*}
            (m_\alpha(\alpha))(v)&= m_\alpha(\lambda) v \text{ by the above remark}
        \end{align*}
        But since $m_\alpha \in I_\alpha$, the LHS of this equation is zero. Since $v \neq \zv$, we must instead have that $m_{\alpha}(\lambda) = 0$.
    \end{proofdirection}
\end{proof}
\begin{definition}{Minimal multiplicity}
    For $V$ a finite-dimensional vector space over $\F$ and $\alpha \in \L(V, V)$, the \underline{minimal multiplicity} of an eigenvalue $\lambda$, $c_\lambda$, is the multiplicity of $\lambda$ as a root of the minimal polynomial.
\end{definition}
\begin{remark}
    By theorem~\ref{thmCayleyHamilton}, $1 \leq c_\lambda \leq a_\lambda$.
\end{remark}
\begin{examples}{
        
    }
    \item Let $A = \lambda I_n$. Then $\chi_A(t) = (t - \lambda)^n$, and $V_\lambda = \F^n$. However, the minimal polynomial is $m_\alpha(t) = t - \lambda$, so the multiplicities are $a_\lambda = n, g_\lambda = n, c_\lambda = 1$.
    \item Let $A$ be the matrix:
        \begin{equation*}
            \begin{pmatrix}
                \lambda & 1 & 0 & \cdots & 0 \\
                0 & \lambda & 1 & \cdots & 0 \\
                0 & 0 & \ddots & \ddots & \vdots \\
                \vdots & \vdots & & \lambda & 1 \\
                0 & 0 & \cdots & 0 & \lambda
            \end{pmatrix}
        \end{equation*}
        Then $\chi_A(t) = (t - \lambda)^n$ because $A$ is upper-triangular, but if we consider the eigenspace of this eigenvalue we get only:
        \begin{equation*}
            V_\lambda = \spn{\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix}}
        \end{equation*}
        We know that the minimal polynomial must have the form $m_A(A) = (A - \lambda I)^k$ for some $1 \leq k \leq n$. In order to find $k$, consider:
        \begin{equation*}
            A - \lambda I =
            \begin{pmatrix}
                0 & 1 & 0 & \cdots & 0 \\
                0 & 0 & 1 & & \vdots \\
                \vdots & \vdots & & \ddots & 0 \\
                0 & 0 & 0 & 0 & 1 \\
                0 & 0 & 0 & 0 & 0
            \end{pmatrix}
        \end{equation*}
        By multiplying this matrix out, we find that each power shifts the diagonal of 1s up each time, so the smallest $k$ for which this is zero is in fact $n$. That is, $m_A = \chi_A$.

        This gives us that $a_\lambda = n, g_\lambda = 1, c_\lambda = n$.
        \label{expJordanPolys}
\end{examples}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha \in \L(V, V)$. If there exists a polynomial $p \in I_\alpha$ which is a product of distinct linear factors, then $\alpha$ is diagonalisable.
    \label{thmProdDistinctFactorsDiag}
\end{theorem}
\begin{proof}
    We can assume that $p$ is monic by rescaling. Write $p$ in the form:
    \begin{equation*}
        p(t) = \prod_{i=1}^{k} (t - \mu_i) \text{ for distinct $\mu_i$.}
    \end{equation*}
    Define also $p_i(t) = \prod_{\substack{j = 1\\j \neq i}}^{k} (t - \mu_j)$, the polynomial excluding the root $\mu_i$. Then by corollary~\ref{corBezoutPolynom}, there exist $q_i \in \F[t]$ such that:
    \begin{equation*}
        1 = \sum_{i=1}^{k} p_i(t) q_i(t)
    \end{equation*}
    Where here $1$ represents the constant polynomial. Setting $\pi_i = p_i(\alpha) q_i(\alpha) \in \L(V, V)$, for $v \in V$,
    \begin{equation*}
        v = \Id_V(v) = \sum_{i=1}^{k} \pi_i(v)
    \end{equation*}
    Hence $V = \im(\pi_1) + \cdots, + \im(\pi_k)$. Further:
    \begin{align*}
        &p(t) = (t - \mu_i)p_i(t) \\
        &\therefore p(\alpha) = (\alpha - \mu_i \Id_V)(p_i(\alpha)) = 0 \\
        &\therefore (\alpha - \mu_i\Id_V) \circ \pi_i = 0
    \end{align*}
    And so $\im(\pi_i) \leq V_{\mu_i}$. Hence, $V = \sum_{i=1}^{k} \im(\pi_i) \leq \bigoplus_{i=1}^k V_{\mu_i} \leq V$.

    Therefore. $V$ is a direct sum of eigenspaces, so $\alpha$ is diagonalisable.
\end{proof}
We provide a theorem to summarise our characterisation of diagonalisability:
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha \in \L(V, V)$. The following are equivalent:
    \begin{enumerate}
        \item $\alpha$ is diagonalisable.
        \item $V$ has a basis consisting of eigenvectors of $\alpha$
        \item There exists a non-zero polynomial $p \in I_\alpha$ that is a product of distinct linear factors.
        \item $m_\alpha$ is a product of distinct linear factors.
        \item {[Only in the case $\F = \C$]} $a_\lambda = g_\lambda$ for all eigenvalues $\lambda$.
    \end{enumerate}
    \label{thmDiagConditions}
\end{theorem}
\begin{proof}
    Statement 1 is equivalent to statement 2 by proposition~\ref{propDiagIffBasis}, and statement 1 is equivalent to statement 5 (in the case of the complex field) by proposition~\ref{propDiagIfNotDegenerate}.

    \begin{subproof}{Statement 3 is equivalent to statement 4}
        Because $m_\alpha$ divides any polynomial in $I_\alpha$, if $p \in I_\alpha$ is a product of distinct linear factors then so is $m_\alpha$. If instead $m_\alpha$ is a product of distinct linear factors then take $p = m_\alpha$ in theorem~\ref{thmProdDistinctFactorsDiag}.
    \end{subproof}
    Statement 3 implies statement 1 by theorem~\ref{thmProdDistinctFactorsDiag} directly.
    \begin{subproof}{Statement 1 implies statement 3}
        Let $B$ be a basis for $V$ such that $[\alpha]_B^B$ is diagonal. After possibly rearranging the basis, ensure that vectors in the same eigenspace are next to each other. In this basis,
        \begin{equation*}
            [\alpha]_B^B =
            \begin{pmatrix}
                \lambda_1 I_{n_1} & 0 & 0 & \cdots & 0 \\
                0 & \lambda_2 I_{n_2} & 0 & \cdots & 0 \\
                0 & 0 & \lambda_3 I_{n_3} & & \vdots \\
                \vdots & \vdots & & \ddots & 0 \\
                0 & 0 & \cdots & 0 & \lambda_k I_{n_k}
            \end{pmatrix}
            \text{ where } n_k = g_{\lambda_k}
        \end{equation*}
        Then set $p(t) = \prod_{i=1}^{k} (t - \lambda_i)$. By considering the matrix $p([\alpha]_B^B)$, we see that each factor of $p$ deletes one of the blocks above. Since all blocks get deleted in one of the factors of $p$, and we are multiplying together diagonal matrices, we must have that indeed $p(\alpha) = 0$.
    \end{subproof}
\end{proof}
\begin{definition}{Simultaneous diagonalisability}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha, \beta \in \L(V, V)$. Then $\alpha$ and $\beta$ are \underline{simultaneously diagonalisable} if there exists a basis for $V$ such that in this basis both $[\alpha]_B^B$ and $[\beta]_B^B$ are diagonalisable.
\end{definition}
Before we prove a theorem about simultaneous diagonalisability, we need the following lemma:
\begin{lemma}
    Let $\alpha, \beta$ commute. Then for an eigenspace $V_\lambda$ of $\alpha$, $\beta(V_\lambda) \leq V_\lambda$.
    \label{lemSimDiagSameEvecs}
\end{lemma}
\begin{proof}
    Let $v \in V_\lambda$.
    \begin{align*}
        \alpha(\beta(v_\lambda))&= \beta(\alpha(v_\lambda)) \\
        &= \beta(\lambda v_\lambda) \\
        &= \lambda \beta(v)
    \end{align*}
    and hence $\beta(v) \in V_\lambda$.
\end{proof}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha, \beta \in \L(V, V)$.

    $\alpha$ and $\beta$ are simultaneously diagonalisable if and only if they commute.
    \label{thmSimDiag}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let $\alpha$ and $\beta$ be simultaneously diagonalisable}
        Choose the basis in which they are both diagonal. Then since diagonal matrices commute,
        \begin{align*}
            [\alpha \circ \beta]_B^B&= [\alpha]_B^B [\beta]_B^B \\
            &= [\beta]_B^B [\alpha]_B^B \\
            &= [\beta \circ \alpha]_B^B
        \end{align*}
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $\alpha$ and $\beta$ commute.}
        $\alpha$ is diagonalisable and so take $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$ where the eigenvalues are distinct.
        Let $p \in I_\beta$ be a product of distinct linear factors. By lemma~\ref{lemSimDiagSameEvecs}, we can define the restriction $\beta |_{V_{\lambda_i}}$. Note also that $p(\beta |_{V_{\lambda_i}}) = p(\beta)|_{V_{\lambda_i}} = 0$, and so this restriction is diagonalisable. Let the basis in which it is diagonalisable be $B_i$, consisting of eigenvectors of $\beta$.

        The basis $B = \bigcup_{i=1}^k B_i$ is a basis for $V$, consisting of eigenvectors of both $\alpha$ and $\beta$. Therefore, we have constructed a basis in which $\alpha$ and $\beta$ are both diagonalisable.
    \end{proofdirection}
\end{proof}
\section{Jordan Normal Form}
In this section set $\F = \C$.
\begin{definition}{Jordan Matrix}
    For $\lambda \in \C$, the $(n \times n)$-\underline{Jordan matrix} is:
    \begin{equation*}
        J_n(\lambda) =
        \begin{pmatrix}
            \lambda & 1 & 0 & \cdots & 0 \\
            0 & \lambda & 1 & & \vdots \\
            \vdots & \vdots & \ddots & \ddots & 0 \\
            0 & 0 & 0 & \lambda & 1 \\
            0 & 0 & 0 & 0 & \lambda
        \end{pmatrix}
    \end{equation*}
\end{definition}
\begin{remark}
    By example~\ref{expJordanPolys}, this has characteristic polynomial:
    \begin{equation*}
        \chi_{J_n(\lambda)} = (t - \lambda)^n = m_{J_n(\lambda)}
    \end{equation*}
    and $\lambda$-eigenspace $\spn{\vec{e_1}}$.
    \label{remJordanBlockMults}
\end{remark}
\begin{definition}{Jordan Normal Form}
    A matrix $A \in M_{n \times n}(\C)$ is in \underline{Jordan normal form} if:
    \begin{equation*}
        A =
        \begin{pmatrix}
            J_{n_1}(\lambda_1) & 0 & 0 & \cdots & 0 \\
            0 & J_{n_2}(\lambda_2) & 0 & \cdots & 0 \\
            0 & 0 & J_{n_3}(\lambda_3) & & \vdots \\
            \vdots & \vdots & & \ddots & 0 \\
            0 & 0 & \cdots & 0 & J_{n_k}(\lambda_k)
        \end{pmatrix}
    \end{equation*}
    for some $n_i \in \N$ summing to $n$, and some $\lambda_i \in \C$ (not necessarily distinct).

    The $J_{n_i}(\lambda_i)$ are the \underline{Jordan blocks} of $A$. A Jordan block of size $1$ is just $\left(\begin{smallmatrix}\lambda\end{smallmatrix}\right)$.
\end{definition}
\begin{propositions}{
        Let $A$ be a matrix in $M_{n \times n}(\C)$ in Jordan Normal Form.
        \label{propsJNFMults}
    }
    \item The algebraic multiplicity of an eigenvalue $\lambda$ in $A$ is the sum of the sizes of each $\lambda$-Jordan block in $A$. \label{propJNFAMult}
    \item The geometric multiplicity of an eigenvalue $\lambda$ in $A$ is the number of $\lambda$-Jordan blocks in $A$. \label{propJNFGMult}
    \item The minimal multiplicity of an eigenvalue $\lambda$ is the size of the largest $\lambda$-Jordan block in $A$. \label{propGNFCMult}
\end{propositions}
\begin{proof}
    Let $B$ be a Jordan block,
    \begin{equation*}
        J_n(\lambda) =
        \begin{pmatrix}
            \lambda & 1 & 0 & \cdots & 0 \\
            0 & \lambda & 1 & & \vdots \\
            \vdots & \vdots & \ddots & \ddots & 0 \\
            0 & 0 & 0 & \lambda & 1 \\
            0 & 0 & 0 & 0 & \lambda
        \end{pmatrix}
    \end{equation*}
    then by remark~\ref{remJordanBlockMults}, $a_\lambda = n$, $g_{\lambda} = 1$, $c_\lambda = n$. Using this,
    \begin{align*}
        \chi_A(t) &= \prod_{i=1}^{k} \chi_{A_i}(t), \text{ so }a_{\lambda} = \sum_{i=1}^{k}a_\lambda(A_i) \\
        g_\lambda(A) &= \sum_{i=1}^{k} g_{\lambda}(A_i) \\
        m_A(t) &= \operatorname{lcm}(M_{A_1}(t), \cdots, M_{A_k}(t)), \text{ so }c_\lambda(A) = \max_{i} (c_\lambda(A_i))
    \end{align*}
\end{proof}
\begin{theorem}
    Every matrix $A \in M_{n \times n}(\C)$ is similar to a matrix $\operatorname{JNF}(A)$ in Jordan Normal Form, unique up to rearranging the Jordan blocks.
    \label{thmJNF}
\end{theorem}
\begin{proof}[Of uniqueness only]
    Assume existence of JNF, so define $\bar{A} = \operatorname{JNF}(A)$. For these matrices, define:
    \begin{equation*}
        R_{\lambda, r}(\bar{A}) = \text{ number of $\lambda$ Jordan blocks in $\bar{A}$ of size $\geq r$.}
    \end{equation*}
    Then the $R_{\lambda, r}$ uniquely determine $\overline{A}$ up to rearranging the Jordan blocks.
    \begin{subproof}{$R_{\lambda, r}(A) = rk((A - \lambda I)^{r-1}) - rk((A - \lambda I)^r)$}
        For $P \in GL_n(\C)$,
        \begin{align*}
            P(A - \lambda I)^k P^{-1} &= (P(A - \lambda I)P^{-1})^k \\
            &= (PAP^{-1} - \lambda I)^k
        \end{align*}
        Thus, for similar matrices with change-of-basis matrix $P$,
        \begin{equation*}
            rk((PAP^{-1} - \lambda I)^k) = rk(A - \lambda I)^k
        \end{equation*}
        Now consider $PAP^{-1} = \bar{A}$ in JNF.
        Recall that:
        \begin{equation*}
            J_l(\lambda) - \lambda I = 
            \begin{pmatrix}
                0 & 1 & 0 & \cdots & 0 \\
                0 & 0 & 1 & & \vdots \\
                \vdots & \vdots & & \ddots & 0 \\
                0 & 0 & 0 & 0 & 1 \\
                0 & 0 & 0 & 0 & 0
            \end{pmatrix}
        \end{equation*}
        And increasing the power of this moves the diagonal of $1$s up, reducing the rank by 1 each time. Therefore:
        \begin{equation*}
            rk((J_l(\lambda) - \lambda I)^k) =
            \begin{cases}
                l - k & k \leq l \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation*}
        For $\mu \neq \lambda$, $J_l(\mu) - \lambda I = J_l(\mu - \lambda)$ which is invertible. Then:
        \begin{align*}
            rk((\bar{A} &- \lambda I)^k) \\
            &= n - k \times (\text{ number of Jordan blocks of $\bar{A}$ of size $\geq k$}) \\
                &\quad -(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< k$})
        \end{align*}
        And so taking the difference of two adjacent values for $k$, $r$ and $r-1$, in this formula:
        \begin{align*}
            & - n + r \times (\text{ number of Jordan blocks of $\bar{A}$ of size $\geq r$}) \\
                &\quad +(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r$}) \\
                &\quad + n - (r-1) \times (\text{ number of Jordan blocks of $\bar{A}$ of size $\geq r-1$}) \\
                &\quad -(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r-1$}) \\
            & 1 \times (\text{ number of Jordan blocks of $\bar{A}$ of size $\geq r$}) \\
                &\quad +(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r$}) \\
                &\quad - (r-1) \times (\text{ number of Jordan blocks of $\bar{A}$ of size $r-1$}) \\
                &\quad -(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r-1$}) \\
            & 1 \times (\text{ number of Jordan blocks of $\bar{A}$ of size $\geq r$}) \\
                &\quad +(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r$}) \\
                &\quad -(\text{ sum of sizes of Jordan blocks of $\bar{A}$ of size $< r$}) \\
            &= \text{number of Jordan blocks of $\bar{A}$ of size $\geq r$} \\
            &= R_{\lambda, r}
        \end{align*}
    \end{subproof}
    Now we can easily reach a contradiction:
    Suppose $A$ is similar to two matrices $B$ and $C$ in JNF. Then by the claim they will have the same values for $R_{\lambda, r}$ for each value of $r$ and each eigenvalue $\lambda$ of $A$. Therefore, $B$ and $C$ must be same matrix in JNF, up to rearrangement of the Jordan blocks.
\end{proof}
\begin{example}[Jordan Normal Form for $2 \times 2$ matrices]
    Let $\lambda_1, \lambda_2$ be distinct. The possible JNFs for $A \in M_{2 \times 2}(\C)$ are:
    \begin{align*}
    A &= &\begin{pmatrix}\lambda_1 & 1 \\ 0 & \lambda_1\end{pmatrix} && \begin{pmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{pmatrix} && \begin{pmatrix}\lambda_1 & 0 \\ -0 & \lambda_2\end{pmatrix} \\
    m_A(t) &= & (t - \lambda_1)^2 && (t - \lambda_1)(t - \lambda_2) && (t - \lambda_1)
    \end{align*}
\end{example}
\begin{example}[Jordan Normal Form for $3 \times 3$ matrices]
    Let $\lambda_1, \lambda_2, \lambda_3$ be distinct. The possible JNFs for $A \in M_{3 \times 3}(\C)$ are:
    \begin{align*}
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_2 & 0 \\
        0 & 0 & \lambda_3
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_2
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    \\
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_2
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 1 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    \end{align*}
    \label{expJNF3}
\end{example}
\begin{remark}
    For $n \leq 3$, we find that all the JNFs have different minimal/characteristic polynomial combinations. This is not the case for larger $n$, we already have a counterexample for $n = 4$.
\end{remark}
\begin{example}
    Consider the following matrix:
    \begin{equation*}
        A =
        \begin{pmatrix}
            3 & -2 & 0 \\
            1 & 0 & 0 \\
            1 & 0 & 1
        \end{pmatrix}
    \end{equation*}
    Then we want $P \in GL_3(\C)$ such that $PAP^{-1}$ is in JNF. We calculate:
    \begin{equation*}
        \chi_A(t) = (t-1)^2 (t-2)
    \end{equation*}
    Example~\ref{expJNF3} that the only possible JNFs are:
    \begin{equation*}
        \begin{pmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 2
        \end{pmatrix}
        \text{ or }
        \begin{pmatrix}
            1 & 1 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 2
        \end{pmatrix}
    \end{equation*}
    The difference between these are the geometric multiplicities of the eigenvalue 1. Considering $A - I$:
    \begin{equation*}
        \begin{pmatrix}
            2 & -2 & 0 \\
            1 & -1 & 0 \\
            1 & 0 & 0
        \end{pmatrix}
    \end{equation*}
    This has rank 2, so the geometric multiplicity of the eigenvalue $1$ is $1$, and we are in the second case. To find the required basis, set $\vec{v_1} = \begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix}$ and $\vec{v_3} = \begin{pmatrix}2 \\ 1 \\ 2\end{pmatrix}$ because it is an eigenvector with eigenvalue $2$. Finally, we need to solve $A \vec{v_2} = \vec{v_1} + \vec{v_2}$. This is not uniquely determined, but we find that:
    \begin{equation*}
        \vec{v_2} = \begin{pmatrix}1 \\ 1 \\ 0\end{pmatrix}
    \end{equation*}
    solves the equation.

    Our basis is:
    \begin{equation*}
        \left\{\begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix}, \begin{pmatrix}1 \\ 1 \\ 0\end{pmatrix}, \begin{pmatrix}2 \\ 1 \\ 2\end{pmatrix}\right\}
    \end{equation*}
\end{example}
\begin{definition}{Generalised eigenspace}
    Let $V$ be a finite-dimensional vector space over $\C$, and let $\alpha \in \L(V, V)$. Write:
    \begin{equation*}
        m_\alpha(t) = (t - \lambda_1)^{c_1} \cdots (t - \lambda_k)^{c_k}
    \end{equation*}
    Then the \underline{generalised $\lambda_i$-eigenspace} of $\alpha$ is:
    \begin{equation*}
        \gespace{V}{\lambda_i} = \ker\left(\left(\alpha - \lambda_i \Id_V\right)^{c_i}\right)\leq V
    \end{equation*}
\end{definition}
\begin{theorem}[Generalised Eigenspace Decomposition Theorem]
    $V$ is a direct sum of its generalised eigenspaces:
    \begin{equation*}
        V = \bigoplus_{i=1}^k \gespace{V}{\lambda_i}
    \end{equation*}
    \label{thmGenESpaceDecompose}
\end{theorem}
\begin{proof}
    The proof is on the example sheet. Note the following lemma:
    Non-zero elements of distinct generalised eigenspaces are linearly independent. We can prove this:

    Let $0 \leq d_i < c_i$ be such that:
    \begin{align*}
        \zv \neq w_i &= (\alpha - \lambda_i \Id_V)^{d_i} (v_i) \\
        \text{but } &(\alpha - \lambda_i \Id_V)^{d_i + 1}(v_i) = \zv
    \end{align*}
    Then $w_i$ is a $\lambda_i$ eigenvector of $\alpha$ so the $w_i$ with different eigenvalues are linearly independent.

    We can also define:
    \begin{equation*}
        \beta = \prod_{j=1}^{k} (\alpha - \lambda_j \Id_V)^{d_j}
    \end{equation*}
    Then because these factors commute:
    \begin{align*}
        \beta(v_i) &= \left(\prod_{\substack{j=1 \\ j \neq i}}^k (\alpha - \lambda_j \Id_v)^{d_j}\right)w_i \\
        &= \left(\prod_{j \neq i} (\lambda_i - \lambda_j)^{d_j}\right)w_i \\
        &= \mu_i w_i \neq \zv
    \end{align*}
    Then suppose that a linear combination of the $v_i$ sum to zero:
    \begin{align*}
        \zv &= \sum_{i=1}^{k}\nu_i v_i \\
        &= \beta\left(\sum_{i=1}^{k} \nu_i v_i\right) \\
        &= \sum_i \mu_i \nu_i w_i \\
        &\implies \nu_i = 0~\forall i
    \end{align*}
\end{proof}
We can now provide a \textbf{non-examinable} sketch proof of the existence of JNF.
\begin{proof}
    Note that $A$ must commute with $A - \lambda_i I$ so $A$ preserves the generalised eigenspace $\gespace{V}{\lambda_i}$. Therefore, $A$ is similar to:
    \begin{equation*}
        \begin{pmatrix}
            A_1 &  & 0 \\
             & \ddots &  \\
            0 &  & A_k
        \end{pmatrix}
        \text{ where } m_{A_i}(t) = (t - \lambda_i)^{c_i}
    \end{equation*}
    by theorem~\ref{thmGenESpaceDecompose}.

    Now we can restrict to this block-diagonal form, with the $A_i$ having only one eigenvalue. Therefore, we want to prove that these $A_i$ are in JNF, and this is enough to show that in fact $A$ can be put in JNF. For ease of notation, let $A$ have only one eigenvalue (let $A$ be one of these blocks $A_i$).

    Further, we can re-label $A - \lambda_i I$ to $A$, to consider only the case where the one eigenvalue is zero.

    Therefore, because now $A$ has a zero eigenvalue, we can assume that there exists $m$ such that $A^m = 0$. Such a matrix is called \underline{nilpotent}.

    Now consider a matrix $B$ in JNF with all zero eigenvalues. We have that the diagonal one above the main diagonal will have either 1s or zeroes, represented by $\mu_i \in \{0,1\}$:
    \begin{equation*}
        B =
        \begin{pmatrix}
            0 & \mu_1 & 0 & \cdots & 0 \\
            0 & 0 & \mu_2 & & \vdots \\
            \vdots & \vdots & & \ddots & 0 \\
            0 & 0 & 0 & 0 & \mu_{n-1} \\
            0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \end{equation*}
    Note in particular what happens to the basis vectors under this matrix. For a basis vector $\vec{e_i}$, these will either map to the zero vector $\zv$ or to the previous basis vector $\vec{e_{i-1}}$. We claim that we can find a basis such that $\alpha$, the linear map represented in the standard basis by $A$, also has this property and is therefore in JNF.
    \begin{subproof}{There exists a basis $B$ for $\alpha$ such that $\alpha(b_i) \in \{b_{i-1}, \zv\}$}
        \induction{$\dim(V) = 1$}{
            In this case we must have $A = \left( 0 \right)$, which sends the basis vector $1$ to $0$.
        }{$\dim(V) < k$}{
            Assume that there exists a basis for $\alpha$ such that $\alpha(b_i) \in \{b_{i-1}, \zv\}$.
        }{$\dim(V) = k$}{
            Let $W$ be $\im(\alpha)$. We know that $\dim(W) < \dim(V)$ because $A$ is nilpotent. We know also that $\alpha(W) \leq W$, so we can define the restriction:
            \begin{equation*}
                \alpha |_W : W \mapsto W
            \end{equation*}
            which is also nilpotent. By induction, we have a basis:
            \begin{align*}
                B' = \{&\alpha^{l_1}(w_1), \alpha^{l_1 - 1}(w_1), \cdots, \alpha(w_1), w_1 \\
                &\alpha^{l_2}(w_2), \alpha^{l_2 - 1}(w_2), \cdots, \alpha(w_2), w_2 \\
                &\vdots \\
                &\alpha^{l_k}(w_k), \alpha^{l_k - 1}(w_k), \cdots, \alpha(w_k), w_k\}
            \end{align*}
            and this is made up of $k$ chains of basis vectors all mapping to the previous basis vector (or zero in the case of $\alpha^{l_i}(w_i)$).
            Now we want to extend this to a basis for $V$, but we must be careful about how we do this. We write $w_i = \alpha(v_i)$ for some $v_i \in V$, because $W = \im(\alpha)$. However, we also need the basis vectors that correspond to Jordan blocks of size 1, so extend the kernel of $\alpha|_W$ to a basis of $\kera$.

            We can show that this is what we want, although the proof is non-examinable so it's not worth doing.
        }
    \end{subproof}
    Now we have bases for the individual generalised eigenspaces of $A$, so putting these together we get a basis for $A$ such that it is in JNF.
\end{proof}
\end{document}