\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Polynomials}
\begin{definition}{Polynomial}
    A \underline{polynomial} $f$ over a field $\F$ is a formal expression:
    \begin{equation*}
        f(t) = \sum_{i=0}^{n} a_i t^i
    \end{equation*}
    Here $n \in \N \cup \{0\}$ and $a_i \in \F$ are the \underline{coefficients}.
\end{definition}
Define $\F[t]$ to be the vector space over $\F$ of all polynomials. We have the basis $\{1 = t^0, t^1, t^2, \cdots\}$.

\begin{definition}{Degree}
    The \underline{degree} $\deg{f}$ of $f \in \F[t]$ is the largest $i$ such that $a_i \neq 0$. We have the convention that if $f$ is the \underline{zero polynomial}, $f(t) = 0$, $\deg(f) = -\infty$.
    We call this $a_i$ the \underline{leading coefficient}
\end{definition}
\begin{definition}{Monic polynomial}
    A polynomial $f \in \F[t]$ is \underline{monic} if its leading coefficient is $1$.
\end{definition}
We define addition and multiplication for any polynomials $f, g \in \F[t]$
\begin{align*}
    (f + g)(t) &= \sum_{i=0}^{\max\{\deg(f), \deg(g)\}} (a_i + b_i)t^i \\
    (fg)(t) &= \sum_{i=0}^{\deg(f) + \deg(g)} \left(\sum_{k=0}^{i} a_kb_{i-k}\right)t^i
\end{align*}
Note that the degree of $f + g$ is at most $\max\{\deg(f), \deg(g)\}$ (because the leading coefficient can cancel), but the degree of $fg$ is exactly $\deg(f) + \deg(g)$.

Write $f | g$ if there exists $h \in \F[t]$ satisfying $g = fh$.

For $\lambda \in \F$ we can \underline{evaluate} $f$ at $\lambda$ by:
\begin{equation*}
    f(\lambda) = \sum_{i=0}^{n} a_i \lambda^i
\end{equation*}
\begin{warning}
    We distinguish between $\F[t]$ and the space of polynomial maps $\F \mapsto \F$. If $\F$ is finite, then $\F[t]$ is not finite-dimensional but the space of polynomial maps on $\F$ is a subspace of $\F^{\F}$ which is finite-dimensional. This comes about because some polynomials agree on all points in $\F$ and so are not unique as functions (but have different coefficients). For example, in $\F = \Z / 5\Z$, the polynomials $f(t) = t$ and $f(t) = t^5$ encode the same underlying function but have different coefficients.
\end{warning}
\begin{proposition}[Euclid's Algorithm for Polynomials]
    For $f, g \in \F[t]$, $g \neq 0$, there exist $q, r \in \F[t]$ such that:
    \begin{equation*}
        f = qg + r
    \end{equation*}
    where $\deg(r) < \deg(q)$.
    \label{propEuclidAlgPoly}
\end{proposition}
\begin{proof}
    The proof is very similar to the integer case.

    Let $n = \deg(f), m = \deg(g)$. If $m > n$ set $r = f, q = 0$.

    If $n \geq m$ we can perform an induction by setting $\tilde{f}(t) = f(t) - \left(\frac{a_n}{b_m}\right)t^{n-m} g(t)$, which has degree less than $\deg(f)$.
\end{proof}
\begin{corollary}[B\'ezout's Lemma for Polynomials]
    If $f_1, \cdots, f_k$ are polynomials with no non-constant common divisor, then:
    \begin{equation*}
        \exists q_1, \cdots, q_n \in \F[t] \text{ s.t. } 1 = \sum_{i=1}^{k} f_i q_i
    \end{equation*}
    \label{corBezoutPolynom}
\end{corollary}
\begin{lemma}
    For $\lambda \in \F$, $f(\lambda) = 0$ if and only if $(t - \lambda) | f(t)$.
    \label{lemFactorTheorem}
\end{lemma}
The proof is by applying Euclid's Algorithm to $f(t)$ with $g(t) = t - \lambda$.

\begin{definition}{Root of a polynomial}
    A scalar $\lambda \in \F$ is a \underline{root} of a polynomial $f \in \F[t]$ if $f(\lambda) = 0$.
\end{definition}
\begin{definition}{Multiplicity}
    A root $\lambda \in \F$ for a polynomial $f$ has \underline{multiplicity} $e$ which is the largest integer satisfying $(t - \lambda)^e | f$.
\end{definition}
\begin{corollary}
    If $\deg(f) = n \geq 0$, then $f$ has at most $n$ roots (when considering multiplicity).
    \label{corPolyNRoots}
\end{corollary}
\begin{corollary}
    If $\deg(f), \deg(g) < n$ and there exists $n$ distinct $\lambda_i$ which are roots of both $f$ and $g$ then $f = g$.
    \label{corPolyCompareRoots}
\end{corollary}
\begin{proof}
    The proof is by factoring both $f$ and $g$ into a product of linear factors $(x - \lambda_i)$, and noting that these factorisations are the same so the coefficients of $f$ and $g$ are equal.
\end{proof}
\begin{theorem}[Fundamental Theorem of Algebra]
    Every $f \in \C[t]$ of degree $n \geq 1$ has exactly $n$ roots, when considering multiplicity.
    \label{thmFundamentalAlgebra}
\end{theorem}
\begin{proof}
    The proof is seen in IB Complex Analysis.
\end{proof}
\section{An Introduction to Eigenspaces}
\begin{definition}{Diagonalisability}
    Let $V$ be a finite-dimensional vector space. Let $\alpha : V \mapsto V$. This is \underline{diagonalisable} if there exists a basis $B$ of $V$ such that $[\alpha]_B^B$ is a diagonal matrix.
\end{definition}
\begin{definition}{Triangularisability}
    Let $V$ be a finite-dimensional vector space. Let $\alpha : V \mapsto V$. This is \underline{triangularisable} if there exists a basis $B$ of $V$ such that $[\alpha]_B^B$ is a triangular matrix.
\end{definition}
\begin{remarks}
    \item $A \in M_{n \times n}(\F)$ is diagonalisable if $A$ is similar to a diagonal matrix.
    \item $A \in M_{n \times n}(\F)$ is triangularisable if $A$ is similar to a triangular matrix.
    \item By change of basis, $\alpha$ is diagonalisable if and only if for some basis $B$ of $V$, $[\alpha]_B^B$ is diagonalisable.
    \item By change of basis, $\alpha$ is triangularisable if and only if for some basis $B$ of $V$, $[\alpha]_B^B$ is triangularisable.
\end{remarks}
Diagonal and triangular matrices have nice properties with their determinants. It is easy to compute the determinant of an upper triangular matrix, since this is simply the product of the diagonal elements, and it is easy to understand the similarity classes of a diagonal matrix.

If $B = \{v_1, \cdots, v_n\}$ is a basis for a space $V$, and in this basis a map $\alpha : V \mapsto V$ is given by:
\begin{equation*}
    [\alpha]_B^B =
    \begin{pmatrix}
        \lambda_1 &  & 0 \\
         & \ddots &  \\
        0 &  & \lambda_n
    \end{pmatrix}
    = \diag(\lambda_1, \cdots, \lambda_n)
\end{equation*}
then $\alpha(v_i) = \lambda_i v_i$.
\begin{definition}{Eigenvalue}
    Let $V$ be a vector space over $\F$, and let $\alpha \in \L(V, V)$. $\lambda \in \F$ is an \underline{eigenvalue} of $\alpha$ if there exists a non-zero vector $v \in V$ such that:
    \begin{equation*}
        \alpha(v) = \lambda v
    \end{equation*}
    In this case, $v$ is an \underline{eigenvector} of $\alpha$.
\end{definition}
\begin{definition}{Eigenspace}
    Let $V$ be a vector space over $\F$. For an eigenvalue $\lambda$ of a linear map $\alpha : V \mapsto V$, the \underline{$\lambda$-eigenspace} is given by:
    \begin{equation*}
        V_{\lambda} = \subsetselect{v \in V}{\alpha(v) = \lambda v}
    \end{equation*}
    That is, all the eigenvectors with eigenvalue $\lambda$.
\end{definition}
\begin{propositions}{
        Let $V$ be a vector space over a field $\F$.
        \label{propsEVals}
    }
    \item $V_\lambda = \ker(\alpha - \lambda \Id_V) \leq V$ \label{propEValKernel}
    \item For $V$ finite-dimensional, $\alpha$ is diagonalisable if and only if $V$ has a basis of eigenvectors of $\alpha$. \label{propDiagIffBasis}
    \item If $v \in V_{\mu}$, for $\lambda \neq \mu$, then
        \begin{equation*}
            (\alpha - \lambda \Id_V) (v) = (\mu - \lambda)(v)
        \end{equation*}
        Thus $(\alpha - \lambda \Id_V)$ preserves $V_{\mu}$ and $(\alpha - \lambda \Id_V) \vline_{V_\mu}$ is invertible with inverse $v \mapsto (\mu - \lambda)^{-1} v$.
        \label{propEValDifferentKernel}
\end{propositions}
\begin{proof}
    The proofs of these statements are quite straightforward.
    \begin{enumerate}
        \item If $v \in V_{\lambda}$ then $\alpha(v) = \lambda v$. Equivalently, $(\alpha(v) - \lambda \Id_V(v)) = \zv$, so $v$ is in $\ker(\alpha - \Id_V)$. Similarly, if $v \in \ker(\alpha - \Id_V)$ then $\alpha(v) = \lambda v$ and $v \in V_{\lambda}$.
        \item This is by noting that the basis required by the definition of a diagonal matrix is in fact the eigenvector basis.
        \item This is immediate by working through the steps in the statement.
    \end{enumerate}
\end{proof}
\begin{lemma}
    Let $\lambda_1, \cdots, \lambda_k \in \F$ be a set of distinct eigenvalues over a vector space $V$, and $\zv \neq v_i \in V_{\lambda_i}$ be a set of corresponding eigenvectors. Then the set $\{v_1, \cdots, v_k\}$ is linearly independent.
    \label{lemEVecLI}
\end{lemma}
\begin{remark}
    The lemma is more succinctly stated: eigenvectors corresponding to distinct eigenvalues are linearly independent.
\end{remark}
\begin{proof}
    Suppose not. Let $\sum_{i=0}^{k} \mu_i v_i = \zv_V$ and assume that $\mu_i$ are not all zero. After possibly re-labelling, assume $\mu_1 \neq 0$. Define also:
    \begin{equation*}
        \beta = \prod_{i=2}^{k}(\alpha - \lambda_i \Id_V)
    \end{equation*}
    Then by proposition~\ref{propEValDifferentKernel},
    \begin{align*}
        \beta(v_j) &= \left(\prod_{i=2}^{k}(\lambda_j - \lambda_i)\right) v_j \\
        &\neq 0 \text{ if and only if } j = 1. \\
        \therefore \zv &= \beta\left(\sum_{i = 1}^k \mu_i v_i\right) \\
        &= \beta(\mu_1 v_1) \text{ by previous remark} \\
        &= \left(\mu_1 \prod_{i = 1}^k(\lambda_1 - \lambda_k)\right) v_1
    \end{align*}
    But, since the eigenvalues are all distinct, we must have $\mu_1 = 0$ \contradiction
\end{proof}
\begin{corollary}
    Let $V$ be a vector space over a field $\F$ with dimension $n$. Any linear map $\alpha \in \L(V, V)$ has at most $n$ eigenvalues.
    \label{corFinitelyManyEVals}
\end{corollary}
\begin{proof}
    Let $\alpha$ have $m$ eigenvalues $\lambda_1, \cdots, \lambda_m$. Then each eigenvalue has a corresponding eigenvector $v_i$, and by lemma~\ref{lemEVecLI}, these form a linearly independent set. By theorem~\ref{thmSteinitzExchange}, a linearly independent set of $V$ can have size at most $n$. Therefore, there can be at most $n$ eigenvectors and, because each eigenvalue has at least one eigenvector, at most $n$ eigenvalues.
\end{proof}
\begin{propositions}{
        Let $V$ be a finite-dimensional vector space over $\F$. Let $\alpha \in \L(V, V)$ with eigenvalues $\lambda_1, \cdots, \lambda_k \in F$.
        \label{propsEigenspaces}
    }
    \item $\spn{\bigcup_{i = 1}^k V_{\lambda_k}} = \bigoplus_{i = 1}^k V_{\lambda_k}$ \label{propSpanESpaces}
    \item $\alpha$ is diagonalisable if and only if $V = \bigoplus_{i = 1}^k V_{\lambda_k}$ \label{propESpaceWholeSpace}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Recall that the map in proposition~\ref{propMapDSToSum} is surjective, so $\spn{\bigcup_{i = 1}^k V_{\lambda_k}} \geq \bigoplus_{i = 1}^k V_{\lambda_k}$. If it is not also injective, there exists a vector in the kernel of this map. That is, there exists a linear combination:
            \begin{equation*}
                \sum_{i=1}^{k} v_j = \zv
            \end{equation*}
            but not all the $v_j$ are zero. This contradicts lemma~\ref{lemEVecLI}.
        \item
            \begin{proofdirection}{$\Rightarrow$}{Suppose that $\alpha$ is diagonalisable}
                By proposition~\ref{propDiagIffBasis}, we have a basis of eigenvectors. Then this means $V$ is a direct sum of the spaces spanned by each of these eigenvectors, which are the $V_{\lambda_i}$.
            \end{proofdirection}
            \begin{proofdirection}{$\Leftarrow$}{Suppose that $V$ is a direct sum}
                Because we are in finite-dimensional space, each $V_{\lambda_i}$ has a basis $B_i$. By Ex1Q8, the union $B = \bigcup_{i = 1}^k B_i$ is a basis for $V$. That is, $\alpha$ has a basis of eigenvectors, so $[\alpha]_B^B$ is diagonal.
            \end{proofdirection}
    \end{enumerate}
\end{proof}
\section{The Characteristic Polynomial}
Recall that for $V$ finite-dimensional, and $\beta \in \L(V, V)$, $\det(\beta) \neq 0 \iff \ker{\beta} \neq \{\zv_V\}$. Therefore, $\lambda \in \F$ is an eigenvalue of $\alpha \in \L(V, V)$ if and only if $\det(\alpha - \lambda \Id_V) = 0$.
\begin{definition}{Matrix characteristic polynomial}
    For $A \in M_{n \times n}(\F)$ the \underline{characteristic polynomial} of $A$ is:
    \begin{equation*}
        \chi_A(t) = \det(t I_n - A) \in \F[t]
    \end{equation*}
\end{definition}
\begin{definition}{Linear map characteristic polynomial}
    For a finite-dimensional vector space $V$ over a field $\F$, and $\alpha \in \L(V, V)$, the \underline{characteristic polynomial} of $\alpha$ is:
    \begin{equation*}
        \chi_\alpha(t) = \det(t \Id_V - \alpha)
    \end{equation*}
\end{definition}
We make concrete the remark we made earlier:
\begin{lemma}
    Let $V$ be a finite dimensional vector space over a field $\F$ and $\alpha \in \L(V, V)$.
    The roots of $\chi_\alpha$ are exactly the eigenvalues of $\alpha$.
    \label{lemCharPolyIsEvals}
\end{lemma}
\begin{remarks}
    \item $\chi_\alpha$ is monic. This is why we define it the ``other way round'' to in IA Vectors and Matrices, where we had that the leading coefficient was $(-1)^n$.
    \item Similar matrices have the same characteristic polynomial, because determinant is invariant on similarity classes (and $P^{-1} I P = I$).
    \item By the Leibniz formula applied to $tI - A$, we get that:
        \begin{equation*}
            \chi_\alpha(t) = t^n + C_{n-1} t^{n-1} + \cdots + C_1 t + C_0
        \end{equation*}
        These coefficients generally have interesting properties. For example, $C_{n-1} = -\tr(\alpha)$ and $C_0 = (-1)^n \det(A)$.
\end{remarks}
\begin{proposition}
    For $V$ a finite-dimensional vector space and $\alpha \in \L(V, V)$, if $\alpha$ is triangularisable then $\chi_\alpha(t)$ can be written as a product of linear factors.
    \label{propTriangularFactorise}
\end{proposition}
\begin{proof}
    Consider the basis $B$ in which $[\alpha]_B^B$ is triangular. Call this matrix $A$. By proposition~\ref{propTriangularDet}, $\det(t\Id_V - \alpha)$ is the product of the diagonal elements of $tI - A$, and this is precisely:
    \begin{equation*}
        \chi_\alpha(t) = \prod_{i=1}^{n} (t - a_{ii})
    \end{equation*}
\end{proof}
\begin{remark}
    If the characteristic polynomial does not split as linear factors, then the matrix is not triangularisable.
\end{remark}
\begin{example}
    Consider the rotation matrix:
    \begin{equation*}
        A =
        \begin{pmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta)
        \end{pmatrix}
    \end{equation*}
    We can compute the characteristic polynomial:
    \begin{equation*}
        \chi_A(t) = t^2 - 2\cos(\theta) t + 1
    \end{equation*}
    Then considering the discriminant, $4\cos^2(\theta) - 4$, this is negative for $\theta \in (0, \pi)$. So, in the space of \textit{real matrices}, this is not triangularisable.
\end{example}
\begin{theorem}
    Every $A \in M_{n \times n}(\C)$ is triangularisable.
    \label{thmComplexTriangular}
\end{theorem}
\begin{proof}
    By theorem~\ref{thmFundamentalAlgebra}, $\chi_A(t)$ has a root $\lambda \in C$.

    \induction{$n = 2$}{
        There exists a non-zero vector $v \in V$ such that $\alpha(v) = \lambda v$. Extend to a basis $B = \{v, w\}$ of $V$. With respect to this basis, $\alpha$ is given by:
        \begin{equation*}
            [\alpha]_B^B =
            \begin{pmatrix}
                \lambda & a_{12} \\
                0 & a_{22}
            \end{pmatrix}
        \end{equation*}
        This is upper-triangular.
    }{$n = k$}{
        Assume any matrix in $M_{k \times k}(\C)$ is triangularisable.
    }{$n = k + 1$}{
        Given the eigenvalue $\lambda$ there must exist a non-zero vector $v_1 \in V$ such that $\alpha(v) = \lambda v_1$. Then extend to a basis $B = \{v_1, \cdots, v_n\}$ of $V$. Under this basis, $\alpha$ is given by the block matrix:
        \begin{equation*}
            [\alpha]_B^B = A =
            \begin{pmatrix}
                \lambda & \star \\
                0 & C
            \end{pmatrix}
        \end{equation*}
        Now $C \in M_{k \times k}(\F)$, and so by induction there exists a change-of-basis matrix $\bar{P} \in GL_k(\C)$ such that $\bar{P} C \bar{P}^{-1}$ is upper-triangular. Extend $\bar{P}$ to a matrix $P$ by:
        \begin{equation*}
            P =
            \begin{pmatrix}
                1 & 0 \\
                0 & \bar{P}
            \end{pmatrix}
        \end{equation*}
        Then $PAP^{-1}$ is upper-triangular.
    }
\end{proof}
\begin{remark}
    The same proof can be used for any field $\F$, $A \in M_{n \times n}(\F)$ is triangularisable if and only if it is a product of linear factors.
\end{remark}
\begin{corollary}
    Every linear map in a finite-dimensional vector space over $\C$ is triangularisable.
    \label{corComplexLMTriangular}
\end{corollary}
\section{Annihilator Polynomials}
We define notation for applying a polynomial to a linear map. Let $p(t)$ be a polynomial,
\begin{equation*}
    p(t) = \sum_{i=0}^{n}\mu_i t^i
\end{equation*}
then we can apply this to a linear map $\alpha$, $p(\alpha)$, to get the linear map:
\begin{equation*}
    p(\alpha) = \sum_{i=0}^{n} \mu_i \alpha^i
\end{equation*}
We define a similar notion for a square matrix $A$, where we say $A^0 = I$.
\begin{remark}
    If $B$ is a basis for $V$ then $[p(\alpha)]_B^B = p([\alpha]_B^B)$.
\end{remark}
\begin{definition}{Annihilator polynomial}
    For $V$ a finite-dimensional vector space over $\F$ and $\alpha \in \L(V, V)$, $p \in \F[t]$ is an \underline{annihilator polynomial} if $p(\alpha)$ gives the zero map.
\end{definition}
The set of all such polynomials in $\F[t]$ is called the \underline{annihilator ideal} or just \underline{annihilator} of $\alpha$ and is denoted $I_\alpha$. For more about ideals, see the course IB Groups, Rings and Modules.

This section is now devoted to understanding some important members of this set.
\begin{example}[Examples of annihilator polynomials]
    Clearly the zero polynomial is inside $I_\alpha$.

    We can find a non-zero example by noting that $\dim(\L(V, V)) = n^2$, so the set $\{\Id_V, \alpha, \alpha^2, \cdots, \alpha^{n^2}\}$ is linearly dependent. Therefore, there exist coefficients $\mu_i$ not all zero such that:
    \begin{equation*}
        0 = \sum_{i=0}^{n^2} \mu_i \alpha^i = p(\alpha)
    \end{equation*}
    This is therefore another annihilator polynomial.
\end{example}
\begin{warning}
    This is not a standard definition. The idea of an annihilator used in this context is one from Ring Theory (see the course IB Groups, Rings and Modules), not the concept used in this course (although they are connected). This is my own definition to be able to talk about the set of these polynomials more succinctly.
\end{warning}
\begin{definition}{Minimal polynomial}
    The \underline{minimal polynomial} $m_\alpha$ of a linear map $\alpha \in \L(V, V)$ for a finite-dimensional vector space $V$ is the monic polynomial $p \in \F[t]$ with least degree that satisfies:
    \begin{equation*}
        p(\alpha) = 0
    \end{equation*}
\end{definition}
Then we want to show nice properties about this such as uniqueness, and we want to understand its roots.
\begin{lemma}
    For any $f \in I_\alpha$, $m_\alpha~\vline~f$.
    \label{lemMinPolyDivides}
\end{lemma}
\begin{proof}
    By proposition~\ref{propEuclidAlgPoly}, there exist $r, q \in \F[t]$ such that $\deg(r) < \deg(m_\alpha)$ and $f = q m_\alpha + r$. Then rearrange:
    \begin{equation*}
        r(\alpha) = f(\alpha) - q(\alpha) m_\alpha(\alpha) = 0
    \end{equation*}
    Therefore $r \in I_\alpha$ but by minimality of $\deg(m_\alpha)$, $r = 0$.
\end{proof}
\begin{theorem}
    The minimal polynomial, $m_\alpha$, is unique.
    \label{thmMinimalPolynomialUnique}
\end{theorem}
\begin{proof}
    Let $\overline{m_\alpha}$ be another non-zero element of minimal degree in $I_\alpha$, and let it be monic (by perhaps dividing by the leading coefficient). By lemma~\ref{lemMinPolyDivides} we have that $m_\alpha~\vline~\overline{m_\alpha}$, but since they have the same degree we must have that one is a multiple of the other. Because both are monic, in fact they must be equal.
\end{proof}
\begin{remark}
    Because $m_\alpha$ and $\overline{m_\alpha}$ were monic, this theorem tells us that $m_\alpha$ is unique in $I_\alpha$ up to multiplication by a constant.
\end{remark}
\begin{theorem}[Cayley-Hamilton Theorem]
    Let $V$ be a finite-dimensional vector space and let $\alpha \in \L(V, V)$. Let $\chi_\alpha$ be the characteristic polynomial for $\alpha$.
    \begin{equation*}
        \chi_\alpha(\alpha) = 0
    \end{equation*}
    \label{thmCayleyHamilton}
\end{theorem}
\begin{remark}
    Equivalently, $\chi_\alpha \in I_\alpha$.
\end{remark}
\begin{proof}[for $\F = \C$]
    By theorem~\ref{thmComplexTriangular}, there exists a basis $B = \{v_1, \cdots, v_n\}$ for $V$ such that:
    \begin{equation*}
        [\alpha]_B^B =
        \begin{pmatrix}
            \lambda_1 &  & \star \\
             & \ddots &  \\
            0 &  & \lambda_n
        \end{pmatrix}
    \end{equation*}
    Then the characteristic polynomial is a product of linear factors $\chi_\alpha(t) = \prod_{i=1}^{n} (t - \lambda_i)$.

    Define $U_j = \spn{v_1, \cdots, v_j}$ so that $U_0 = \{\zv\}$ and $U_n = V$. Then for $1 \leq j \leq n$,
    \begin{equation*}
        (\alpha - \lambda_j \Id_V)(U_j) \leq U_{j-1}
    \end{equation*}
    because $(\alpha - \lambda_j \Id_V)(v_j) = \zv$.

    Therefore:
    \begin{align*}
        \chi_\alpha(\alpha) (V)&= (\alpha - \lambda_1 \Id_V) \circ \cdots \circ (\alpha - \lambda_n \Id_V)U_n \\
        &= U_0 = \{\zv\} \text{ by telescoping product.}
    \end{align*}
    Therefore, $\chi_\alpha(\alpha)$ must be the zero map, as required.
\end{proof}
\begin{remark}
    The only time we needed $\C$ was for triangularisability, so we have in fact proved that every triangularisable linear map satisfies its own characteristic polynomial.
\end{remark}
\begin{corollary}
    For $V$ finite-dimensional, $\alpha \in \L(V, V)$, $m_\alpha~\vline~\chi_\alpha$.
    \label{corMinPolyDividesCharPoly}
\end{corollary}
\begin{proof}
    This is a direct consequence of lemma~\ref{lemMinPolyDivides}, because we now have $\chi_\alpha \in I_\alpha$ by theorem~\ref{thmCayleyHamilton}.
\end{proof}
\begin{definition}{Algebraic multiplicity}
    Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. The  \underline{algebraic multiplicity} of an eigenvalue $\lambda \in \F$, $a_\lambda$, is the multiplicity of $\lambda$ as a root of $\chi_\alpha(t)$. That is, the largest power of $(t - \lambda)$ that divides $\chi_\alpha$.
\end{definition}
\begin{definition}{Geometric multiplicity}
    Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. The \underline{geometric multiplicity} of $\lambda$, $g_{\lambda}$, is the dimension of its eigenspace.
\end{definition}
\begin{proposition}[The other AM-GM inequality]
    For $V$ a finite-dimensional vector space over $\F$, $\alpha \in \L(V, V)$ and $\lambda \in \F$ an eigenvalue of $\alpha$,
    \begin{equation}
        g_\lambda \leq a_\lambda.
        \label{eqnAMGM}
    \end{equation}
    \label{propAMGM}
\end{proposition}
\begin{proof}
    let $\{v_1, \cdots, v_k\}$ be a basis for $V_\lambda$. Extend to a basis $B = \{v_1, \cdots, v_n\}$ of $V$. Then:
    \begin{equation*}
        [\alpha]_B^B =
        \begin{pmatrix}
            \lambda I_k & \star \\
            0 & C
        \end{pmatrix},~~C \in M_{(n-k) \times (n-k)}(\F).
    \end{equation*}
    Then we have that $\chi_\alpha(t) = (t - \lambda)^k \chi_C(t)$ where $\chi_C(t)$ may contain another factor of $(t - \lambda)^k$ so we have that $a_\lambda \geq k = g_\lambda$.
\end{proof}
\begin{propositions}{
        Let $V$ be a finite-dimensional vector space over $\F$ and let $\alpha \in \L(V, V)$. Let the eigenvalues of $\alpha$ be $\lambda_1, \cdots, \lambda_k \in \F$.
        \label{propsAMGMIneqs}
    }
    \item $\sum_{i=1}^{k} g_{\lambda_i} \leq \dim(V)$ with equality if and only if $\alpha$ is diagonalisable. \label{propCompleteIffDiag}
    \item $\sum_{i = 1}^k a_{\lambda_i} \leq \dim(V)$ with equality if and only if $\alpha$ is triangularisable.
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item We have this result by applying propositions~\ref{propsEigenspaces}.
        \item We write:
            \begin{equation*}
                \chi_\alpha(t) = f(t) \prod_{i=1}^{k}(t - \lambda_i)^{a_{\lambda_i}}.
            \end{equation*}
            Now note that if $f(t)$ has a linear factor, then this is an eigenvalue of $\alpha$ which is not already counted which is a contradiction, so it must not have any linear factors. Moreover,
            \begin{align*}
                \dim(V) &= \deg(\chi_\alpha) \\
                &= \deg(f) + \sum_{i=1}^{k} a_{\lambda_i} \\
                &\leq \sum_{i=1}^{k} a_{\lambda_i}
            \end{align*}
            With equality if and only if in fact $f$ is constant, in which case $\chi_\alpha$ is a product of linear factors. By proposition~\ref{propTriangularFactorise} this is equivalent to triangularisability.
    \end{enumerate}
\end{proof}
\begin{proposition}
    Let $V$ be a finite-dimensional vector field over $\C$. A linear map $\alpha \in \L(V, V)$ is diagonalisable if and only if $a_\lambda = g_\lambda$ for every eigenvalue $\lambda$ of $\alpha$.
    \label{propDiagIfNotDegenerate}
\end{proposition}
\begin{proof}
    By theorem~\ref{thmComplexTriangular}, $\alpha$ is triangularisable and so $\chi_\alpha$ is a product of factors. By proposition~\ref{propAMGM},
    \begin{equation*}
        \sum_{i=1}^{k}g_{\lambda_i} \leq \sum_{i=1}^{k} a_{\lambda_i} = \dim(V)
    \end{equation*}
    So the only way we can get equality (note that diagonalisability is equivalent to the sum of geometric multiplicities being $\dim(V)$) is if in fact $\alpha_\lambda = g_\lambda$ for all eigenvalues $\lambda$.
\end{proof}
\begin{remark}
    For $p(t) = \sum_{i=1}^{n}\mu_i t^i$, $v \in V_\lambda$,
    \begin{equation*}
        (p(\alpha))(v) = \sum_{i=1}^n \mu_i \alpha^i(v) = \sum_{i=1}^{n} \mu_i \lambda^i v = p(\lambda) v
    \end{equation*}
\end{remark}
\begin{lemma}
    For $\lambda \in \F$,
    \begin{equation*}
        \lambda \text{ is a root of } m_\alpha \iff \lambda \text{ is a root of } \chi_\alpha
    \end{equation*}
    \label{lemMinPolyCharPolySameRoots}
\end{lemma}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let $\lambda$ be a root of $m_\alpha$}
        We have that $(t - \lambda)$ divides $\chi_\alpha$. By corollary~\ref{corMinPolyDividesCharPoly}, we must also have that $(t - \lambda)$ divides $\chi_\alpha$, or equivalently that $\lambda$ is a root of $\chi_\alpha$.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Let $\lambda$ be a root of $\chi_{\alpha}$}
        We have that $\lambda$ is an eigenvalue so $\alpha(v) = \lambda v$ for a non-zero eigenvector $v$.
        \begin{align*}
            (m_\alpha(\alpha))(v)&= m_\alpha(\lambda) v \text{ by the above remark}
        \end{align*}
        But since $m_\alpha \in I_\alpha$, the LHS of this equation is zero. Since $v \neq \zv$, we must instead have that $m_{\alpha}(\lambda) = 0$.
    \end{proofdirection}
\end{proof}
\begin{definition}{Minimal multiplicity}
    For $V$ a finite-dimensional vector space over $\F$ and $\alpha \in \L(V, V)$, the \underline{minimal multiplicity} of an eigenvalue $\lambda$, $c_\lambda$, is the multiplicity of $\lambda$ as a root of the minimal polynomial.
\end{definition}
\begin{remark}
    By theorem~\ref{thmCayleyHamilton}, $1 \leq c_\lambda \leq a_\lambda$.
\end{remark}
\begin{examples}{
        
    }
    \item Let $A = \lambda I_n$. Then $\chi_A(t) = (t - \lambda)^n$, and $V_\lambda = \F^n$. However, the minimal polynomial is $m_\alpha(t) = t - \lambda$, so the multiplicities are $a_\lambda = n, g_\lambda = n, c_\lambda = 1$.
    \item Let $A$ be the matrix:
        \begin{equation*}
            \begin{pmatrix}
                \lambda & 1 & 0 & \cdots & 0 \\
                0 & \lambda & 1 & \cdots & 0 \\
                0 & 0 & \ddots & \ddots & \vdots \\
                \vdots & \vdots & & \lambda & 1 \\
                0 & 0 & \cdots & 0 & \lambda
            \end{pmatrix}
        \end{equation*}
        Then $\chi_A(t) = (t - \lambda)^n$ because $A$ is upper-triangular, but if we consider the eigenspace of this eigenvalue we get only:
        \begin{equation*}
            V_\lambda = \spn{\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0\end{pmatrix}}
        \end{equation*}
        We know that the minimal polynomial must have the form $m_A(A) = (A - \lambda I)^k$ for some $1 \leq k \leq n$. In order to find $k$, consider:
        \begin{equation*}
            A - \lambda I =
            \begin{pmatrix}
                0 & 1 & 0 & \cdots & 0 \\
                0 & 0 & 1 & & \vdots \\
                \vdots & \vdots & & \ddots & 0 \\
                0 & 0 & 0 & 0 & 1 \\
                0 & 0 & 0 & 0 & 0
            \end{pmatrix}
        \end{equation*}
        By multiplying this matrix out, we find that each power shifts the diagonal of 1s up each time, so the smallest $k$ for which this is zero is in fact $n$. That is, $m_A = \chi_A$.

        This gives us that $a_\lambda = 1, g_\lambda = 1, c_\lambda = n$.
        \label{expJordanPolys}
\end{examples}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha \in \L(V, V)$. If there exists a polynomial $p \in I_\alpha$ which is a product of distinct linear factors, then $\alpha$ is diagonalisable.
    \label{thmProdDistinctFactorsDiag}
\end{theorem}
\begin{proof}
    We can assume that $p$ is monic by rescaling. Write $p$ in the form:
    \begin{equation*}
        p(t) = \prod_{i=1}^{k} (t - \mu_i) \text{ for distinct $\mu_i$.}
    \end{equation*}
    Define also $p_i(t) = \prod_{\substack{j = 1\\j \neq i}}^{k} (t - \mu_j)$, the polynomial excluding the root $\mu_i$. Then by corollary~\ref{corBezoutPolynom}, there exist $q_i \in \F[t]$ such that:
    \begin{equation*}
        1 = \sum_{i=1}^{k} p_i(t) q_i(t)
    \end{equation*}
    Where here $1$ represents the constant polynomial. Setting $\pi_i = p_i(\alpha) q_i(\alpha) \in \L(V, V)$, for $v \in V$,
    \begin{equation*}
        v = \Id_V(v) = \sum_{i=1}^{k} \pi_i(v)
    \end{equation*}
    Hence $V = \im(\pi_1) + \cdots, + \im(\pi_k)$. Further:
    \begin{align*}
        &p(t) = (t - \mu_i)p_i(t) \\
        &\therefore p(\alpha) = (\alpha - \mu_i \Id_V)(p_i(\alpha)) = 0 \\
        &\therefore (\alpha - \mu_i\Id_V) \circ \pi_i = 0
    \end{align*}
    And so $\im(\pi_i) \leq V_{\mu_i}$. Hence, $V = \sum_{i=1}^{k} \im(\pi_i) \leq \bigoplus_{i=1}^k V_{\mu_i} \leq V$.

    Therefore. $V$ is a direct sum of eigenspaces, so $\alpha$ is diagonalisable.
\end{proof}
We provide a theorem to summarise our characterisation of diagonalisability:
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha \in \L(V, V)$. The following are equivalent:
    \begin{enumerate}
        \item $\alpha$ is diagonalisable.
        \item $V$ has a basis consisting of eigenvectors of $\alpha$
        \item There exists a non-zero polynomial $p \in I_\alpha$ that is a product of distinct linear factors.
        \item $m_\alpha$ is a product of distinct linear factors.
        \item {[Only in the case $\F = \C$]} $a_\lambda = g_\lambda$ for all eigenvalues $\lambda$.
    \end{enumerate}
    \label{thmDiagConditions}
\end{theorem}
\begin{proof}
    Statement 1 is equivalent to statement 2 by proposition~\ref{propDiagIffBasis}, and statement 1 is equivalent to statement 5 (in the case of the complex field) by proposition~\ref{propDiagIfNotDegenerate}.

    \begin{subproof}{Statement 3 is equivalent to statement 4}
        Because $m_\alpha$ divides any polynomial in $I_\alpha$, if $p \in I_\alpha$ is a product of distinct linear factors then so is $m_\alpha$. If instead $m_\alpha$ is a product of distinct linear factors then take $p = m_\alpha$ in theorem~\ref{thmProdDistinctFactorsDiag}.
    \end{subproof}
    Statement 3 implies statement 1 by theorem~\ref{thmProdDistinctFactorsDiag} directly.
    \begin{subproof}{Statement 1 implies statement 3}
        Let $B$ be a basis for $V$ such that $[\alpha]_B^B$ is diagonal. After possibly rearranging the basis, ensure that vectors in the same eigenspace are next to each other. In this basis,
        \begin{equation*}
            [\alpha]_B^B =
            \begin{pmatrix}
                \lambda_1 I_{n_1} & 0 & 0 & \cdots & 0 \\
                0 & \lambda_2 I_{n_2} & 0 & \cdots & 0 \\
                0 & 0 & \lambda_3 I_{n_3} & & \vdots \\
                \vdots & \vdots & & \ddots & 0 \\
                0 & 0 & \cdots & 0 & \lambda_k I_{n_k}
            \end{pmatrix}
            \text{ where } n_k = g_{\lambda_k}
        \end{equation*}
        Then set $p(t) = \prod_{i=1}^{k} (t - \lambda_i)$. By considering the matrix $p([\alpha]_B^B)$, we see that each factor of $p$ deletes one of the blocks above. Since all blocks get deleted in one of the factors of $p$, and we are multiplying together diagonal matrices, we must have that indeed $p(\alpha) = 0$.
    \end{subproof}
\end{proof}
\begin{definition}{Simultaneous diagonalisability}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha, \beta \in \L(V, V)$. Then $\alpha$ and $\beta$ are \underline{simultaneously diagonalisable} if there exists a basis for $V$ such that in this basis both $[\alpha]_B^B$ and $[\beta]_B^B$ are diagonalisable.
\end{definition}
Before we prove a theorem about simultaneous diagonalisability, we need the following lemma:
\begin{lemma}
    Let $\alpha, \beta$ commute. Then for an eigenspace $V_\lambda$ of $\alpha$, $\beta(V_\lambda) = V_\lambda$.
    \label{lemSimDiagSameEvecs}
\end{lemma}
\begin{proof}
    Let $v \in V_\lambda$.
    \begin{align*}
        \alpha(\beta(v_\lambda))&= \beta(\alpha(v_\lambda)) \\
        &= \beta(\lambda v_\lambda) \\
        &= \lambda \beta(v)
    \end{align*}
    and hence $\beta(v) \in V_\lambda$.
\end{proof}
\begin{theorem}
    Let $V$ be a finite-dimensional vector space over $\F$, and $\alpha, \beta \in \L(V, V)$.

    $\alpha$ and $\beta$ are simultaneously diagonalisable if and only if they commute.
    \label{thmSimDiag}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let $\alpha$ and $\beta$ be simultaneously diagonalisable}
        Choose the basis in which they are both diagonal. Then since diagonal matrices commute,
        \begin{align*}
            [\alpha \circ \beta]_B^B&= [\alpha]_B^B [\beta]_B^B \\
            &= [\beta]_B^B [\alpha]_B^B \\
            &= [\beta \circ \alpha]_B^B
        \end{align*}
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $\alpha$ and $\beta$ commute.}
        $\alpha$ is diagonalisable and so take $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$ where the eigenvalues are distinct.
        Let $p \in I_\beta$ be a product of distinct linear factors. By lemma~\ref{lemSimDiagSameEvecs}, we can define the restriction $\beta |_{V_{\lambda_i}}$. Note also that $p(\beta |_{V_{\lambda_i}}) = p(\beta)|_{V_{\lambda_i}} = 0$, and so this restriction is diagonalisable. Let the basis in which it is diagonalisable be $B_i$, consisting of eigenvectors of $\beta$.

        The basis $B = \bigcup_{i=1}^k B_i$ is a basis for $V$, consisting of eigenvectors of both $\alpha$ and $\beta$. Therefore, we have constructed a basis in which $\alpha$ and $\beta$ are both diagonalisable.
    \end{proofdirection}
\end{proof}
\section{Jordan Normal Form}
In this section set $\F = \C$.
\begin{definition}{Jordan Matrix}
    For $\lambda \in \C$, the $(n \times n)$-\underline{Jordan matrix} is:
    \begin{equation*}
        J_n(\lambda) =
        \begin{pmatrix}
            0 & 1 & 0 & \cdots & 0 \\
            0 & 0 & 1 & & \vdots \\
            \vdots & \vdots & & \ddots & 0 \\
            0 & 0 & 0 & 0 & 1 \\
            0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \end{equation*}
\end{definition}
By example~\ref{expJordanPolys}, this has characteristic polynomial:
\begin{equation*}
    \chi_{J_n(\lambda)} = (t - \lambda_n) = m_{J_n(\lambda)}
\end{equation*}
and $\lambda$-eigenspace $\spn{\vec{e_1}}$.
\begin{definition}{Jordan Normal Form}
    A matrix $A \in M_{n \times n}(\C)$ is in \underline{Jordan normal form} if:
    \begin{equation*}
        A =
        \begin{pmatrix}
            J_{n_1}(\lambda_1) & 0 & 0 & \cdots & 0 \\
            0 & J_{n_2}(\lambda_2) & 0 & \cdots & 0 \\
            0 & 0 & J_{n_3}(\lambda_3) & & \vdots \\
            \vdots & \vdots & & \ddots & 0 \\
            0 & 0 & \cdots & 0 & J_{n_k}(\lambda_k)
        \end{pmatrix}
    \end{equation*}
    for some $n_i \in \N$ summing to $n$, and some $\lambda_i \in \C$ (not necessarily distinct).

    The $J_{n_i}(\lambda_i)$ are the \underline{Jordan blocks} of $A$. A Jordan block of size $1$ is just $\left(\begin{smallmatrix}\lambda\end{smallmatrix}\right)$.
\end{definition}
\begin{theorem}
    Every matrix $A \in M_{n \times n}(\C)$ is similar to a matrix $\operatorname{JNF}(A)$ in Jordan Normal Form, unique up to rearranging the Jordan blocks.
    \label{thmJNF}
\end{theorem}
The proof of this statement is non-examinable.
\begin{example}[Jordan Normal Form for $2 \times 2$ matrices]
    Let $\lambda_1, \lambda_2$ be distinct. The possible JNFs for $A \in M_{2 \times 2}(\C)$ are:
    \begin{align*}
    A &= &\begin{pmatrix}\lambda_1 & 1 \\ 0 & \lambda_1\end{pmatrix} && \begin{pmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{pmatrix} && \begin{pmatrix}\lambda_1 & 0 \\ -0 & \lambda_2\end{pmatrix} \\
    m_A(t) &= & (t - \lambda_1)^2 && (t - \lambda_1)(t - \lambda_2) && (t - \lambda_1)
    \end{align*}
\end{example}
\begin{example}[Jordan Normal Form for $3 \times 3$ matrices]
    Let $\lambda_1, \lambda_2, \lambda_3$ be distinct. The possible JNFs for $A \in M_{3 \times 3}(\C)$ are:
    \begin{align*}
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_2 & 0 \\
        0 & 0 & \lambda_3
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_2
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 0 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    \\
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_2
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 0 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    &&
    \begin{pmatrix}
        \lambda_1 & 1 & 0 \\
        0 & \lambda_1 & 1 \\
        0 & 0 & \lambda_1
    \end{pmatrix}
    \end{align*}
\end{example}
\begin{remark}
    For $n \leq 3$, we find that all the JNFs have different minimal/characteristic polynomial combinations. This is not the case for larger $n$, we already have a counterexample for $n = 4$.
\end{remark}
\end{document}