\documentclass[../Main.tex]{subfiles}

\begin{document}
Throughout this chapter, let $A \in M_{n \times n}(\F)$
\section{Defining the Determinant}
\begin{definition}{Volume form}
    A function $F$ is a \underline{volume form} if it satisfies:
    \begin{enumerate}
        \item \textbf{Alternating form:} if $\vec{c_i}, \vec{c_j}$ are equal columns of $A$, $i \neq j$, then $F(A) = 0$.
        \item \textbf{Multilinear in columns:} for all $1 \leq i \leq n$ and $\vec{v_j} \in \F^n$, the function:
        \begin{align*}
            \F^n &\mapsto \F\\
            \vec{v} &\mapsto F(V)
        \end{align*}
        is linear, where $V$ is the matrix with vectors $\vec{v_j}$ as its columns, and $\vec{v}$ in its $i$th column.
    \end{enumerate}
\end{definition}
\begin{propositions}{
        Let $F$ be a $n$-dimensional volume form, and $A \in M_{n \times n}(\F)$.
        \label{propsVolForm}
    }
    \item If $A$ has a zero column then $F(A) = 0$; \label{propVFZeroColumn}
    \item $F(A \cdot T_{ij}) = -F(A)$; \label{propVFColSwap}
    \item $F(A \cdot M_{i, \lambda}) = \lambda F(A)$; \label{propVFColScale}
    \item $F(A \cdot C_{i, j, \lambda}) = F(A)$. \label{propVFColAdd}
\end{propositions}
\begin{proof}
    Let the columns of $A$ be $c_j, 1 \leq j \leq n$. Let $f_i(\vec{v})$ be the function that evaluates $F$ with column $i$ set to $\vec{v}$ in $A$. We know that $f_i$ is linear by the properties of a volume form.

    Further, $f_i(\vec{c_j}) = \delta_{ij}$.
    \begin{enumerate}
        \item If $\vec{c_i} = \zv$, then $F(A) = f_i(\vec{c_i}) = f_i(\zv) = 0$.
        \item Let $\bar{A}$ be the matrix obtained from $A$ be replacing both the $i$th and $j$th columns by $\vec{c_i} + \vec{c_j}$. Then:
            \begin{equation*}
                0 = F(\bar{A}) = F(A) + f_i(\vec{c_j}) + f_j(\vec{c_i}) + F(A \cdot T_{ij})
            \end{equation*}
        \item \begin{equation*}
            F(A \cdot M_{i, j}) = f_i(\lambda \vec{c_i}) = \lambda f_i(\vec{c_i}) = \lambda F(A)
        \end{equation*}
        \item \begin{equation*}
            F(A \cdot C_{i, j, \lambda}) = f_j(\vec{c_j} + \lambda \vec{c_i}) = f_j(\vec{c_j}) + f_j(\lambda \vec{c_i}) = F(A)
        \end{equation*}
    \end{enumerate}
\end{proof}
before we give the main theorem of this chapter, we need some properties from IA groups.
\begin{itemize}
    \item $S_n$ is the group of permutations of a set of size $n$.
    \item $\sgn : S_n \mapsto \{\pm1\}$ is the unique homomorphism satisfying $\sgn(\tau) = -1$ for all transpositions $\tau$.
    \item $A_n = \ker(\sgn)$, and $S_n = A_n \times A_n\tau$ for any transposition $\tau \in S_n$.
\end{itemize}
\begin{theorem}
    There exists a unique function $F : M_{n \times n}(\F) \mapsto \F$ satisfying the following properties:
    \begin{enumerate}
        \item \textbf{Volume form:} $F$ is a volume form (satisfying alternating and multilinear properties).
        \item \textbf{Non-triviality:} $F(I_n) = 1$.
    \end{enumerate}
    \label{thmDeterminant}
\end{theorem}
\begin{proof}
    Let $F$ be an $n$-dimensional volume form satisfying $F(I_n) = 1$. Then by the lemma:
    \begin{align*}
        F(T_{ij}) = -1;~&~F(M_{i, \lambda}) = \lambda;\\
        F(C_{i, j, \lambda}) = 1;~&~F(AE) = F(A)F(E) \text{ for }E\text{ elementary.}
    \end{align*}
    \begin{subproof}{$F$ is unique.}
        Let $A \in M_{n \times n}(\F)$. Then there exist elementary matrices $E_1, \cdots, E_l$ such that: $A' = AE_1 \cdots E_l$ is in CRE form.
        \begin{equation*}
            F(A) = F(A') F(E_1)^{-1} \cdots F(E_l)^{-1}
        \end{equation*}
        Therefore, either $A' = I_n$, so $F(A) = F(E_1)^{-1} \cdots F(E_l)^{-1}$, or $A'$ has a zero column and so $F(A) = 0$.
    \end{subproof}
    \begin{subproof}{$F$ exists.}
        We can write down the following form for $F$:
        \begin{align*}
            F : M_{n \times n}(\F) &\mapsto \F\\
            A &\mapsto \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^{n}a_{\sigma(i), i}
        \end{align*}
        Now we need to check this $F$ is a volume for satisfying $F(I_n) = 1$.

        Given the matrix $I_n$, this has zeroes everywhere except the diagonal so the only $\sigma \in S_n$ contributes to the sum. This gives $F(I_n) = 1$.

        Each product $\prod_{i=1}^{n} a_{\sigma(i), i}$ is multilinear in columns, and therefore so is $F$.

        Now suppose $\vec{c_k} = \vec{c_l}, k \neq l$. Now set $\tau = (k~~l)$ so $a_{i,j} = a_{i, \tau(j)}$.
        \begin{align*}
            F(A) &= \sum_{\sigma \in A_n} \sgn(\sigma) \prod_{i=1}^{n} a_{\sigma(i), i} \\
            &+ \sum_{\sigma \in A_n} \sgn(\sigma \tau) \prod_{i=1}^{n} a_{\sigma\tau(i), i} \\
            &= \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma(i), i} - \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma\tau(i), i} \\
            &= \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma(i), i} - \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma\tau(i), \tau(i)} \\
            &= \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma(i), i} - \sum_{\sigma \in A_n} \prod_{i=1}^{n} a_{\sigma(j), j} \text{ by writing } \tau(i) = j \\
            &= 0
        \end{align*}
    \end{subproof}
\end{proof}
\begin{definition}{Determinant}
    The function defined in theorem~\ref{thmDeterminant} is the \underline{$n$-dimensional determinant}. We write $F(A) = \det(A) = |A|$.
\end{definition}
\begin{corollary}
    $\det(A) \neq 0$ if and only if $A$ is invertible. In this case, $A$ is a product of elementary matrices:
    \begin{equation*}
        A = E_1 \cdots E_l
    \end{equation*}
    and its determinant is the product of individual determinants.
    \label{corDetNZeroInvert}
\end{corollary}
\section{Properties of the Determinant}
\begin{lemma}
    For $A \in M_{n \times n}(\F)$, $\det(A^T) = \det(A)$.
    \label{lemDetTrans}
\end{lemma}
\begin{proof}
    \begin{align*}
        \det(A^T) &= \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^{n} a_{i, \sigma(i)} \\
        &= \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{j=1}^{n} a_{\sigma^{-1}(j), j} \text{ by setting } j = \sigma(i) \\
        &= \sum_{\sigma \in S_n} \sgn(\sigma^{-1}) \prod_{j=1}^{n} a_{\sigma^{-1}(j), j} \\
        &= \det(A)
    \end{align*}
    Here the final step is valid because inverting an element in a group is an isomorphism from a group to itself.
\end{proof}
\begin{proposition}
    For all $A, B \in M_{n \times n}(\F)$,
    \begin{equation*}
        \det(AB) = \det(A) \det(B)
    \end{equation*}
    \label{propDetProd}
\end{proposition}
\begin{proof}
    By lemma~\ref{lemRankOfProduct}, $rk(AB) \leq \min\{rk(A), rk(B)\}$. Therefore, if either $A$ or $B$ has rank less than $n$ then so does $AB$, and therefore by the corollary above $\det(AB) = 0$.

    If not, write:
    \begin{align*}
        A &= E_1 \cdots E_l \\
        B &= E_1' \cdots E_k' \\
        AB &= E_1 \cdots E_l E_1' \cdots E_k' \\
    \end{align*}
    The determinant is, by corollary~\ref{corDetNZeroInvert}:
    \begin{align*}
        \det(AB) &= \det(E_1) \cdots \det(E_l) \det(E_1') \cdots \det(E_k') \\
        &= \det(A)\det(B)
    \end{align*}
\end{proof}
\begin{corollary}
    If $A$ is an invertible matrix then:
    \begin{equation*}
        \det(A^{-1}) = \frac{1}{\det(A)}
    \end{equation*}
    \label{corInvertDet}
\end{corollary}
\begin{proof}
    \begin{align*}
        1 &= \det(I_n) = \det(AA^{-1}) \\
        &= \det(A) \det(A^{-1})
    \end{align*}
\end{proof}
\begin{corollary}
    Similar matrices have the same determinant.
    \label{corSimilarSameDet}
\end{corollary}
\begin{proof}
    Let $A' = PAP^{-1}$. Then:
    \begin{align*}
        \det(A') &= \det(PAP^{-1}) \\
        &= \det(P) \det(A) \det(P^{-1}) \\
        &=\frac{\det(P) \det(A)}{\det(P)} \\
        &= \det(A)
    \end{align*}
\end{proof}
\begin{remark}
    We can alternatively express this as: the determinant function is constant on similarity classes.
\end{remark}
\begin{definition}{Determinant of a linear map}
    Let $V$ be a finite-dimensional vector space, $\alpha \in \L(V, V)$. Let $B$ be a basis for $V$. Then the \underline{determinant} of $\alpha$ is:
    \begin{equation*}
        \det(\alpha) = \det([\alpha]_B^B)
    \end{equation*}
\end{definition}
\begin{proposition}
    For a linear map $\alpha : V\mapsto V$, $\det(A)$ is independent of the basis chosen for $V$.
    \label{propDetLinMapUnique}
\end{proposition}
\begin{proof}
    Let $B'$ be another basis for $B$. Let $P = [\Id_V]^B_{B'}$. Recall that $P$ is invertible, with inverse $P^{-1} = [\Id_V]_B^{B'}$.
    \begin{align*}
        [\alpha]_{B'}^B &= [\Id_V]_{B'}^B [\alpha]_{B'}^{B'} [\Id_V]_{B}^{B'}\\ 
        &= P[\alpha]_{B'}^{B'} P^{-1}
    \end{align*}
    Therefore the two matrices representing $\alpha$ in $B$ and $B'$ are similar, and so have the same determinant by corollary~\ref{corSimilarSameDet}.
\end{proof}
\begin{definition}{Singular linear map}
    A linear map $\alpha : V \mapsto V$ is \underline{singular} if $\det(\alpha) = 0$.
\end{definition}
\begin{corollary}
    For any $\alpha \in \L(V, V)$, the following are equivalent:
    \begin{enumerate}
        \item $rk(\alpha) = \dim(V)$;
        \item $\alpha$ is invertible;
        \item $\alpha$ is non-singular.
    \end{enumerate}
    \label{corInvertabilityLM}
\end{corollary}
\begin{proof}
    We have that statements 1 and 2 are equivalent by theorem~\ref{thmRankNullity}.
    \begin{subproof}{Statement 2 is equivalent to statement 3}
        Fix any basis $B$ for $V$. Set $A = [\alpha]_B^B$. Then $rk(A) = rk(\alpha)$.
        
        Hence, $rk(\alpha) = \dim(V)$ if and only if $rk(A) = \dim(V)$, which is exactly when $\det(A) \neq 0$.
    \end{subproof}
\end{proof}
\begin{proposition}[Block Triangular Matrices]
    Let $A \in M_{k \times k}(\F), B \in M{l \times l}(\F)$. Define:
    \begin{equation*}
        C =
        \begin{pmatrix}
            A & (\star) \\
            0 & B
        \end{pmatrix}
        \in M_{n \times m}(\F)
    \end{equation*}
    Where here $(\star)$ shows that anything may sit in the upper-right block.

    Then we conclude $\det(C) = \det(A) \det(B)$.
    \label{propTriangularDet}
\end{proposition}
\begin{proof}
    Note that $c_{ij} = 0$ for all $i \geq k+1$ and $j \leq k$. Therefore, for $\sigma \in S_n$, if
    \begin{equation*}
        \prod_{i=1}^{n} c_{\sigma(i), i} \neq 0
    \end{equation*}
    then we must have that $\sigma$ preserves $X = \{1, \cdots, k\}$.

    Therefore, we also have $\sigma$ preserves $Y = \{k+1, \cdots, n\}$. Now we can write $\sigma = \tau \cdot \theta$ where $\tau \in \operatorname{Sym}(X)$ and $\theta \in \operatorname{Sym}(Y)$.

    [NOTE: What we really mean here is that $\tau$ fixes all elements of $Y$ and applies some $\tau' \in \operatorname{Sym}(X)$ for each element of $X$. Similar for $\theta$.]

    Conversely, given $\tau \in \operatorname{Sym}(X)$ and $\theta \in \operatorname{Sym}(Y)$ we can write $\sigma \in S_n$ preserving $X$ and $Y$.

    Moreover, we have:
    \begin{align*}
        \prod_{i=1}^{n} c_{(\tau \cdot \theta)(i), i} &= \left(\prod_{x \in X} C_{(\tau \theta)(x), x}\right)\left(\prod_{y \in Y} C_{(\tau \theta)(y), y}\right) \\
        &= \left(\prod_{x \in X} C_{\tau(x), x}\right)\left(\prod_{y \in Y} C_{\theta(y), y}\right) \\
    \end{align*}
    \begin{align*}
        \det(C) &= \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^{n} c_{\sigma(i), i}\\
        &= \sum_{\substack{\tau \in \operatorname{Sym(X)}\\\theta \in \operatorname{Sym(Y)}}} \sgn(\tau\theta) \prod_{i=1}^{n} c_{\tau\theta(i), i} \\
        &= \sum_{\tau, \theta} \sgn{\tau\theta} \left(\prod_{x \in X}C_{\tau(x), x}\right)\left(\prod_{y\in Y} c_{\theta(y), y}\right) \\
        &= \sum_\tau \sum_\theta \sgn(\tau) \sgn(\theta) \left(\prod_{x \in X}C_{\tau(x), x}\right)\left(\prod_{y\in Y} c_{\theta(y), y}\right) \\
        &= \left(\sum_\tau \sgn{\tau} \prod_{x=1}^kC_{\tau(x), x}\right)\left(\sum_\theta \sgn{\theta} \prod_{y = k+1}^n c_{\theta(y), y}\right) \\
        &= \det(A) \det(B)
    \end{align*}
\end{proof}
\begin{corollary}
    If $A$ has the following form:
    \begin{equation*}
        C =
        \begin{pmatrix}
            A_1 & (\star) & (\star) & \cdots & (\star) \\
            0 & A_2 & (\star) & \cdots & (\star) \\
            0 & 0 & \ddots & & \vdots \\
            \vdots & \vdots & & A_{k-1} & (\star) \\
            0 & 0 & \cdots & 0 & A_k
        \end{pmatrix}                
    \end{equation*}
    We have:
    \begin{equation*}
        \det(A) = \prod_{i=1}^{k} \det(A_i)
    \end{equation*}
    \label{corMultiBlockDet}
\end{corollary}
\begin{proof}
    The proof is inductive, by considering the matrix $B$ as containing $A_2$ to $A_k$ and applying the theorem on $C$ and $B$ $k-1$ times.
\end{proof}
\begin{definition}{Minor matrix}
    For $A \in M_{n \times n}(\F)$, the \underline{$(i, j)$-minor matrix} is:
    \begin{equation*}
        \mnr{A}{i}{j} \in M_{n-1, n-1} (\F)
    \end{equation*}
    This is obtained from $A$ by deleting the $i$th row and the $j$th column.
\end{definition}
\begin{lemma}[Laplace Expansion along a column or row]
    \begin{enumerate}
        \item For any fixed $1 \leq i \leq n$,
            \begin{equation*}
                \det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \det(\mnr{A}{i}{j})
            \end{equation*}
        \item For any fixed $1 \leq j \leq n$,
            \begin{equation*}
                \det(A) = \sum_{i=1}^{n} (-1)^{i+j} a_{ij} \det(\mnr{A}{i}{j})
            \end{equation*}
    \end{enumerate}
    \label{lemLaplaceOnRowCol}
\end{lemma}
\begin{proof}
    If we can show the row expansion, we can show the column expansion easily using $\det(A) = \det(A^T)$.

    Recall that the row-swap elementary matrix $T_{ij}$ has determinant $-1$. Let $A$ have columns $(\vec{c_1}, \cdots, \vec{c_n})$. By linearity in column $j$:
    \begin{align*}
        \det(A) &= \sum_{i=1}^{n} a_{ij} \det\left((\vec{c_1} \vline \cdots \vline \vec{c_{j-1}} \vline \vec{e_i} \vline \vec{c_{j+1}} \vline \cdots \vline \vec{c_n})\right) \\
        &= \sum_{i=1}^{n} a_{ij} \det(A^{(i)})
    \end{align*}
    Therefore it suffices to find $\det(A^{(i)})$. Apply $j-1$ elementary column operations $T_{k, k+1}$ to move the $j$th column all the way left. Obtain:
    \begin{equation*}
        \det(A^{(i)}) = (-1)^{j-1} \det((\vec{e_i} \vline \vec{c_1} \vline \cdots \vline \vec{c_{j-1}} \vline \vec{c_{j+1}} \vline \cdots \vline \vec{c_n}))
    \end{equation*}
    Then apply $i-1$ elementary row operations to move the $i$th row to the top (so there is a 1 at the top-left element)
    \begin{equation*}
        \det(A^{(i)}) = (-1)^{i-1} (-1)^{j-1} \det\left(\begin{pmatrix}
            1 & (\star) \\
            0 & \mnr{A}{i}{j}
        \end{pmatrix}
        \right)
    \end{equation*}
    By proposition~\ref{propTriangularDet}, this is $(-1)^{i+j} \det(\mnr{A}{i}{j})$.
\end{proof}
\begin{definition}{Adjugate matrix}
    The \underline{adjugate matrix} $\adj(A)$ of $A$ is given by:
    \begin{equation*}
        \adj(A) = \left((-1)^{i+j} \det(\mnr{A}{j}{i})\right)_{ij}
    \end{equation*}
\end{definition}
\begin{theorem}[Cramer's Rule]
    \begin{enumerate}
        \item $\adj(A) \cdot A = A \cdot \adj(A) = \det(A) I_n$.
        \item If $A$ is invertible then:
            \begin{equation*}
                A^{-1} = \frac{\adj(A)}{\det(A)}
            \end{equation*}
    \end{enumerate}
    \label{thmCramerRule}
\end{theorem}
\begin{proof}
    The second statement is immediate from the first because inverses are unique.

    For the first statement, consider a Laplace expansion about the $j$th column:
    \begin{align*}
        \det(A) &= \sum_i (\adj(A))_{j, i} a_{ij} \\
        &= \left[\adj(A) A\right]_{i, j}
    \end{align*}
    Then for $j \neq k$, replace the $j$th column of $A$ by the $k$th column to obtain $A'$.
    \begin{align*}
        0 &= \det(A')\\
        &= \sum_{i} \adj(A) a_{jk} \text{ by expanding along } j\text{th column} \\
        &= \left[\adj(A) A\right]_{jk}
    \end{align*}

    We can use the same process for $A \adj(A)$ by expanding on a row.
\end{proof}
\section{Trace}
\begin{definition}{Trace}
    For $A \in M_{n \times n}(\F)$, the \underline{trace} of $A$ is given by:
    \begin{equation*}
        \tr(A) = \sum_{i=1}^n a_{i, i}
    \end{equation*}
\end{definition}
\begin{remark}
    Trace is a linear operation on matrices.
\end{remark}
\begin{lemma}
    For all $A, B \in M_{n \times n}(\F)$,
    \begin{equation*}
        \tr(AB) = \tr(BA)
    \end{equation*}
    \label{lemTrCommute}
\end{lemma}
\begin{proof}
    \begin{align*}
        \tr(AB) &= \sum_{i=1}^{n} (AB)_{i, i} \\
        &= \sum_{i=1}^{n} \sum_{j=1}^{n} a_{i, j} b_{j, i} \\
        &= \sum_{j=1}^{n} \sum_{i=1}^{n} b_{j, i} a_{i, j} \\
        &= \sum_{j=1}^{n} (BA)_{j, j} \\
        &= \tr(BA)
    \end{align*}
\end{proof}
\begin{remark}
    In general, we do not have the stronger property that $\tr(AB) = \tr(A) \tr(B)$.
\end{remark}
\begin{corollary}
    Similar matrices have the same trace
    \label{corSimilarSameTr}
\end{corollary}
\begin{proof}
    Let $P \in GL_{n}(\F)$. Then:
    \begin{align*}
        \tr(PAP^{-1}) &= \tr(P^{-1} (PA)) \\
        &= \tr(IA) \\
        &= \tr(A)
    \end{align*}
\end{proof}
\begin{definition}{Trace of a linear map}
    For $V$ a finite-dimensional vector space, and $\alpha \in \L(V, V)$, define the \underline{Trace of $\alpha$} by:
    \begin{equation*}
        \tr(\alpha) = \tr([\alpha]_B^B)
    \end{equation*}
    Where $B$ is any basis for $V$.
\end{definition}
\begin{proposition}
    The trace of a linear map $\alpha$ is independent of the basis chosen for $V$.
    \label{propLMTraceWellDef}
\end{proposition}
\begin{proof}
    If $B'$ is another basis for $V$, $[\alpha]_{B'}^{B'}$ is similar to $[\alpha]_B^B$, and therefore they have the same trace by corollary~\ref{corSimilarSameTr}
\end{proof}
\end{document}