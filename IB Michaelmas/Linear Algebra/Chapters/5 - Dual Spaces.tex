\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Basics of Dual Spaces}
\subsection{Definition and Examples}
\begin{definition}{Dual Space}
    Let $V$ be a vector space over a field $\F$. Then the \underline{dual space} of $V$ is:
    \begin{equation*}
        V^* = \L(V, \F)
    \end{equation*}
    An element $\theta \in V^*$ is sometimes known as a \underline{linear functional} on $V$.
\end{definition}
\begin{examples}{}
    \item Consider $\theta : \R^3 \mapsto \R$ given by $\theta(\vec{x}) = x - 2y + 3z$.
    \item Note that the trace function is in the dual space $M_{n \times n}(\F)^*$
    \item Consider $V$ the set of functions in $C([0, 1])$. Then consider:
        \begin{align*}
            \theta : C([0, 1]) &\mapsto \R\\
            f &\mapsto \int_{0}^{1} e^{-t}f(t) dt 
        \end{align*}
        Then $\theta \in V^*$. We can now more clearly see why $\theta$ is a \underline{linear functional} of $C([0, 1])$.
\end{examples}
\subsection{Connection with Bases}
Suppose $V$ is a vector space over a field $\F$. Suppose $B$ is a basis of $V$. For $b \in B$, define $b^* \in V^*$ such that:
\begin{equation*}
    b^* \left(\sum_{c \in B}' \lambda_i c\right) = \lambda_b,~b^*(c) = \delta_{b, c}
\end{equation*}
Then we can define $B^* = \subsetselect{b^*}{b \in B}$.
\begin{propositions}{
        Suppose $V, B, B^*$ are defined as above.
        \label{propsDualBasis}
    }
    \item $B^*$ is linearly independent.
    \item If $V$ is finite-dimensional, then $B^*$ is a basis for $V^*$
\end{propositions}
\begin{proof}
    First suppose $\sum_{b \in B}' \lambda_b b^* = \zv$ in $V^*$. Then for $c \in B$,
    \begin{equation*}
        0 = \left(\sum_{b \in B} \lambda_b b^*\right)(c) = \sum_{b \in B}' \lambda_i b^*(c) = \lambda_c
    \end{equation*}
    and therefore we must have $c = 0$.

    For the second claim, we can give a short proof and a constructive proof.

    By theorem~\ref{findthereference}, for $V, W$ finite-dimensional,
    \begin{equation*}
        \dim(\L(V, W)) = \dim(V) \dim(W)
    \end{equation*}
    \begin{align*}
        \dim(V^*) &=\dim(\L(V, \F)) \\
        &= \dim(V)
    \end{align*}
    Also $B^*$ is a linearly independent subset of $V^*$ with size $\dim(V) = \dim(V^*)$, hence $B^*$ is a basis for $V$.

    We can provide a constructive proof:
    
    Given $\theta \in V^*$ and $b \in B$, set $\lambda_b = \theta(b) \in \F$ and take:
    \begin{equation*}
        \theta = \sum_{b \in B}; \lambda_b b^* \in V^*
    \end{equation*}
    then $\bar{\theta} \in \spn{B^*}$ and for $c \in B$,
    \begin{equation*}
        \bar{\theta}(c) = \lambda_c = \theta(c)
    \end{equation*}
    hence $\theta = \bar{\theta}$ since they agree on the basis vectors, and $\theta \in \spn{B^*}$.
\end{proof}
\begin{definition}{Dual basis}
    The \underline{dual basis} of a finite-dimensional vector space $V$ with basis $B$ is given as above.
\end{definition}
\begin{example}[Dual bases are more complicated in infinite dimensions]
    Consider the space of polynomial functions $\P(\R)$. Then we can show that the dual space is isomorphic to $\R^\N$, but we showed on the first example sheet that these are not isomorphic, in contrast to the finite-dimensional case.
\end{example}
\begin{corollary}
    For $V$ a finite-dimensional vector space over $\F$, $V \isom V^*$.
    \label{corDualBasisIsom}
\end{corollary}
\begin{proof}
    By proposition~\ref{propsDualBasis}, $\dim(V) = \dim(V^*)$. Note $V$ is finite dimensional so we can form a basis $B = \{v_1, \cdots, v_n\}$.

    For $v \in V, \theta \in V^*$, write:
    \begin{equation*}
        v = \sum_{i=1}^{n} \lambda_i v_i, ~~\theta = \sum_{j=1}^{n} \mu_j v_j^*
    \end{equation*}
    Then $\theta(v)$ is given by:
    \begin{align*}
        \theta(v) &= \sum_{i, j = 1}^{n} \lambda_i \mu_i v_{j}^*(v_i) \\
        &= \sum_{i=1}^n \lambda_i \mu_i \\
        &= \left([\theta]_{B^*}\right)^T [v_B]
    \end{align*}
\end{proof}
\subsection{Annihilators}
\begin{definition}{Annihilator}
    For $V$ a finite-dimensional vector space over $\F$, $S \subseteq V$, the \underline{annihilator} of $S$ is:
    \begin{equation*}
        S^0 = \subsetselect{\theta \in V^*}{\forall s \in S, \theta(s) = 0} \subseteq V^*
    \end{equation*}
\end{definition}
\begin{propositions}{
        Let $V$ be a finite-dimensional vector space over $\F$. Let $S, T \subseteq V$.
        \label{propsAnnihilator}
    }
    \item $S^0 \leq V^*$ \label{propAnnihilatorIsSubspace}
    \item If $S \subseteq T$ then $T^0 \leq S^0$ \label{propAnnihilatorInclReverse}
    \item $S^0 = \spn{S}^0$ \label{propAnnihilatorSpan}
    \item $V^0 = \{\zv_{V^*}\}$ and $\{\zv_V\}^0 = V^*$. \label{propAnnihilatorExtremes}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item We can simply apply the subspace test in this case.
        \item By the first part, it suffices to check $T^0 \subseteq S^0$. For $\theta \in T^0, s \in S$, we have that $s \in T$ and so $\theta(s) = 0$.
        \item Note that $S \subseteq \spn{S}$ and by the second part, we have immediately that $\spn{S}^0 \leq S^0$.
        
            Then suppose $\theta \in S^0, v \in \spn{S}$. Write $v = \sum_{s \in S}' \lambda_s s$, so:
            \begin{equation*}
                \theta(v) = \sum_{s \in S}' \lambda_s \theta(s) = 0
            \end{equation*}
            Therefore, $\theta \in \spn{S}^0$.
        \item If $\theta \in V^*$ and $\theta$ sends all elements to zero, $\theta$ must be the zero functional.
        
        For any $\theta \in V^*$ $\theta(\zv_V) = 0$ so $\theta \in \{\zv_V\}^0$.
    \end{enumerate}
\end{proof}
\begin{proposition}
    For $V$ finite-dimensional, and $U \leq V$,
    \begin{equation*}
        \dim(V) = \dim(U) + \dim(U^0)
    \end{equation*}
    \label{propAnnihilatorDim}
\end{proposition}
\begin{proof}
    Let $\dim(V) = n, \dim(U) = k$.
    Let $B_U = \{v_1, \cdots, v_k\}$ be a basis for $U$. Extend this basis to $B_V = \{v_1, \cdots, v_n\}$.

    Then $B_V^* = \{v_1^*, \cdots, v_n^*\}$ is a basis for $V^*$.
    \begin{subproof}{$\{v_{k+1}^*, \cdots, v_n^*\}$ forms a basis for $U^0$}
        First we show that this is a subset of $U^0$. For $i \leq k$ and $j \geq k+1$, $v_j^*(v_i) = 0$ and so:
        \begin{align*}
            v_j^* \in (B_U)^0 &= \spn{B_U}^0 \\
            &= U^0
        \end{align*}
        We have that this is also certainly linearly independent because it is a subset of a basis. The final thing to check is that this spans $U^0$. For this, let $\theta \in U^0$. Write it as a linear combination:
        \begin{equation*}
            \theta = \sum_{j=1}^n \lambda_j v_j^*
        \end{equation*}
        For $i \leq k, v_i \in U$,
        \begin{equation*}
            0 = \theta(v_i) = \sum_{j=1}^{n} \lambda_j v_j^*(v_i) = \lambda_i
        \end{equation*}
        Therefore, $\theta = \sum_{j = k + 1}^{n} \lambda_i v_j^* \in \spn{v_{k+1}^*, \cdots, v_n^*}$.
    \end{subproof}
\end{proof}
\begin{remark}
    For an infinite-dimensional vector space $V$, we can use the idea of direct sums to show that if $U, W \leq V$ satisfy $V = U \oplus W$ then $U^0 \isom W^*$.
\end{remark}
\begin{propositions}{
        Suppose $V$ is a vector space over $\F$. Let $U, W \subseteq V$.
        \label{propsAnnihilatorSubsets}
    }
    \item $U^0 \cap W^0 = (U + W)^0$;
    \item $U^0 + W^0 \leq (U \cap W)^0$;
    \item If $V$ is finite-dimensional then we have equality in both cases.
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item For $\theta \in V^*$, $\theta \in (U + W)^0$ implies that for all $u \in U, w \in W$, $\theta(u + w) = 0$. This is equivalent to $\theta(u) = \theta(w) = 0$, so $\theta \in U^0 \cap W^0$. Conversely, if $\theta$ annihilates everything in $U$ and $W$ then it must annihilate everything in $(U + W)$ by linearity.
        \item $U \cap W \leq U, W$, so by proposition~\ref{propAnnihilatorInclReverse},
            \begin{align*}
                U^0, W^0 & \leq (U\cap W)^0 \\
                U^0 + W^0 &\leq (U \cap W)^0
            \end{align*}
        \item Now let $\dim(V) = n$. Then:
            \begin{align*}
                \dim(U^0 + W^0) &= \dim(U^0) + \dim(W^0) - \dim(U^0 \cap W^0) \\
                &= \dim(U^0) + \dim(W^0) - \dim((U + W)^0) \\
                &= (n- \dim(U)) + (n - \dim(W)) - (n - \dim(U + W)) \\
                &= n - \dim(U \cap W) \\
                &= \dim((U \cap W)^0)
            \end{align*}
    \end{enumerate}
\end{proof}
\section{Dual Linear Maps}
\begin{definition}{Dual map}
    Let $\alpha \in \L(V, W)$ for vector spaces $V, W$ over $\F$. The \underline{dual map} of $\alpha$ is given by:
    \begin{align*}
        \alpha^* : W^* &\mapsto V^*\\
        \theta &\mapsto \theta \circ \alpha
    \end{align*}
\end{definition}
\begin{propositions}{
    If $\alpha, \beta \in \L(V, W)$, $\gamma \in \L(U, V), \lambda \in \F$, then:
        \label{propsDLM}
    }
    \item $\alpha^*$ is linear. \label{propDLMLinear}
    \item $(\alpha + \lambda \beta)^* = \alpha^* + \lambda\beta^*$ \label{propDLMLinearOp}
    \item $(Id_V)^* = Id_{V^*}$ \label{propDLMId}
    \item If $\beta$ is an isomorphism then so is $\beta^*$ with inverse:
        \begin{equation*}
            (\beta^*)^{-1} = (\beta^{-1})^*
        \end{equation*}
        \label{propDLMIsomPreserved}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Let $\theta, \eta \in W^*$, $\mu \in \F$. Then for $v \in V$, consider $\alpha^*(\theta + \mu \eta)$.
            \begin{align*}
                \alpha^*(\theta + \mu \eta)(v) &= (\theta + \mu \eta)(\alpha(v)) \\
                &= \theta(\alpha(v)) + \mu \eta(\alpha(v)) \\
                &= \alpha^*(\theta)(v) + \mu \alpha^*(\eta)(v) \\
                &= \left[\alpha^*(\theta) + \mu \alpha^*(\eta)\right](v)
            \end{align*}
            and since $v$ was arbitrary, it is the case that:
            \begin{equation*}
                \alpha^*(\theta + \mu \eta) = \alpha^*(\theta) + \mu \alpha^*(\eta)
            \end{equation*}
        \item For $\theta \in W^*$, $v \in V$,
            \begin{align*}
                (\alpha + \lambda \beta)*^(\theta)(v) &= \theta(\alpha + \lambda \beta)(v) \\
                &= \theta(\alpha(v) + \lambda \beta(v)) \\
                &= \theta(\alpha(v)) + \lambda\theta(\beta(v)) \\
                &= \alpha^*(\theta)(v) + \lambda\beta^*(\theta)(v) \\
                &= (\alpha^*(\theta) + \lambda \beta^*(\theta))(v)
            \end{align*}
            Since $v, \theta$ were arbitrary, we must have that:
            \begin{equation*}
                (\alpha + \lambda \beta)^* = \alpha^* + \lambda \beta^*
            \end{equation*}
        \item Consider $\theta \in W^*$.
            \begin{align*}
                (\alpha \circ \gamma)^*(\theta) &= \theta \circ (\alpha \circ \gamma) \\
                &= \alpha^*(\theta) \circ \gamma \\
                &= \gamma^*(\alpha^*(\theta)) \\
                &= (\gamma^* \circ \alpha^*)(\theta)
            \end{align*}
            We again conclude that, since $\theta$ was arbitrary,
            \begin{equation*}
                (\alpha \circ \gamma)^* = \gamma^* \circ \alpha^*
            \end{equation*}
        \item For all $\theta \in V^*$, $(Id_V)^*(\theta) = \theta \circ Id_V = \theta$.
        \item Now consider:
            \begin{align*}
                Id_{V^*} &= (\beta^{-1} \circ \beta) \\
                &= \beta^* + (\beta^{-1})^*
            \end{align*}
            Similarly, $id_{W^*} = (\beta^{-1})^* \circ \beta^*$.
    \end{enumerate}
\end{proof}
\begin{proposition}
    Let $V, W$ be finite-dimensional vector space over $\F$. Let $\alpha \in \L(V, W)$.

    Consider $B, C$ bases for $V, W$. Then:
    \begin{equation*}
        [\alpha^*]_{B^*}^{C^*} = \left([\alpha]_C^B\right)^T
    \end{equation*}
    \label{propDLMIsTranspose}
\end{proposition}
\begin{proof}
    Let $n = \dim(V), m = \dim(W)$. Let $B = \{b_1, \cdots, b_n\}$ and $C = \{c_1, \cdots, c_m\}$. Define:
    \begin{align*}
        A &= [\alpha]_C^B,~~\alpha(b_i) = \sum_{k=1}^{m} a_{ki} c_k \\
        A' &= [\alpha^*]_{B^*}^{C^*},~~\alpha(c_j^*) = \sum_{l=1}^{n}a_{lj}' b_l^*
    \end{align*}
    Then we find:
    \begin{align*}
        a_{ij}' &=\alpha^*(c_j^*)(b_i) \\
        &= c_j^*(\alpha(b_i)) \\
        &= c_j^* \left(\sum_{k=1}^{m} a_{ki} c_k\right) \\
        &= \alpha_{ji}
    \end{align*}
\end{proof}
\begin{example}[Change-of-Basis]
    Let $B, C$ be bases of $V$. Set $P = [Id_V]_C^B$. Then $[Id_{V^*}]_{B^*}^{C^*} = P^T$.

    Then $[Id_{V^*}]_{C^*}^{B^*} = (P^T)^{-1}$.
\end{example}
\begin{corollary}
    Consider the same assumptions as proposition~\ref{propDLMIsTranspose}.

    \begin{equation*}
        rk(\alpha^*) = rk(\alpha)
    \end{equation*}
    \label{corDLMRank}
\end{corollary}
\begin{proof}
    The ranks of $\alpha, \alpha^*$ are the column ranks of their corresponding matrices. Since these are transposes of each other, and since column rank is equal to row rank, we have that the linear maps must have the same rank.
\end{proof}
\begin{propositions}{
        Let $V, W$ be vector spaces over a field $\F$. For $\alpha \in \L(V, W)$:
        \label{propsDLMImKer}
    }
    \item $\ker(\alpha^*) = \im(\alpha)^0$ \label{propDLMKer} \\
    \item $\im(\alpha^*) \leq \ker(\alpha)^0$ \label{propDLMIm} \\
    \item If $V$ and $W$ are finite-dimensional then we have equality in the above. \label{propDLMFinteIm}
\end{propositions}
\begin{proof}
    \begin{enumerate}
        \item Let $\theta \in W^*$. Observe the following chain of equivalence:
            \begin{align*}
                \theta \in \ker(\alpha^*) &\iff \theta \circ \alpha = \zv_{V^*} \\
                &\iff \forall v \in V, \theta(\alpha(v)) = \zv_V \\
                &\iff \theta \in \im(\alpha)^0
            \end{align*}
        \item Now consider $\eta \in V^*$. Then if $\eta \in \im(\alpha^*)$ there exists $\theta \in W^*$ such that $\eta = \theta \circ \alpha$. Therefore, for all $v \in \kera$,
            \begin{align*}
                \eta(v) &= \theta(\alpha(v)) \\
                &= \theta(\zv_V) \\
                &= \zv_{V^*} \\
                &\implies (\kera)^0
            \end{align*}
        \item By corollary~\ref{corDLMRank},
            \begin{align*}
                rk(\alpha^*) &= rk(\alpha) \\
                &= \dim(V) - \dim(\kera) \\
                &= \dim(\kera^0)
            \end{align*}
    \end{enumerate}
\end{proof}
\section{The Double Dual}
We now consider the dual space of the dual space, $V^{**} = (V^*)^*$.
\begin{theorem}[Double dual isomorphism]
    Let $V$ be a vector space over $\F$. Then there exists a linear map $E : V \mapsto V^{**}$ given by $E(v)(\theta) = \theta(v)$.
    
    If $V$ is finite-dimensional then this is an isomorphism.
    \label{thmDDLMIsom}
\end{theorem}
\begin{proof}
    \begin{subproof}{$E$ is linear}
        Let $v, w \in V, \lambda \in \F, \theta \in V^*$.
        \begin{align*}
            E(v + \lambda w)(\theta) &= \theta(v + \lambda w) \\
            &= \theta(v) + \lambda \theta(w) \\
            &= E (v)(\theta) + \lambda E(w)\theta \\
            &= \left(E(v) + \lambda E(w)\right)(\theta)
        \end{align*}
        Then this is true for all $\theta$, so $E$ must be linear.
    \end{subproof}
    Now suppose $V$ is finite-dimensional.
    \begin{subproof}{$E$ is injective.}
        Suppose $v$ is a non-zero vector in $\ker(E)$. Therefore, for all $\theta \in V^*$, $E(v)(\theta) = 0$. Consider a basis $B$ of $V$ with first element $v$. Let $B^*$ be the usual dual basis of $B$. Now set $\theta = v_1^*$. Then $0 = \theta(v) = v_1^*(v_1) = 1$.

        That is, $1 = 0$. \contradiction
    \end{subproof}
    Surjectivity follows from the rank-nullity theorem because:
    \begin{equation*}
        \dim(V) = \dim(V^*) = \dim(V^{**})
    \end{equation*}
\end{proof}
\begin{remark}
    We are able to show injectivity in the infinite-dimensional case if we assume the axiom of choice. However, surjectivity does not hold in general.
\end{remark}
\end{document}