\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Vector Space of Linear Maps}
\begin{definition}{Linear Map Space}
    Let $V$ and $W$ be vector spaces. Then the \underline{linear map space} $\L(V, W)$ is the set:
    \begin{equation*}
        \L(V, W) = \subsetselect{\alpha : V \mapsto W}{\alpha \text{ linear}}
    \end{equation*}
\end{definition}
Then we can define addition as $(\alpha + \beta)(v) = \alpha(v) + \beta(v)$, and scalar multiplication as $(\lambda \alpha)(v) = \lambda (\alpha(v))$. Then $\L(V, W)$ is a vector space under these operations.
\subsection{Facts About Matrices}
Let $M_{m \times n}(\F)$ be the space of $m \times n$ matrices with elements in $\F$. For $A \in M_{m \times n}(\F)$, we write $A = (a_{ij})_{i, j}$ where $a_{ij} \in \F$ are the elements of $A$, with $1 \leq i \leq m, 1 \leq j \leq n$.

In this section, require that bases are ordered. Let $B = \{v_1, \cdots, v_n\}$ be a basis for $V$, and let $C = \{w_1, \cdots, w_m\}$ be a basis for $W$.

Now consider $\alpha \in M_{m \times n}(\F)$. Write:
\begin{align}
    \alpha(v_1) &= a_{11}w_1 + a_{21}w_2 + \cdots + a_{m1}w_m \nonumber \\
    \alpha(v_2) &= a_{12}w_1 + a_{22}w_2 + \cdots + a_{m2}w_m \label{eqnLinMapMatrix} \\
    \vdots~&~~~~~\vdots \nonumber \\
    \alpha(v_n) &= a_{1n}w_1 + a_{2n}w_2 + \cdots + a_{mn}w_m \nonumber
\end{align}
\begin{definition}{Matrix of a linear map}
    The \underline{matrix of a linear map} $\alpha$ with respect to the ordered bases $B, C$ as above defined is:
    \begin{equation*}
        [\alpha]_C^B = (\alpha_{ij})_{ij} \in M_{m \times n}(\F)
    \end{equation*}
\end{definition}
\begin{warning}
    The rows of the array given in equation~\ref{eqnLinMapMatrix} becomes the columns of $[\alpha]_{C}^B$.
\end{warning}
\subsection{Linking Matrices and Linear Maps}
As for why we have mixed up the rows and columns, recall that there exists a linear isomorphism from a vector to its coordinate vector:
\begin{align*}
    E_B : V &\mapsto \F^n\\
    v = \sum_{i=1}^{n}\lambda_i v_i &\mapsto (\lambda_i)_i = [v]_B
\end{align*}
then we have the following theorem:
\begin{theorem}
    Let $V, W$ be finite-dimensional vector spaces with dimension $n$ and $m$ respectively. Let $\alpha : V \mapsto W$ be a linear map.
    \begin{enumerate}
        \item For all $v \in V$, $[\alpha]_C^B [v]_B = [\alpha(v)]_C$ \label{thmPartLMMat}
        \item $[\alpha]_C^B$ is the only matrix in $M_{m \times n}(\F)$ satisfying part~\ref{thmPartLMMat}. \label{thmPartLMMatUnique}
        \item There is an isomorphism:
            \begin{align*}
                E_C^B : \L(V, W) &\mapsto M_{m \times n}(\F)\\
                \alpha &\mapsto [\alpha]_C^B
            \end{align*}
            \label{thmPartLMIsom}
    \end{enumerate}
    \label{thmMatrixOfLinMap}
\end{theorem}

\begin{proof}
    For part~\ref{thmPartLMMat}, let $v \in V$. Write $v = \sum_{j=1}^n \lambda_i v_i$. Then:
    \begin{align*}
        \alpha(v) &= \sum_{j=1}^{n}\lambda_i \alpha(v_i) \\
        &= \sum_{j=1}^{n}\lambda_i \sum_{i=1}^{m} a_{ij} w_i \\
        &= \sum_{i=1}^{m} \left(\sum_{j=1}^{n}\lambda_i a_{ij}\right)w_i \\
        \therefore [\alpha(v)]_C &= \left(\sum_{j=1}^{n}a_{ij}\lambda_j\right)_i \\
        &= [(a_{ij})(\lambda_j)]_i \\
        &= [\alpha]_C^B [v]_B
    \end{align*}

    Now for part~\ref{thmPartLMMatUnique}, consider $A \in M_{m \times n}(\F)$. Let $[v_j]_B = e_j$ (standard basis). Therefore, $A[v_j]_B$ is the first column of $A$. But,
    \begin{align*}
        A[v_j]_B &= [\alpha(v_i)]_C \\
        &= [\alpha]_C^B [v_i]_B \\
        &= [\alpha]_C^B e_i
    \end{align*}
    thus all the columns are the same, so $A = [\alpha]_C^B$.

    For part~\ref{thmPartLMIsom}, we set up the isomorphism as described.
    For linearity, let $\alpha, \beta \in \L(V, W), \lambda \in \F$. Then:
    \begin{align*}
        [\alpha + \lambda\beta]_C^B [v]_B &= [(\alpha + \lambda\beta)(v)]_C \\
        &= [\alpha(v) + \lambda\beta(v)]_C \\
        &= [\alpha(v)]_C + \lambda[\beta(v)]_C \text{ by linearity of } E_C \\
        &= \left([\alpha]_C^B + \lambda[\beta]_C^B\right)[v]_B \text{ by part~\ref{thmPartLMMat}} 
    \end{align*}
    for all $v \in V$, and therefore we have linearity by part~\ref{thmPartLMMatUnique}.

    Now to prove injectivity, let $\alpha \in \ker(E_C^B)$. That is, $[\alpha]_B^C = 0_{m \times n}$. Then by part~\ref{thmPartLMMat}, for all $v \in V$ $[\alpha(v)]_C = \zv$. But $E_C$ is an isomorphism so $\alpha(v) = \zv$ for all $v \in V$, that is, $\alpha$ is the zero map.

    Now let $A \in M_{m \times n}(\F)$. Define:
    \begin{align*}
        f : B &\mapsto W\\
        v_i &\mapsto \sum_{j=1}^{m}a_{ij}w_i
    \end{align*}
    and extend $f$ to a linear map $F$ by proposition~\ref{propBasisExtension}. Then $[F]_B^C = A$.
\end{proof}
\begin{remark}
    We can produce two maps $V \mapsto \F^m$. Via $W$, $\alpha : V \mapsto W$ and then $E_C : W \mapsto \F^m$. Via $\F^n$, $E_B : V \mapsto \F^n$ and then $[\alpha]_B^C : \F^n \mapsto \F^m$. This is a \underline{commutative diagram}.
\end{remark}
\section{Properties of Matrices}
\subsection{Change-of-Basis}
\begin{proposition}
    Let $V, W, X$ be finite-dimensional vector spaces with bases $B, C, D$ respectively. Let $\alpha \in \L(V,W), \beta \in \L(W, X)$. Then:
    \begin{equation*}
        [\beta \circ\alpha]_D^B =[\beta]_D^C [\alpha]_C^B
    \end{equation*}
    \label{propMatrixCompose}
\end{proposition}
\begin{proof}
    By theorem~\ref{thmMatrixOfLinMap}, $[\beta\circ\alpha]_C^B$ is the unique matrix satisfying:
    \begin{equation*}
        A[v]_B = [\beta(\alpha(v))]_D, v \in V
    \end{equation*}
    but:
    \begin{equation*}
        [\beta]_D^C[\alpha]_C^B[v]_B = [\beta]_D^C[\alpha(v)]_C = [\beta(\alpha(v))]_D
    \end{equation*}
    so by uniqueness of this matrix, they are equal.
\end{proof}
\begin{remark}
    For any basis $B$ of $V$, $[Id_V]_B^B = I_n$, the identity matrix of size $n\times n$.
\end{remark}
\begin{definition}{Change-of-basis matrix}
    For a vector space $V$ with dimension $n$, and bases $B, B'$, the \underline{change-of-basis matrix} from $B$ to $B'$ is given by:
    \begin{equation*}
        P = [Id_V]_{B'}^B \in M_{n \times n}(\F)
    \end{equation*}
\end{definition}
We also can find the elements of $P$, if $B = \{v_1, \cdots, v_n\}$ and $B' = \{v_1', \cdots, v_n'\}$,
\begin{equation*}
    v_j = \sum_{i=1}^{n}p_{ij} v_i'
\end{equation*}
and so the $j$th column of $P$ is $[v_j]_B'$.

Recall that $A \in M_{m \times n}(\F)$ is \underline{invertible} if there exists $A^{-1} \in M_{m \times n}(\F)$ such that $A A^{-1} = A^{-1} A = I_n$. Then $A^{-1}$ is the \underline{inverse} of $A$.

$GL_n(\F)$ denotes the group of invertible matrices of size $n$ over $\F$.
\begin{propositions}{
        Let $V, B, B'$ be defined as above. Let $W$ be a vector space of dimension $m$.
        \label{propsChangeOfBasis}
    }
    \item $[Id_V]_{B'}^B \in GL_n(\F)$ with inverse $[Id_V]_B^{B'}$. \label{propCOBInverse}
    \item If $\alpha \in \L(V, W)$, and $C, C'$ are bases for $W$,
        \begin{equation*}
            [\alpha]_{C'}^{B'} = [Id_W]_{C'}^C [\alpha]_C^B [Id_V]_B^{B'}
        \end{equation*}
\end{propositions}
\begin{proof}
    For the first part, use proposition~\ref{propMatrixCompose} to get:
    \begin{equation*}
        I_n = [Id_V \circ Id_V]_B^B = [Id_V]_B^{B'} [Id_V]_{B'}^B
    \end{equation*}
    so indeed the matrix $[Id_V]_{B'}^B$ is invertible with the required inverse. To show the second part, use proposition~\ref{propMatrixCompose} again.
\end{proof}
\begin{definition}{Equivalent matrices}
    Matrices $A, A' \in M_{m \times n}(\F)$ are \underline{equivalent} if there exist matrices $P \in GL_m(\F), Q \in GL_n(\F)$ such that $A' = PAQ$.
\end{definition}
\begin{remarks}
    \item Equivalent matrices represent the same linear map under different bases of the input/output spaces.
    \item This is indeed an equivalence relation: Inverting $P$ and $Q$ gives that $A$ is equivalent to $A'$. Using the identity matrix gives that $A$ is equivalent to itself, and transitivity holds by composing two matrices either side of $A$.
\end{remarks}
\begin{theorem}
    Let $V$ and $W$ be finite-dimensional vector spaces over $\F$. Let their dimensions be $n, m$ respectively. Let $\alpha : V \mapsto W$ be linear. Let $r = rk(\alpha)$. Then:
    \begin{enumerate}
        \item There exist bases $B, C$ for $V$ and $W$ such that:
            \begin{equation*}
                [\alpha]_C^B =
                \begin{pmatrix}
                    I_r & 0 \\
                    0 & 0
                \end{pmatrix}
            \end{equation*}
        \item If
            \begin{equation*}
                [\alpha]_{C'}^{B'} =
                \begin{pmatrix}
                    I_{r'} & 0 \\
                    0 & 0
                \end{pmatrix}
            \end{equation*}
            for some bases $B', C'$ and some $r'$, then $r' = r$.
    \end{enumerate}
    \label{thmBlockMatDecomp}
\end{theorem}
\begin{proof}
    By theorem~\ref{thmRankNullity}, $n(\alpha) = n-r$. Therefore let $\{v_{r+1}, \cdots, v_n\}$ be a basis for this kernel. Extend to a basis of $V$, $B = \{v_1, \cdots, v_n\}$. Then applying $\alpha$:
    \begin{equation*}
        \alpha(B) = \{\alpha(v_1), \cdots, \alpha(v_r), \zv, \cdots, \zv\}
    \end{equation*}
    and note that the first $r$ vectors must be a basis for $\im(\alpha)$, because there are $r$ linearly independent vectors. Extend the first $r$ vectors of $\alpha(B)$ to a basis of $W$:
    \begin{equation*}
        C = \{w_1 = \alpha(v_1), \cdots, w_r = \alpha(v_r), w_{r+1}, \cdots, w_m\}
    \end{equation*}
    Therefore applying $\alpha$ to $B$:
    \begin{equation*}
        \alpha(v_j) =
        \begin{cases}
            w_j & 1 \leq j \leq r \\
            \zv & \text{otherwise}
        \end{cases}
    \end{equation*}
    and therefore we have that:
    \begin{equation*}
        [\alpha]_C^B =
        \begin{pmatrix}
            I_r & 0 \\
            0 & 0
        \end{pmatrix}
    \end{equation*}

    Now prove uniqueness. Suppose that:
    \begin{equation*}
        [\alpha]_{C'}^{B'} =
        \begin{pmatrix}
            I_{r'} & 0 \\
            0 & 0
        \end{pmatrix}
    \end{equation*}
    and so:
    \begin{equation*}
        \alpha(v_j') =
        \begin{cases}
            w_j' & 1 \leq j \leq r' \\
            \zv & \text{otherwise}
        \end{cases}
    \end{equation*}
    hence $w_1', \cdots, w_{r'}'$ span the image and are linearly independent, so $r = rk(\alpha) = r'$.
\end{proof}
\subsection{Rank of a Matrix}
\begin{definition}{Column space}
    For $A \in M_{m \times n}(\F)$, the \underline{column space} of $A$, $\Col(A)$, is the subspace of $\F^m$ spanned by the columns of $A$. The \underline{column rank} of $A$ is $\crk(A) = \dim(\Col(A))$.
\end{definition}
\begin{definition}{Row space}
    For $A \in M_{m \times n}(\F)$, the \underline{row space} of $A$, $\Row(A)$, is the subspace of $\F^m$ spanned by the transposed rows of $A$. The \underline{row rank} of $A$ is $\rrk(A) = \dim(\Row(A))$.
\end{definition}
The row and column ranks are related by: $\rrk(A) = \crk(A^T)$.
\begin{lemma}
    For $A \in M_{m \times n}(\F)$ and $B \in M_{n \times p}(\F)$, and let these represent linear maps $\alpha, \beta$ in standard basis. Then:
    \begin{equation*}
        rk(\alpha \circ \beta) = \min\{rk(\alpha), rk(\beta)\}
    \end{equation*}
    \label{lemRankOfProduct}
\end{lemma}
\begin{proof}
    $\im(\alpha \circ \beta) \leq \im(\alpha)$, so $rk(\alpha \circ \beta) \leq rk(\alpha)$. If $Bv = \zv$ for $v \in \F^p$, then $ABv = 0$ so $n(\beta) \leq n(\alpha\beta)$. Then by theorem~\ref{thmRankNullity},
    \begin{align*}
        p - rk(\beta) &\leq p - rk(\alpha\beta) \\
        \therefore rk(\alpha\beta) &\leq rk(\beta)
    \end{align*}
\end{proof}
\begin{theorem}
    Let $A, A' \in M_{m \times n}(\F)$.
    \begin{enumerate}
        \item $A$ is equivalent to:
        \begin{equation*}
            \begin{pmatrix}
                I_r & 0 \\
                0 & 0
            \end{pmatrix}
        \end{equation*}
        \item $A$ and $A'$ are equivalent if and only if $\crk(A) = \crk(A')$.
    \end{enumerate}
    \label{thmMatEquivClasses}
\end{theorem}
\begin{proof}
    Again let $\alpha$ represent $A$ in standard basis (let $E_k$ be standard basis for $\F^k$). Now there exist bases $B, C$ of $\F^n, \F^m$ such that:
    \begin{align*}
        \begin{pmatrix}
            I_r & 0 \\
            0 & 0
        \end{pmatrix}
        &= [\alpha]_C^B \\
        &= [Id_{\F^m}]_C^{E_m} [\alpha]_{E_m}^{E_n} [Id_{\F^n}]^{E_n}_B
    \end{align*}
    and this is $PAQ$ for matrices $P, Q$ seen above. Therefore this is equivalent to $A$, with $rk(\alpha) = \crk(A)$.

    For the second part of the theorem, consider $A'$ with column-rank $r$ representing a linear map $\alpha'$ in standard basis. Then $A'$ can be represented as a block-matrix with upper-left identity by the first part. Hence $A'$ is equivalent to $A$. Conversely, if $A' = PAQ$ then by the lemma,
    \begin{equation*}
        rk(\alpha') \leq \crk(AQ) \leq rk(\alpha)
    \end{equation*}
    We can also use the inverse change of basis, $A = P^{-1} A' Q^{-1}$ to get the other side of the inequality, resulting in $rk(\alpha) = rk(\alpha')$.
\end{proof}
\begin{theorem}
    For any $A \in M_{m \times n}(\F)$,
    \begin{equation*}
        \rrk(A) = \crk(A) = rk(A)
    \end{equation*}
    \label{thmMatrixRank}
\end{theorem}
\begin{proof}
    Note that if $P$ is invertible, so is $P^T$ with inverse $(P^{-1})^T$. Now let $\crk(A) = r$. Then there exists $P \in GL_m(\F),~Q \in GL_n(\F)$ such that:
    \begin{equation*}
        PAQ =
        \begin{pmatrix}
            I_r & 0 \\
            0 & 0
        \end{pmatrix}
        \in M_{m \times n}(\F)
    \end{equation*}
    and $A^T$ is equivalent to:
    \begin{equation*}
        Q^TA^TP^T = (PAQ)^T
        \begin{pmatrix}
            I_r & 0 \\
            0 & 0
        \end{pmatrix}
        \in M_{n \times m}(\F)
    \end{equation*}
    (note that these block matrices are different sizes).
    Then by the previous theorem, $\crk(A) = \crk(A^T) = r$.
\end{proof}
Now consider $V$ a finite-dimensional vector space. Let $B, B'$ be bases for $V$, $\alpha : V \mapsto V$ linear. Then:
\begin{align*}
    [\alpha]_{B'}^B &= [Id_V]_{B'}^B [\alpha]_B^B [Id_V]_B^{B'} \\
    &= P^{-1} [\alpha]_B^B P
\end{align*}
\subsection{Matrix Similarity}
\begin{definition}{Similar matrices}
    Matrices $A, A' \in M_{n \times n}(\F)$ are \underline{similar} of there exists $P \in GL_n(\F)$ such that:
    \begin{equation*}
        A' = P^{-1} A P
    \end{equation*}
\end{definition}
\begin{remarks}
    \item Note that similarity is an equivalence relation on $M_{n \times n}(\F)$.
    \item Similar matrices are also equivalent, but equivalent matrices need not be similar.
\end{remarks}
\begin{example}[Equivalence does not imply similarity]
    $I_n$ is equivalent to every matrix in $GL_n(\F)$ but the similarity class is $\{I_n\}$ because any matrices on the left and right will cancel out.
\end{example}
\section{Elementary Operations on Matrices}
Let $\vec{r_1}, \cdots, \vec{r_m}$ be the rows of a matrix $A$. There exist 3 types of \underline{elementary row operations} on $A$:
\begin{enumerate}
    \item Swapping $\vec{r_i}$ and $\vec{r_j}$;
    \item Multiplying $\vec{r_i}$ elementwise by $\lambda$ (scaling).
    \item Adding $\lambda \vec{r_j}$ to $\vec{r_i}$.
\end{enumerate}
\begin{remark}
    By transposing the matrix, applying a row operation, and transposing it back, we have 3 corresponding \underline{elementary column operations}.
\end{remark}
We also have the corresponding \underline{elementary matrices}:
\begin{enumerate}
    \item $T_{i,j}$
    \item $M_{i, \lambda}$
    \item $C_{i, j, \lambda}$
    %TODO: Figure out how to represent these matrices.
\end{enumerate}
\begin{remarks}
    \item These correspond exactly to applying an elementary row operation to the identity matrix.
    \item The elementary row operations are invertible, so the elementary matrices are invertible with inverses:
    \begin{equation*}
        T_{i,j}^{-1} = T_{i,j},~M_{i, \lambda}^{-1} = M_{i, \lambda^{-1}},~C_{i, j, \lambda}^{-1} = C_{i, j, -\lambda}
    \end{equation*}
\end{remarks}
\begin{lemma}
    If $E$ is an elementary matrix, and $A$ is a matrix, $EA$ is the matrix obtained by applying the corresponding row operation encoded by $E$.
    \label{lemMatAndRowOP}
\end{lemma}
\begin{proof}
    The proof is by multiplying the matrices explicitly and noting the row operation is as required.
\end{proof}
\begin{remark}
    Right-multiplying by $E$, encodes an elementary column operation. The one exception is that $C_{i, j, \lambda}$ adds $\lambda$ times column $i$ to column $j$.
\end{remark}
An important property of row and column operations is that they preserve the row and column spaces of the matrix. Therefore, these operations preserve the rank of the matrix.
\begin{definition}{Row-reduced echelon form}
    A matrix $A \in M_{m \times n}(\F)$ is in \underline{row-reduced echelon form} (RRE form) if:
    \begin{enumerate}
        \item All non-zero rows of $A$ appear above all zero rows in $A$.
        \item The leftmost non-zero element of a non-zero row is $1$. This is the \underline{pivot entry} $1 \leq p(\vec{r_i}) \leq n$.
        \item If $\vec{r_i}, \vec{r_j} \neq \zv$, $i < j$, then
            \begin{equation*}
                p(\vec{r_i}) < p(\vec{r_j})
            \end{equation*} 
        \item In a column containing a pivot entry, every other entry is zero.
    \end{enumerate}
\end{definition}
\begin{definition}{Column-reduced echelon form}
    A matrix $A \in M_{n \times m}(\F)$ is in \underline{column-reduced echelon form} (CRE form) if the matrix $A^T$ is in RRE form.
\end{definition}
\begin{lemma}
    If $A$ is in RRE form then $\rrk(A)$ is the number of non-zero rows in $A$.
    \label{lemRRECrk}
\end{lemma}
\begin{proof}
    Let $\vec{r_1}, \cdots, \vec{r_k}$ by the non-zero rows of $A$. Let $j_i = p(\vec{r_i})$ be the pivot entry. Note $j_i$ are strictly increasing.

    Certainly these rows span $\Row(A)$, since all other rows are zero.

    Suppose $\vec{v} = \sum_{i=1}^{k} \lambda_i \vec{r_i} = \zv$. Then $0 = (\vec{v})_{j_i} = \lambda_i$, so the set must be linearly independent. That is, the rows form a basis of $\Row(A)$, and so $k = \rrk(A)$.
\end{proof}
\begin{theorem}
    Every matrix $A \in M_{m \times n}(\F)$ can be put into RRE form.
    \label{thmRREFormation}
\end{theorem}
\begin{proof}
    Let $\{\vec{c_1}, \cdots, \vec{c_n}\}$ be the columns of $A$.
    \induction{$n = 1$}{
        If a matrix has only 1 column, perform a row operation of type 2 on the first element to ensure it is 1, then use row operations of type 3 to remove all the other elements in the matrix, so the column is simply:
        \begin{equation*}
            \begin{pmatrix}
            1 \\ 0 \\ \vdots \\ 0
            \end{pmatrix}
        \end{equation*}
    }{$n = k$}{Suppose a matrix with $k$ columns can be put in RRE form}
    {$n = k+1$}{
        If $\vec{c_1} = 0$, apply the hypothesis to the rest of the columns. Then if $\vec{c_1} \neq \zv$, suppose entry $(i, 1)$ is non-zero. Using a row operation of type 1, we can suppose that entry $(1, 1)$ is non-zero. By scaling (row operation type 2), we can assume that this entry $A_{1, 1} = 1$.
        We can now use row operations of type 3, we can ensure that the rest of the entries in $\vec{c_1}$ are zero.
        
        Finally, via further row operations of type 3, we can ensure that all the elements in the first row are zero.
    }
\end{proof}
\begin{remarks}
    \item Putting a matrix into RRE form preserves the row space.
    \item There is only one RRE form for a given matrix.
    \item By transposing, theorem~\ref{thmRREFormation} applies for CRE form too.
    \item IF $A$ is a square matrix in RRE form, then either $A$ has a zero row or $A$ is the identity matrix.
\end{remarks}
\begin{theorem}
    For $A \in M_{n \times n}(\F)$, the following are equivalent:
    \begin{enumerate}
        \item $rk(A) = n$.
        \item $A$ is a product of elementary matrices
        \item $A$ is invertible.
    \end{enumerate}
    \label{thmInvertibilityCriterion}
\end{theorem}
\begin{proof}
    \begin{subproof}{Assume $A$ has rank $n$}
        By theorem~\ref{thmRREFormation} there exist elementary matrices $E_i$ such that:
        \begin{equation*}
            I_n = E_1 E_2 \cdots E_l A
        \end{equation*}
        and since the elementary matrices are invertible, $A$ is a product of elementary matrices.

        Statement 1 implies statement 2.
    \end{subproof}
    \begin{subproof}{Suppose $A$ is a product of elementary matrices}
        Then the elementary matrices are invertible and so we can find the inverse of a product of invertible matrices to find that $A$ is invertible.
    \end{subproof}
    \begin{subproof}{Suppose $A$ is invertible.}
        For $\vec{v} \in \F^n$, $\alpha$ the linear map represented by $A$,
        \begin{equation*}
            v = (AA^{-1})(v) = A(A^{-1}v)
        \end{equation*}
        so $v \in \im(\alpha)$. That is, $\im(\alpha)$ has $n$ dimensions so $rk(A) = n$.
    \end{subproof}
\end{proof}
We can now provide an alternative proof of theorem~\ref{thmMatEquivClasses}.
\begin{proof}[of theorem~\ref{thmMatEquivClasses}]
    Apply elementary matrices $E_1, \cdots, E_l$ such that $E_1 \cdots E_L A$ is in RRE form. Let the new matrix be $PA$.

    Apply column operations of type 1 to move the pivot columns to the left. Then the result of this, $PAE_1' \cdots E_k'$ has the form:
    \begin{equation*}
        \begin{pmatrix}
            I_r & B \\
            0 & 0
        \end{pmatrix}
    \end{equation*}
    This is good, but we have $B \neq 0$. We can use column operations of type 3 to get rid of all the elements of $B$, and so:
    \begin{equation*}
        PAQ = PAE_1' \cdots E_k' E_1'' \cdots E_j'' =
        \begin{pmatrix}
            I_r & 0 \\
            0 & 0
        \end{pmatrix}
    \end{equation*}
\end{proof}
\begin{remark}
    If $A \in GL_n(\F)$, we can reduce $A$ to $I_m$ using only elementary row operations, or only elementary column operations.
\end{remark}
\end{document}