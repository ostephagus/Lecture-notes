\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Vector Space of Linear Maps}
\begin{definition}{Linear Map Space}
    Let $V$ and $W$ be vector spaces. Then the \underline{linear map space} $\L(V, W)$ is the set:
    \begin{equation*}
        \L(V, W) = \subsetselect{\alpha : V \mapsto W}{\alpha \text{ linear}}
    \end{equation*}
\end{definition}
Then we can define addition as $(\alpha + \beta)(v) = \alpha(v) + \beta(v)$, and scalar multiplication as $(\lambda \alpha)(v) = \lambda (\alpha(v))$. Then $\L(V, W)$ is a vector space under these operations.
\subsection{Facts About Matrices}
Let $M_{m \times n}(\F)$ be the space of $m \times n$ matrices with elements in $\F$. For $A \in M_{m \times n}(\F)$, we write $A = (a_{ij})_{i, j}$ where $a_{ij} \in \F$ are the elements of $A$, with $1 \leq i \leq m, 1 \leq j \leq n$.

In this section, require that bases are ordered. Let $B = \{v_1, \cdots, v_n\}$ be a basis for $V$, and let $C = \{w_1, \cdots, w_m\}$ be a basis for $W$.

Now consider $\alpha \in M_{m \times n}(\F)$. Write:
\begin{align}
    \alpha(v_1) &= a_{11}w_1 + a_{21}w_2 + \cdots + a_{m1}w_m \nonumber \\
    \alpha(v_2) &= a_{12}w_1 + a_{22}w_2 + \cdots + a_{m2}w_m \label{eqnLinMapMatrix} \\
    \vdots~&~~~~~\vdots \nonumber \\
    \alpha(v_n) &= a_{1n}w_1 + a_{2n}w_2 + \cdots + a_{mn}w_m \nonumber
\end{align}
\begin{definition}{Matrix of a linear map}
    The \underline{matrix of a linear map} $\alpha$ with respect to the ordered bases $B, C$ as above defined is:
    \begin{equation*}
        [\alpha]_C^B = (\alpha_{ij})_{ij} \in M_{m \times n}(\F)
    \end{equation*}
\end{definition}
\begin{warning}
    The rows of the array given in equation~\ref{eqnLinMapMatrix} becomes the columns of $[\alpha]_{C}^B$.
\end{warning}
\subsection{Linking Matrices and Linear Maps}
As for why we have mixed up the rows and columns, recall that there exists a linear isomorphism from a vector to its coordinate vector:
\begin{align*}
    E_B : V &\mapsto \F^n\\
    v = \sum_{i=1}^{n}\lambda_i v_i &\mapsto (\lambda_i)_i = [v]_B
\end{align*}
then we have the following theorem:
\begin{theorem}
    Let $V, W$ be finite-dimensional vector spaces with dimension $n$ and $m$ respectively. Let $\alpha : V \mapsto W$ be a linear map.
    \begin{enumerate}
        \item For all $v \in V$, $[\alpha]_C^B [v]_B = [\alpha(v)]_C$ \label{thmPartLMMat}
        \item $[\alpha]_C^B$ is the only matrix in $M_{m \times n}(\F)$ satisfying part~\ref{thmPartLMMat}. \label{thmPartLMMatUnique}
        \item There is an isomorphism:
            \begin{align*}
                E_C^B : \L(V, W) &\mapsto M_{m \times n}(\F)\\
                \alpha &\mapsto [\alpha]_C^B
            \end{align*}
            \label{thmPartLMIsom}
    \end{enumerate}
    \label{thmMatrixOfLinMap}
\end{theorem}

\begin{proof}
    For part~\ref{thmPartLMMat}, let $v \in V$. Write $v = \sum_{j=1}^n \lambda_i v_i$. Then:
    \begin{align*}
        \alpha(v) &= \sum_{j=1}^{n}\lambda_i \alpha(v_i) \\
        &= \sum_{j=1}^{n}\lambda_i \sum_{i=1}^{m} a_{ij} w_i \\
        &= \sum_{i=1}^{m} \left(\sum_{j=1}^{n}\lambda_i a_{ij}\right)w_i \\
        \therefore [\alpha(v)]_C &= \left(\sum_{j=1}^{n}a_{ij}\lambda_j\right)_i \\
        &= [(a_{ij})(\lambda_j)]_i \\
        &= [\alpha]_C^B [v]_B
    \end{align*}

    Now for part~\ref{thmPartLMMatUnique}, consider $A \in M_{m \times n}(\F)$. Let $[v_j]_B = e_j$ (standard basis). Therefore, $A[v_j]_B$ is the first column of $A$. But,
    \begin{align*}
        A[v_j]_B &= [\alpha(v_i)]_C \\
        &= [\alpha]_C^B [v_i]_B \\
        &= [\alpha]_C^B e_i
    \end{align*}
    thus all the columns are the same, so $A = [\alpha]_C^B$.

    For part~\ref{thmPartLMIsom}, we set up the isomorphism as described.
    For linearity, let $\alpha, \beta \in \L(V, W), \lambda \in \F$. Then:
    \begin{align*}
        [\alpha + \lambda\beta]_C^B [v]_B &= [(\alpha + \lambda\beta)(v)]_C \\
        &= [\alpha(v) + \lambda\beta(v)]_C \\
        &= [\alpha(v)]_C + \lambda[\beta(v)]_C \text{ by linearity of } E_C \\
        &= \left([\alpha]_C^B + \lambda[\beta]_C^B\right)[v]_B \text{ by part~\ref{thmPartLMMat}} 
    \end{align*}
    for all $v \in V$, and therefore we have linearity by part~\ref{thmPartLMMatUnique}.

    Now to prove injectivity, let $\alpha \in \ker(E_C^B)$. That is, $[\alpha]_B^C = 0_{m \times n}$. Then by part~\ref{thmPartLMMat}, for all $v \in V$ $[\alpha(v)]_C = \zv$. But $E_C$ is an isomorphism so $\alpha(v) = \zv$ for all $v \in V$, that is, $\alpha$ is the zero map.

    Now let $A \in M_{m \times n}(\F)$. Define:
    \begin{align*}
        f : B &\mapsto W\\
        v_i &\mapsto \sum_{j=1}^{m}a_{ij}w_i
    \end{align*}
    and extend $f$ to a linear map $F$ by proposition~\ref{propBasisExtension}. Then $[F]_B^C = A$.
\end{proof}
\begin{remark}
    We can produce two maps $V \mapsto \F^m$. Via $W$, $\alpha : V \mapsto W$ and then $E_C : W \mapsto \F^m$. Via $\F^n$, $E_B : V \mapsto \F^n$ and then $[\alpha]_B^C : \F^n \mapsto \F^m$. This is a \underline{commutative diagram}.
\end{remark}
\begin{proposition}
    Let $V, W, X$ be finite-dimensional vector spaces with bases $B, C, D$ respectively. Let $\alpha \in \L(V,W), \beta \in \L(W, X)$. Then:
    \begin{equation*}
        [\beta \circ\alpha]_D^B =[\beta]_D^C [\alpha]_C^B
    \end{equation*}
    \label{propMatrixCompose}
\end{proposition}
\begin{proof}
    By theorem~\ref{thmMatrixOfLinMap}, $[\beta\circ\alpha]_C^B$ is the unique matrix satisfying:
    \begin{equation*}
        A[v]_B = [\beta(\alpha(v))]_D, v \in V
    \end{equation*}
    but:
    \begin{equation*}
        [\beta]_D^C[\alpha]_C^B[v]_B = [\beta]_D^C[\alpha(v)]_C = [\beta(\alpha(v))]_D
    \end{equation*}
    so by uniqueness of this matrix, they are equal.
\end{proof}
\begin{remark}
    For any basis $B$ of $V$, $[Id_V]_B^B = I_n$, the identity matrix of size $n\times n$.
\end{remark}
\begin{definition}{Change-of-basis matrix}
    For a vector space $V$ with dimension $n$, and bases $B, B'$, the \underline{change-of-basis matrix} from $B$ to $B'$ is given by:
    \begin{equation*}
        P = [Id_V]_{B'}^B \in M_{n \times n}(\F)
    \end{equation*}
\end{definition}
We also can find the elements of $P$, if $B = \{v_1, \cdots, v_n\}$ and $B' = \{v_1', \cdots, v_n'\}$,
\begin{equation*}
    v_j = \sum_{i=1}^{n}p_{ij} v_i'
\end{equation*}
and so the $j$th column of $P$ is $[v_j]_B'$.

Recall that $A \in M_{m \times n}(\F)$ is \underline{invertible} if there exists $A^{-1} \in M_{m \times n}(\F)$ such that $A A^{-1} = A^{-1} A = I_n$. Then $A^{-1}$ is the \underline{inverse} of $A$.

$GL_n(\F)$ denotes the group of invertible matrices of size $n$ over $\F$.
\begin{propositions}{
        Let $V, B, B'$ be defined as above. Let $W$ be a vector space of dimension $m$.
        \label{propsChangeOfBasis}
    }
    \item $[Id_V]_{B'}^B \in GL_n(\F)$ with inverse $[Id_V]_B^{B'}$. \label{propCOBInverse}
    \item If $\alpha \in \L(V, W)$, and $C, C'$ are bases for $W$,
        \begin{equation*}
            [\alpha]_{C'}^{B'} = [Id_W]_{C'}^C [\alpha]_C^B [Id_V]_B^{B'}
        \end{equation*}
\end{propositions}
\begin{proof}
    For the first part, use proposition~\ref{propMatrixCompose} to get:
    \begin{equation*}
        I_n = [Id_V \circ Id_V]_B^B = [Id_V]_B^{B'} [Id_V]_{B'}^B
    \end{equation*}
    so indeed the matrix $[Id_V]_{B'}^B$ is invertible with the required inverse. To show the second part, use proposition~\ref{propMatrixCompose} again.
\end{proof}
\end{document}