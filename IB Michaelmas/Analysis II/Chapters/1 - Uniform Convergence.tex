\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Pointwise Convergence}
\subsection{Definitions}
Consider a set $E \subseteq \R$, and consider a sequence of functions:
\begin{equation*}
    f_n : E \mapsto \R
\end{equation*}
for each $n \in \N$.

We have already defined the idea of convergence for sequences of real numbers, so what could we define for functions to converge?

\begin{definition}{Convergence at a point}
    A sequence of functions $(f_n)_{n \in \N}$ \underline{converges at a point} $x \in R$ if the sequence of real numbers $(f_n(x))_{n \in \N}$ converges.
\end{definition}
\begin{definition}{Pointwise convergence}
    A set of functions $(f_n)_{n \in \N}$ \underline{converges pointwise} to a function $f$ if the sequence of functions converges pointwise for every $x \in E$, and therefore $f$ is defined as $f(x) = \lim_{n \to \infty} f_n(x)$.
\end{definition}
\subsection{The Problem with Pointwise Convergence}
\begin{example}[Continuity is not preserved]
    Consider the most basic discontinuous function, the step function.
    \begin{align*}
        f : \R &\mapsto \R \\
        x &= 
        \begin{cases}
            1 & x > 0 \\
            0 & x = 0 \\
            -1 & x < 0
        \end{cases}
    \end{align*}
    Then we can consider a sequence of continuous functions that converge to this limit, such as:
    \begin{equation*}
        f_n(x) = \frac{2}{1 + e^{-nx}} - 1
    \end{equation*}
    Then this sequence of functions converges pointwise to the step function, but clearly the continuity property does not hold.
\end{example}
\begin{example}[Integrability is not preserved]
    Let $q_1, q_2 \cdots$ be an enumeration of $\Q \cap [0, 1]$. Define:
    \begin{align*}
        f_n : [0, 1] &\mapsto \R \\
        f_n(x) &=
        \begin{cases}
            1 & x \in \{q_1, \cdots, q_n\} \\
            0 & x \text{ otherwise}
        \end{cases}
    \end{align*}    
    then $f_n$ is integrable, because it is continuous on all but a finite set of points, but the limit $f$ is not integrable as found in Analysis I.
\end{example}
\begin{example}[Integrals are not preserved]
    Consider a sequence of functions that represent triangles as their graphs. Let the triangle begin at the origin, include $(\frac1n, n)$ and $(\frac2n, 0)$ which is a triangle with area $1$, so the functions in the sequence have $\int_0^1 f(x) dx = 1$.

    However, this converges pointwise to the zero function, which has integral 0.
\end{example}

This tells us that pointwise convergence is nowhere near strong enough to guarantee any properties that we would like. We therefore need a much stronger notion of convergence to be able to carry over properties like continuity and integrability to the limit function.
\section{Defining Uniform Convergence}
\begin{definition}{Uniform convergence}
    Let $f_n, f : E \subseteq \R \mapsto \R$. Then the sequence $(f_n)_{n \in \N}$ \underline{converges uniformly} to $f$ if for all $\epsilon > 0$, there exists $N \in \N$ that may depend on $\epsilon$, so that for every $x \in E$ and $n \geq N$, $|f_n(x) - f(x)| < \epsilon$.
\end{definition}
\begin{remark}
    The definition is equivalent to:
    \begin{equation*}
        \forall \epsilon > 0,~ \exists N \in \N~s.t.~~n \geq N \implies \sup_{x \in E} |f_n(x) - f(x)| < \epsilon
    \end{equation*}
\end{remark}
The key difference between this and piecewise convergence is that $N$ is allowed to depend on $\epsilon$ and the specific point $x \in E$ where the convergence is applied. In uniform convergence the same $N$ works for all $x \in E$ given an $\epsilon$.

Uniform convergence is a stronger notion than pointwise convergence, $f_n \to f$ uniformly $\implies f_n \to f$ pointwise on a set $E$. This gives a useful way to compute uniform limits of sequences of functions when they exist, or if a uniform limit exists. We first compute the pointwise limit of $f_n$, since if this does not exist then there is no uniform limit either. If it does exist, we have the function for which we can check $\sup_E |f_n - f|$, to ensure it goes to $0$.
\section{Properties of Uniform Convergence}
\subsection{Cauchy Criterion}
\begin{definition}{Uniformly Cauchy Sequence}
    Let $f_n : E \mapsto \R$ be a sequence of functions. We say $(f_n)$ is a\\\underline{uniformly cauchy sequence} if, for all $\epsilon > 0, \exists N(\epsilon)$ such that:
    \begin{equation}
        n, m \geq N \implies \sup_{x \in E} |f_n(x) - f_m(x)| < \epsilon
        \label{eqnSequenceCauchy}
    \end{equation}
\end{definition}
\begin{theorem}[Cauchy criterion for uniform convergenece]
    Let $(f_n)$ be a sequence of functions $f_n : E \mapsto \R$. Then the sequence converges uniformly on $E$ if and only if the sequence is uniformly Cauchy on $E$.
    \label{thmCauchyCriterion}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Suppose $f$ converges uniformly to $f : E \mapsto \R$}
        Then for any given $\epsilon$, there exists $N$ such that $n \geq N \implies \sup_{E} |f_n - f| < \epsilon$.
        Choose also $m \geq N$:
        \begin{align*}
            |f_n(x) - f_m(x) &\leq |f_n(x) - f(x)| + |f_m(x) - f(x)| \\
            &\leq \sup_E |f_n - f| + \sup_E |f_m - f| \\
            \therefore \sup_E |f_n(x) - f_m(x)| &\leq \sup_E |f_n - f| + \sup_E |f_m - f| < 2\epsilon.
        \end{align*}
        therefore we have bounded by a fixed multiple of $\epsilon$, the sequence is uniformly Cauchy.
    \end{proofdirection}
    \begin{proofdirection}{$\Rightarrow$}{Suppose $f_n$ is uniformly Cauchy}
        Then the sequence of real numbers $(f_n(x))_n$ is Cauchy for all $x \in E$, so it has a limit. Let the limit be $f(x)$. Therefore we have $f_n \to f$ pointwise on $E$. We now have a candidate uniform limit for $(f_n)$, but we need to check it converges uniformly.

        Fix $\epsilon > 0$, and use the Cauchy property:
        \begin{equation*}
            \exists N(\epsilon) \text{ such that } n, m \geq N \implies |f_n(x) - f_m(x)| < \epsilon
        \end{equation*}
        Therefore fix $n \geq N$, and let $m \to \infty$. This gives us the required bound:
        \begin{equation*}
            |f_n(x) - f(x)| \leq \epsilon \implies \sup_E |f_n(x) - f(x)| \leq \epsilon
        \end{equation*}
        and therefore $f_n \to f$ uniformly.
    \end{proofdirection}
\end{proof}
\begin{example}
    $f_n : \R \mapsto \R$. Let $f_n(x) = \frac{x}{n}$. Then $f_n(x) \to 0$ pointwise in $\R$. However, we do not have uniform convergence because there always exists an $x$ large enough that $|f_n(x) - 0| > \epsilon$.

    However, restricting the domain to $(-a, a)$ allows the function to converge uniformly, because the supremum over $(-a, a)$ of $|f_n(x)|$ is $\frac{a}{n} \to 0$ as required.
\end{example}
\subsection{Continuity}
\begin{theorem}[Continuity is preserved under uniform limits]
    Let $f_n, f : [a, b] \mapsto \R$. Suppose that $f_n \to f$ uniformly. If, for all $n$, $f_n$ are continuous at some $x \in [a, b]$, then $f$ is continuous at $x$.
    \label{thmUCContinuity}
\end{theorem}
\begin{proof}
    Fix $\epsilon > 0$. By uniform convergence, we have $N = N(\epsilon)$ such that
    \begin{equation*}
        n \geq N \implies \sup_{y \in [a, b]} |f_n(y) - f(y)| < \epsilon
    \end{equation*}

    Then by continuity of $F_N$ at $x$, we have $\delta(N, x, \epsilon) > 0$ such that for any $y \in [a, b]$ where $|x - y| < \delta$ then $|f_N(y) - f_N(x)| < \epsilon$.
    
    Then we have:
    \begin{align*}
        |f(y) - f(x)| &\leq |f(y) - f_N(y)| + |f_N(y) - f_N(x) + |f_N(x) - f(x)| \\
        &\leq \epsilon + \epsilon + \epsilon = 3\epsilon
    \end{align*}
    Therefore, since we have found that the difference between the points is less than a fixed multiple of $\epsilon$, $f$ must be continuous at $x$.
\end{proof}
\begin{corollary}[Uniform limits of continuous functions are continuous]
    If $f_n, f : [a, b] \mapsto \R$, $f_n \to f$ uniformly, and $f_n$ are continuous on $[a, b]$ for each $n$, then $f$ is continuous on $[a, b]$.
    \label{corLimitCtsUC}
\end{corollary}
\begin{proof}
    Apply theorem~\ref{thmUCContinuity} for all $x \in [a, b]$.
\end{proof}
\begin{remark}
    We define $C(E)$ to be the set of all continuous functions on $E$, assuming $E$ an appropriate set like an interval of $\R$ (if $E$ is not such a set, $C(E) = \emptyset$). This is a vector space.
\end{remark}
We can also provide an alternative statement for theorem~\ref{thmUCContinuity}:
\begin{corollary}
    Let $(f_n)$ be a uniformly Cauchy sequence of functions in $C([a, b])$. Then it converges to a function $f \in C([a, b])$.
    \label{corUCauchyCts}
\end{corollary}
Alternatively, $C([a, b])$ has no holes.
\begin{proof}
    The proof is by using thorem~\ref{thmCauchyCriterion} and corollary~\ref{corLimitCtsUC}.
\end{proof}
\subsection{Integrability} %TODO: If appropriate, put the proof of integrability here
\begin{theorem}
    Let $f_n, f : [a, b] \mapsto \R$ be bounded and integrable on $[a, b]$.

    If $f_n \to f$ uniformly on $[a, b]$, then $\int_{a}^{b} f_n(x) dx \to \int_{a}^{b} f(x) dx$.
    \label{thmUCIntegrals}
\end{theorem}
\begin{remark}
    The assumption that $f$ is integrable is redundant, as later we will show that uniform convergence preserves integrability.
\end{remark}
\begin{proof}
    \begin{align*}
        \left|\int_{a}^{b} f_n(x) dx - \int_{a}^{b} f(x) dx\right| &= \left|\int_{a}^{b} (f_n(x) - f(x)) dx\right| \\
        &\leq \int_{a}^{b} |f_n(x) - f(x)| dx \\
        &\leq \sup_{x \in [a, b]} |f_n(x) - f(x)|(b - a) \\
        &\to 0 \text{ by uniform convergence}
    \end{align*}
\end{proof}
\subsection{Differentiability}
This is more subtle than continuity or integrability. If $f_n \to f$ uniformly on some interval, and if $f_n$ are differentiable, it does not hold in general that $f$ is differentiable. Even if $f$ is differentiable, the derivatives may not converge even pointwise.
\begin{example}
    Consider the series of functions:
    \begin{align*}
        f_n : [-1, 1] &\mapsto \R \\
        x &\mapsto |x|^{1 + \frac1n}
    \end{align*}
    \begin{align*}
        \lim_{x \to 0} \frac{f_n(x) - f_n(0)}{x} &= \lim_{x \to 0} \frac{f_n(x)}{x} \\
        &= \lim_{x \to 0} \left(\text{sgn}(x) x^{\frac1n}\right) = 0
    \end{align*}
    And so $f_n$ is differentiable at $x = 0$ with $f_n'(0) = 0$.

    However, $f(x) = |x|$, which is not differentiable at $0$.
\end{example}
\begin{example}
    Consider now:
    \begin{align*}
        f_n : \R &\mapsto \R \
        x &\mapsto \frac{\sin(nx)}{\sqrt{x}}
    \end{align*}
    Then $f_n \to 0$ uniformly on $\R$. We have an infinitely differentiable limit since $f \equiv 0$. However, the derivative of $f_n$ at $0$ is $\sqrt{n} \cos(nx)$ which blow up to infinity as $n \to \infty$.
\end{example}
So far we have seen that a series of differentiable functions need to converge uniformly to a differentiable limit, and a series of non-differentiable functions can converge uniformly to a differentiable limit.

\begin{theorem}
    Let $f_n : [a, b] \mapsto \R$ be a sequence of differentiable functions on $(a, b)$, and require that the limit of the difference quotient exists on the one-sided limit at the endpoints.

    Suppose that $f_n'$ converges uniformly to a function $g : [a, b] \mapsto \R$, and for some $c \in [a, b]$ the sequence $(f_n(c))$ converges.

    Then $(f_n)$ converges uniformly to a function $f : [a, b] \mapsto \R$ and $f$ is differentiable on $[a, b]$ with derivative $g$.
    \label{thmUCDiff}
\end{theorem}
\begin{remarks}
    \item The statement tells us that:
        \begin{equation*}
            \frac{d}{dx}\left(\lim_{n \to \infty} f_n\right) = \lim_{n \to \infty} \left(\frac{d^{}f_n}{dx^{}}\right)
        \end{equation*}
        or, we can interchange the operations of taking derivatives and taking limits.
    \item If we assume that $f_n'$ are continuous, then the proof is more straightforward and can be based on the Fundamental Theorem of Calculus.
    \item Indeed, for this theorem, $f_n'$ need not be continuous.
\end{remarks}
\begin{proof}
    \begin{subproof}{$f_n \to f$ uniformly}
        We first start with a clever idea: instead of applying the Mean Value Theorem twice to two different function $f_n$ and $f_m$, consider applying it to their difference, $f_n - f_m$. This avoids two different arbitrary points generated by the MVT.
        \begin{equation*}
            f_n(x) - f_m(x) = f_n(c) - f_m(c) + (x - c) \frac{d}{dx}\left[f_n - f_m\right]_{x = x_{n, m}}
        \end{equation*}
        where here $x_{n, m}$ is a single point generated by the MVT.
        \begin{align*}
            &|f_n(x) - f_m(x) \leq |f_n(c) - f_m(c)| + (b - a) |f_n'(x_{n, m}) - f_m'(x_{n, m})| \\
            &\sup_{n, m} |f_n(x) - f_m(x)| \leq |f_n(c) - f_m(c)| + (b-a) \sup_{n, m} |f_n' - f_m'|
        \end{align*}
        and this tends to $0$ as $m, n \to \infty$.

        Therefore $(f_n)$ is uniformly Cauchy, and by theorem~\ref{thmCauchyCriterion}, there exists a function $f$ for which $f_n \to f$ uniformly.
    \end{subproof}
    \begin{subproof}{$f$ is differentiable with derivative $g$}
        Fix some $y \in [a, b]$.
        
        The second clever idea of the proof is to re-formulate the differentiability of $f$ as the continuity of a auxiliary function $h$

        Define $h(x)$:
        \begin{equation*}
            h(x) =
            \begin{cases}
                \frac{f(x) - f(y)}{x - y} & x \neq y \\
                g(y) & x = y
            \end{cases}
        \end{equation*}
        and a series of function $h_n(x)$:
        \begin{equation*}
            h_n(x) =
            \begin{cases}
                \frac{f_n(x) - f_n(y)}{x - y} & x \neq y \\
                f_n'(y) & x = y
            \end{cases}
        \end{equation*}
        Then, since $f_n$ is differentiable at $y$, we see that $h_n$ is continuous on $[a, b]$.
        The pointwise limit of $h_n$ is $h$, since $f_n' \to g$. Therefore, if this convergence is uniform we can apply theorem~\ref{thmUCContinuity} to get continuity.

        We can use the Cauchy criterion again:
        \begin{align*}
            h_n(x) - h_m(x) &=
            \begin{cases}
                \frac{(f_n - f_m)(x) - (f_n - f_m)(y)}{x - y} & x \neq y \\
                (f_n' - f_m')(y) & x = y
            \end{cases} \\
            &= (f_n' - f_m')(x_{n, m}) \text{ where } x_{n, m} \in [x, y] \\
            \sup (h_n - h_m) &\leq \sup |f_n' - f_m'| \to 0 \text{ as } n, m \to \infty
        \end{align*}
    \end{subproof}
\end{proof}
\begin{example}[Derivatives need not be continuous in theorem~\ref{thmUCDiff}]
    Consider:
    \begin{align*}
        f : [-1, 1] &\mapsto \R\\
        x &\mapsto \begin{cases}
            x^2 \sin\left(\frac{1}{x}\right) & x \neq 0 \\
            0 & x = 0
         \end{cases}
    \end{align*}
    Then $f_n$ is differentiable on $[-1, 1]$ with derivative:
    \begin{equation*}
        f'(x) = 
        \begin{cases}
            2x\sin\left(\frac{1}{x}\right) - \sin\left(\frac{1}{x}\right) & x \neq 0 \\
            0 & x = 0
        \end{cases}
    \end{equation*}
    Then we can take $f_n(x) = f(x)$ or $f_n(x) = f(x) + \frac{x}{n}$.
\end{example}
As we stated earlier, we can provide a shorter proof if we assume $f_n'$ are continuous:
\begin{proof}[Assume $f_n'$ continuous in theorem~\ref{thmUCDiff}]
    For any $x \in [a, b]$ we can write:
    \begin{equation}
        f_n(x) = f_n(c) + \int_{c}^{x} f'(t) dt
        \label{eqnFTCImplication}
    \end{equation}
    by the Fundamental Theorem of Calculus.

    \begin{align*}
        |f_n(x) - f_m(x)| &= \left|\left(f_n(c) - f_m(c)\right) + \int_{c}^{x} \left(f_n'(t) - f_m'(t)\right) dt \right| \\
        &\leq |f_n(c) - f_m(c)| + \sup_{t \int [a, b]} \left|f_n'(t) - f_m'(t)\right|
    \end{align*}
    and this tends to $0$ by uniform convergence of $f'$.

    This shows that $f_n$ are uniformly Cauchy, and so converge uniformly to some limit $f : [a, b] \mapsto \R$.

    Note that by theorem~\ref{thmUCIntegrals} that:
    \begin{equation*}
        \int_{c}^{x} f'(t) dt \to \int_{c}^{x} g(t) dt
    \end{equation*}
    where the continuity of $g$ implies its integrability, so this theorem's assumptions are satisfied. Now letting $n \to \infty$ in equation~\ref{eqnFTCImplication}:
    \begin{equation*}
        f(x) = f(c) + \int_{c}^{x} g(t) dt 
    \end{equation*}
    and differentiating gives $f'(x) = g(x)$, again by the Fundamental Theorem of Calculus.
\end{proof}
\begin{propositions}{
        Let $f_n, g_n : E \mapsto \R$ with $f_n \to f$ and $g_n \to g$ uniformly on $E$.
    }
    \item $f_n + g_n \to f + g$ uniformly on $E$ \label{propUCSum}
    \item If $h : E \mapsto \R$ is a bounded function on $E$, $hf_n \to hf$ uniformly on $E$. \label{propUCMultiple}
    \label{propUCLinearity}
\end{propositions}
The proof of this is simple, and is on the example sheet.
\begin{example}[$h$ must be bounded in proposition~\ref{propUCMultiple}]
    Let $f_n, h : \R \mapsto \R$ with $f_n(x) = \frac{1}{n}$ and $h = x$. Then $f_n \to 0$ uniformly on $\R$ but $(hf_n)(x) = \frac{x}{n}$ which does not converge uniformly to $0$.
\end{example}
\end{document}