\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Well-Posedness}
\begin{definition}{Well-posedness}
    A problem is \underline{well-posed} if:
    \begin{enumerate}
        \item A solution exists
        \item The solution is unique
        \item The solution depends continuously on the given data (such as boundary conditions and initial conditions).
    \end{enumerate}
\end{definition}
Conditions 1 and 2 are obvious, but 3 is more subtle: to mention continuity implicitly means that we have to assign some topology which permits this definition.

For example, if we have an IBVP whose initial data and boundary data lie in a space $X$, and a time $t > 0$ the solution to this problem $u(x, t)$ belongs to another space $Y$, we get some abstract map:
\begin{equation*}
    S_t : X \mapsto Y
\end{equation*}
and we need to know whether this map is continuous. This depends on the metrics of $X$ and $Y$. Further, we want to have \textit{sensible} metrics - the discrete metric (assigning distance 1 to nonequal points, and 0 to equal points) will not do.
\begin{example}
    Consider the initial-value problem:
    \begin{equation*}
        \frac{dx}{dt} = -x,\qquad x(0) = x_0
    \end{equation*}
    Then clearly the first and second requirements of well-posedness are satisfied. The solution is:
    \begin{equation*}
        X_0(t) = x_0 e^{-t}
    \end{equation*}
    Now consider a different initial condition, $x(0) = x_1$. This has solution $X_1(t) = x_1 e^{-t}$. The difference in these solutions at time $t$ is:
    \begin{equation*}
        |X_1(t) - X_0(t)| = e^{-t}|x_1 - x_0| \leq |x_1 - x_0|
    \end{equation*}
    So, in the sense of the Euclidean norm, the problem is well-posed.
\end{example}
\begin{example}
    We can try to find an ill-posed problem by looking at the heat equation. This has the behaviour that two different initial conditions give similar solutions after sufficient time, because the heat simply spreads out. However, we can reverse the direction of time and this means that two similar initial solutions will evolve very differently.

    Consider the IBVP on $\Omega = (0, \pi)$:
    \begin{equation*}
        \begin{cases}
            \phi_t + \phi_{xx} = 0 & \Omega \times (0, \infty) \\
            \phi = 0 & \partial \Omega \times (0, \infty) \\
            \phi = f & \Omega \times \{t = 0\}
        \end{cases}
    \end{equation*}
    Then if $f(x) = 0$, we get the solution $\phi(x, t) = 0$.

    If we take instead $f(x) = f_n(x) = \frac{1}{n} \sin(nx)$, we get the solution:
    \begin{equation*}
        \phi_n(x, t) = \frac{1}{n} e^{n^2 t} \sin(nx)
    \end{equation*}
    We see here that two initial conditions that are close give two solutions that are far apart. Consider the supremum norm on $(0, \pi)$:
    \begin{align*}
        ||f - f_n||_\infty &= \sup_{(0, \pi)} |f(x) - f_n(x)| = \frac1n \to 0 \\
        ||\phi(x, t) - \phi_n(x, t)|_\infty &= \frac{1}{n} e^{n^2 t} \to \infty
    \end{align*}
    Then this is ill-posed. We have another notion (local well-posedness) when we consider initial conditions that are close together, and time not too long. However, this problem is not even locally well-posed.
\end{example}
\section{The Method of Characteristics}
We want to solve a PDE of the following form:
\begin{equation*}
    a(x, y) \frac{\partial u}{\partial x} + b(x, y) \frac{\partial u}{\partial y} = c(x, y, u)
\end{equation*}
This is a quasi-linear problem, because the nonlinearity occurs only on the RHS.

We consider this equation, along with initial conditions $u(x, y, 0) = \phi(x, y)$ on a curve $C \subseteq \R^2$.

The key idea for this is attributed to Bernhard Riemann. This idea is to look at curves $(x, y) = (x(t), y(t))$ defined by:
\begin{gather*}
    \frac{dx}{dt} = a(x, y) \\
    \frac{dy}{dt} = b(x, y) \\
    (x(0),y(0)) \in C
\end{gather*}
We call these the \underline{characteristic curves}. Using these, we get a whole family of solutions determined by starting point $(x(0), y(0))$. Consider the evolution of $u(x, y)$ along a given characteristic. Set:
\begin{equation*}
    z(t) = u(x(t), y(t))
\end{equation*}
By the chain rule:
\begin{align*}
    \frac{dz}{dt} &= \frac{\partial u}{\partial x} \frac{dx}{dt} + \frac{\partial u}{\partial y} \frac{dy}{dt} \\
    &= a(x, t) \frac{\partial u}{\partial x} + b(x, t) \frac{\partial u}{\partial y} \\
    &= c(x, y, z) \text{ where $x, y, z$ are all functions of $t$.}
\end{align*}
Now we have an ordinary differential equation for $z$, with initial condition:
\begin{equation*}
    z(0) = u(x(0), y(0)) = \phi(x(0), y(0)) \text{ which is given.}
\end{equation*}
Instead of $(x(0), y(0))$, we will consider the curve $C$ to be parameterised by $s$, so then any point $(x, y) \in \R^2$ is given by $(s, t)$, where $s$ defines the characteristic and $t$ defines the point on the characteristic. We want to invert the relationship between $(x, y)$ and $(s, t)$.

\begin{example}
    Consider the problem:
    \begin{equation*}
        \frac{\partial u}{\partial x} + \frac{\partial u}{\partial y} = u,\qquad u(x, 0) = f(x)
    \end{equation*}
    Therefore our curve $C$ is $\{(s, 0), s \in \R\}$.

    The derivatives $\frac{dx}{dt}$ and $\frac{dy}{dt}$ are both $1$, so the characteristic curves are:
    \begin{equation*}
        \left\{
        \begin{split}
            x &= t + x_0  \\
            y &= t + y_0
        \end{split}
        \right.
    \end{equation*}
    We want $(x_0, y_0) \in C$ so take $(x_0, y_0) = (s, 0)$. Then $x = t + s, y = t$.
    \begin{align*}
        \frac{dz}{dt} &=c(x, y, z) \\
        &= z \\
        \therefore z(t) &= z_0 e^t \\
        z_0 &= u(x_0, y_0) \\
        &= u(s, 0) = f(s) \\
        \therefore z(t, s) &= f(s) e^t
    \end{align*}
    We now want to invert the relationship between $(x, y)$ and $(s, t)$ to get $s$ and $t$ in terms of $x$ and $y$:
    \begin{equation*}
        t = y,\qquad s = x - y
    \end{equation*}
    Then our final solution is:
    \begin{equation*}
        u(x, y) = z(t(x, y), s(x, y)) = f(x - y) e^y
    \end{equation*}
\end{example}
\begin{example}
    Consider the problem:
    \begin{equation*}
        (1 + x^2) \frac{\partial u}{\partial x} + \frac{\partial u}{\partial y} = u + 1, \qquad u(0, y) = f(y)
    \end{equation*}
    Then our curve $C$ is $\{(0, s), s \in \R\}$.

    The time derivatives for $x$ and $y$ are $\dot{x} = (1 + x^2)$, $\dot{y} = 1$, so:
    \begin{equation*}
        x(t) = \tan(t + \arctan(x_0)), \qquad y(y) = t + y_0
    \end{equation*}
    Then we need $(x_0, y_0) \in C$, $(x_0, y_0) = (0, s)$.
    \begin{equation*}
        \left\{
        \begin{split}
            x &= \tan(t) \\
            y &= t + s
        \end{split}
        \right.
    \end{equation*}
    Then we find $z$:
    \begin{align*}
        \frac{dz}{dt}&= z + 1 \\
        z(t) &= -1 + [z_0 + 1]e^t \\
        z_0 &= u(x_0, y_0) = f(s) \\
        \intertext{Hence}
        z(t, s) &= -1 + \left[f(s) + 1\right]e^t
    \end{align*}
    Then inverting the $(t, s) \mapsto (x, t)$ map:
    \begin{equation*}
        \left\{
        \begin{split}
            t &= \arctan(x) \\
            s &= y - \arctan(x)
        \end{split}
        \right.
    \end{equation*}
    We get our solution:
    \begin{equation*}
        u(x, y) = -1 + \left[f(y - \arctan(x)) + 1\right] e^{\arctan{x}}
    \end{equation*}
\end{example}
However, we have a problem: what if the characteristic curves cross? In this case we would have two different characteristics for a single point, which means that the solution would take multiple values at that point. In fact this tells us that the problem has non-unique solutions, so is not well-posed.
\section{Classifying Second-Order PDEs}
We will classify PDEs of the form $Lu = g$ where the operator $L$ is given by:
\begin{equation}
    a(x, y) \frac{\partial^{2}}{\partial x^{2}} + 2b(x, y) \frac{\partial^{2}}{\partial x\partial y} + c(x, y) \frac{\partial^{2}}{\partial y^{2}} + d(x, y) \frac{\partial }{\partial x} + e(x, y) \frac{\partial }{\partial y} + f(x, y)
    \label{eqnGeneralSOPDE}
\end{equation}
The terms we most want to control are the higher-order terms, the second derivatives. We will focus on these, and introduce a change of variables $(x, y) \mapsto (\xi, \eta)$ to simplify the problem.

Introduce vectors:
\begin{equation*}
    \vec{x} = \begin{pmatrix}x \\ y\end{pmatrix} = \begin{pmatrix}x_1 \\ x_2\end{pmatrix},\quad \vec{\xi} = \begin{pmatrix}\xi \\ \eta\end{pmatrix} = \begin{pmatrix}\xi_1 \\ \xi_2\end{pmatrix}
\end{equation*}
Then with this new notation the operator $L$ is:
\begin{equation*}
    L = \sum_{i, j = 1}^2 a_{ij} \frac{\partial^{2}u}{\partial x_i\partial x_j} + \text{lower order terms}
\end{equation*}
where $(a_{ij})$ is a matrix given by:
\begin{equation*}
    (a_{ij})(x, y) = \begin{pmatrix}
        a(x, y) & b(x, y) \\
        b(x, y) & c(x, y)
    \end{pmatrix}
\end{equation*}
The following proposition tells us what happens under this coordinate transform:
\begin{proposition}
    Consider any transform $\vec{x} \mapsto \vec{\xi}$.
    Let $U(\vec{\xi}) = u(\vec{x})$.
    Then the operator $L$ for $u$ transforms to the operator $\tilde{L}$ for $U$ defined by:
    \begin{equation*}
        \tilde{L}U = \sum_{p, q = 1}^{2} A_{pq} \frac{\partial^{2}U}{\partial \xi_p\partial \xi_q}
    \end{equation*}
    where:
    \begin{equation*}
        A_{pq} = \sum_{i, j = 1}^{2} a_{ij} \frac{\partial \xi_p}{\partial x_i} \frac{\partial \xi_q}{\partial x_j}
    \end{equation*}
    \label{propTransformPDE}
\end{proposition}
\begin{proof}
    Use summation convention. Then by the chain rule:
    \begin{equation*}
        \frac{\partial u}{\partial x_i} = \frac{\partial u}{\partial \xi_p} \frac{\partial \xi_p}{\partial x_i}
    \end{equation*}
    and so,
    \begin{align*}
        a_{ij} \frac{\partial^{2}u}{\partial x_i\partial x_j}&= a_{ij} \frac{\partial^{2}U}{\partial \xi_p\partial \xi_q} \frac{\partial \xi_p}{\partial x_i} \frac{\partial \xi_q}{\partial x_j} + a_{ij} \frac{\partial U}{\partial \xi_p} \frac{\partial^{2}\xi_p}{\partial x_i\partial x_j} \\
        &= A_{pq} \frac{\partial^{2}U}{\partial \xi_p\partial \xi_q} + \text{lower order terms}
    \end{align*}
\end{proof}
Then we use proposition~\ref{propTransformPDE} to get the elements of the matrix $(A_{pq})$, back in terms of $\xi$ and $\eta$:
\begin{equation*}
    (A_{pq}) =
    \begin{pmatrix}
        a \xi_x^2 + 2b \xi_x \xi_y + c\xi_y^2 & a\xi_x\eta_x + b(\xi_x \eta_y + \xi_y \eta_x) + c \xi_y \eta_y \\
        a\xi_x\eta_x + b(\xi_x \eta_y + \xi_y \eta_x) + c \xi_y \eta_y & a\eta_x^2 + 2b\eta_x\eta_y + c^2 \eta_y^2
    \end{pmatrix}
\end{equation*}
We can simplify the PDE greatly if we can make $A_{11}$ and $A_{22}$ zero. This happens when the variables $M = \xi_x / \xi_y$ or $N = \eta_x / \eta_y$ satisfy the quadratic equation:
\begin{equation*}
    az^2 + 2bz + c = 0
\end{equation*}
Then by the quadratic formula, the solutions are:
\begin{equation*}
    z = \frac{-b \pm \sqrt{b^2 - ac}}{a}
\end{equation*}
When $M$ or $N$ are chosen to satisfy this equation we call $\xi(x, y) = \text{const}$ and $\eta(x, y) = \text{const}$ the \underline{characteristic curves} of the PDE. When such curves exist, along them we have:
\begin{equation*}
    \xi_x + \frac{dy}{dx} \xi_y = 0,\qquad \eta_x + \frac{dy}{dx} \eta_y = 0
\end{equation*}
We can alternatively write this as $(x, y) = (x, y(x))$ where $y(x)$ is defined implicitly $\xi(x, y(x)) = \text{const}$ or $\eta(x, y(x)) = \text{const}$.

We then have a family of characteristic curves defined by:
\begin{equation}
    \frac{dy}{dx} = -\frac{-b \pm \sqrt{b^2 - ac}}{a}
    \label{eqnCCurvesImplicit}
\end{equation}
Then we can, at last, provide classifications for the PDE in equation~\ref{eqnGeneralSOPDE}:
\begin{definition}{Ellipitic, parabolic, hyperbolic operators}
    \begin{itemize}
        \item If $b^2 < ac$ then $L$ is an \underline{elliptic operator}, with no real characteristic curves.
        \item If $b^2 = ac$ then $L$ is a \underline{parabolic operator}, with 1 family of real characteristic curves.
        \item If $b^2 > ac$ then $L$ is a \underline{hyperbolic operator}, with 2 families of real characteristic curves.
    \end{itemize}
\end{definition}
The case of a hyperbolic operator is when we introduce the coordinates $(\xi, \eta)$ defined by equation~\ref{eqnCCurvesImplicit}, giving us the new partial differential operator of the form:
\begin{equation*}
    \tilde{L} = \frac{\partial^{2}}{\partial \xi\partial \eta} + \text{lower order terms}
\end{equation*}
and the new problem $\tilde{L}U = G(\xi, \eta)$ where $G(\xi, \eta) = g(x, y)$.
\begin{example}[Hyperbolic operator: wave equation]
    Consider the wave equation with speed $c = 1$:
    \begin{equation*}
        \frac{\partial^{2}u}{\partial x^{2}} - \frac{\partial^{2}u}{\partial y^{2}} = 0
    \end{equation*}
    Then our coefficient functions are, $a = 1, b = 0, c = -1$ and $b^2 - ac = 1$. This is greater than $0$, so the problem is hyperbolic. Because this holds for all $x$, we say the problem is \underline{globally hyperbolic}. The characteristic curves are defined by equation~\ref{eqnCCurvesImplicit}, which becomes, in this case,
    \begin{equation*}
        \frac{dy}{dx} = \mp 1
    \end{equation*}
    Integrating these to find the characteristic curves:
    \begin{equation*}
        y \pm x = \text{const}
    \end{equation*}
    Then we introduce the coordinates $\xi(x, y) = x - y$, $\eta(x, y) = x + y$. In these coordinates we indeed have $A_{11} = A_{22} = 0$, and the off-diagonal entries are:
    \begin{equation*}
        A_{12} = A_{21} = 2
    \end{equation*}
    Then for the new function $U(\xi, \eta) = u(x, y)$, the problem becomes:
    \begin{equation*}
        4 \frac{\partial^{2}U}{\partial x\partial y} = 0
    \end{equation*}
    which has solutions $U(\xi, \eta) = A(\xi) + B(\eta)$, where $A$ and $B$ are arbitrary functions. We can then find the solution for $u(x, y)$:
    \begin{equation*}
        u(x, y) = A(x - y) + B(x + y)
    \end{equation*}
    Now suppose we want to solve the initial value problem for $u(x, t)$ on a domain $\Omega = \R$:
    \begin{equation*}
        \begin{cases}
            Lu = 0 & \Omega \times (0, \infty) \\
            u = f & \Omega \times \{t = 0\} \\
            u_t = g &\Omega \times \{t = 0\} 
        \end{cases}
    \end{equation*}
    Now we know that the solution must have the form:
    \begin{equation*}
        u(x, t) = A(x - t) + B(x + t)
    \end{equation*}
    Matching the initial conditions, we find:
    \begin{equation*}
        u(x, t) = \frac12 \left[f(x + t) - f(x - t)\right] + \frac12 \int_{x - t}^{x + t} g(s) ds 
    \end{equation*}
\end{example}
\begin{example}[Locally hyperbolic operator]
    Consider the partial differential equation:
    \begin{equation*}
        xy \frac{\partial^{2}u}{\partial x^{2}} - \frac{\partial^{2}u}{\partial y^{2}} = 0
    \end{equation*}
    Then our coefficient functions are $a = xy, b = 0, c = 1$. The discriminant $b^2 - ac$ is $xy$, so we have that the PDE is hyperbolic only on the region $xy > 0$. In these two quarters of the plane we have the characteristic curves defined by the differential equation:
    \begin{equation*}
        \frac{dy}{dx} = \mp \frac{1}{\sqrt{xy}}
    \end{equation*}
    Integrating this up:
    \begin{equation*}
        \frac13 y^{3/2} \pm x^{1/2} = \text{const}
    \end{equation*}
    therefore, introduce the new coordinates $(\xi, \eta)$:
    \begin{equation*}
        \xi(x, y) = \frac13 y^{3/2} + x^{1/2},\quad\eta(x, y) = \frac13 y^{3/2} - x^{1/2}
    \end{equation*}
    Then our new PDE is:
    \begin{equation*}
        -\frac12 y \frac{\partial^{2}U}{\partial \xi\partial \eta} + \text{lower order terms} = 0
    \end{equation*}
\end{example}
\section{Fourier Transform in Higher Dimensions}
\begin{definition}{Fourier Transform ($\R^n)$}
    For $f : \R^n \mapsto \C$, define the \underline{Fourier transform} of $f$ as:
    \begin{equation*}
        \hat{f}(\vec{\lambda}) = \int_{\R^n} e^{-i \vec{\lambda} \cdot \vec{x}} d^n \vec{x}
    \end{equation*}
    and define the inverse Fourier Transform:
    \begin{equation*}
        f(\vec{x}) = \frac{1}{(2\pi)^n} \int_{\R^n} e^{i \vec{\lambda} \cdot \vec{x}} \hat{f}(\vec{\lambda}) d^n \vec{\lambda}
    \end{equation*}
\end{definition}
\begin{definition}{Convolution ($\R^n$)}
    For functions $f, g : \R^n \mapsto \C$, define the \underline{convolution} of $f$ and $g$:
    \begin{equation*}
        f \star g(\vec{x}) = \int_{\R^n} f(\vec{x} - \vec{y}) g(\vec{y}) d^n \vec{y}
    \end{equation*}
\end{definition}
Then we have the usual convolution theorem:
\begin{equation*}
    \FT[f \star g(\vec{x})] = \hat{f}(\vec{\lambda}) \hat{g}(\vec{\lambda})
\end{equation*}
and we also know what happens to derivatives of $f$:
\begin{equation*}
    \FT\left[\left(\frac{\partial}{\partial x_1}\right)^{\alpha_1} \cdots \left(\frac{\partial}{\partial x_n}\right)^{\alpha_n} f(\vec{x})\right] = (i \lambda_1)^{\alpha_1} \cdots (i\lambda_n)^{\alpha_n} \hat{f}(\vec{\lambda})
\end{equation*}
\begin{example}
    We can take the Fourier transform of $\Delta f(\vec{x})$:
    \begin{align*}
        \Delta f(\vec{x}) &= \left(\frac{\partial}{\partial x_1}\right)^{2} \cdots \left(\frac{\partial}{\partial x_n}\right)^{2} f(\vec{x}) \\
        \FT[\Delta f(\vec{x})]&= (-\lambda_1^2 - \lambda_2^2 - \cdots - \lambda_n^2) \hat{f}(\vec{\lambda}) \\
        &= -|\lambda|^2 \hat{f}(\vec{\lambda})
    \end{align*}
\end{example}
We can also find a higher-dimensional analogue of the Dirac delta function. We let this be the function (actually, distribution) that satisfies $\delta(\vec{x}) = 0$ for $\vec{x} \neq \vec{0}$, and
\begin{equation*}
    \int_{|\vec{x}| < \epsilon} \delta(\vec{x}) d^n \vec{x} = 1~\forall \epsilon > 0.
\end{equation*}
Then we get the sampling property that we recognise from 1D, and the Fourier transform is:
\begin{equation*}
    \hat{\delta}(\vec{\lambda}) = \int_{\R^n} e^{- \vec{\lambda} \cdot \vec{x}} \delta(\vec{x}) d^n \vec{x} = 1
\end{equation*}
Also, by the inversion formula,
\begin{equation*}
    \delta(\vec{x}) = \frac{1}{(2\pi)^n} \int_{\R^n} e^{i \vec{\lambda} \cdot \vec{x}} d^n \vec{\lambda}
\end{equation*}
\begin{example}[Multidimensional Gaussian]
    Recall that, if $g(x) = \frac{1}{\sqrt{2\pi}} e^{x^2 / 2}$, then $\hat{g}(\lambda) = \sqrt{2\pi} g(\lambda)$. We can find a similar result for the multidimensional Gaussian.

    Let $g(\vec{x}) = \frac{1}{(2\pi)^{n / 2}} e^{-|\vec{x}|^2 / 2}$. Then we have the result we want:
    \begin{align*}
        \hat{g}(\vec{\lambda})&= \frac1{(2\pi)^{n / 2}} \int_{\R^n} e^{-i \vec{\lambda} \cdot \vec{x}} e^{-|\vec{x}|^2 / 2} d^n \vec{x}\\
        %See photo.
    \end{align*}
\end{example}
\section{Heat Equation}
We want to solve:
\begin{equation}
    \begin{cases}
        u_t = \kappa \Delta u = F(\vec{x}, t) & (x, t) \in \R^n \times (0, \infty) \\
        u(\vec{x}, 0) = f(\vec{x}) & \vec{x} \in \R^n
    \end{cases}
    \label{eqnHeatUnbounded}
\end{equation}
Then we can split the problem and superpose later. We can consider the two problems:
\begin{itemize}
    \item Zero forcing ($F = 0$), arbitrary initial data to make the algebra nicer,
    \item Zero initial data ($f = 0$), arbitrary forcing to make the algebra nicer.
\end{itemize}
To solve, we will use the \textit{heat kernel}. This is defined as:
\begin{equation}
    k(\vec{x};t) = \frac{1}{(4 \pi \kappa t)^{n / 2}} \exp\left[-\frac{|\vec{x}|^2}{4 \kappa t}\right]
    \label{eqnHeatKernel}
\end{equation}
\begin{proposition}
    Taking the Fourier transform of the heat kernel gives:
    \begin{equation*}
        \hat{k}(\vec{\lambda};t) = e^{-\kappa t |\vec{\lambda}|^2}
    \end{equation*}
    \label{propHeatKernelFT}
\end{proposition}
\begin{proof}
    Denote the multivariate Gaussian by $g(\vec{x})$,
    \begin{align*}
        g(\vec{x}) &= \frac{1}{(2\pi)^{n / 2}} e^{-|\vec{x}|^2 / 2} \\
        \hat{g}(\vec{\lambda}) &= e^{-|\vec{\lambda}|^2 / 2} \\
    \end{align*}
    \begin{align*}
        \hat{k}(\vec{\lambda};t) &= \int_{\R^n} e^{-i \vec{\lambda} \cdot \vec{x}} k(\vec{x};t) d^n\vec{x} \\
        &= \frac{1}{(4\pi \kappa t)^{n/2}} \int_{\R^n} e^{-i \vec{\lambda} \cdot \vec{x}} e^{|\vec{x}|^2 / 4\kappa t} d^n \vec{x} \\
        \intertext{Now define the new variable $\vec{y}$ with $\vec{x} = \sqrt{2\kappa t} \vec{y}$:}
        &= \frac{1}{(4\pi \kappa t)^{n/2}} \int_{\R^n} e^{-i \vec{\lambda} \cdot \sqrt{2 \kappa t}\vec{y}} e^{|\vec{y}|^2 / 2} (2\kappa t)^{n / 2}d^n \vec{y} \\
        &= \frac{1}{(2\pi)^{n/2}} \int_{\R^n} e^{-i (\sqrt{2\kappa t}\vec{\lambda}) \cdot \vec{y}} e^{|\vec{y}|^2 / 2} d^n \vec{y} \\
        &= \int_{\R^n} e^{-i (\sqrt{2\kappa t}\vec{\lambda}) \cdot \vec{y}} g(\vec{x}) d^n \vec{y} \\
        &= \hat{g}(\sqrt{2 \kappa t} \vec{\lambda}) \\
        &= e^{-\kappa t |\vec{\lambda}|^2}
    \end{align*}
\end{proof}
\begin{remark}
    As $t \to 0+$, $\hat{k}(\vec{\lambda};t) \to 1$, and we can say that, as $t \to 0$, $k(\vec{x}, t) \to \delta(\vec{x})$.
\end{remark}
\begin{proposition}
    The solution to the problem of zero forcing in the heat equation is:
    \begin{equation*}
        u(\vec{x}, t) = k(\vec{x};t) \star f(\vec{x}).
    \end{equation*}
    \label{propHeatUnboundedSoln}
\end{proposition}
\begin{proof}
    Take the Fourier Transform of the heat equation:
    \begin{equation*}
        \frac{\partial \hat{u}}{\partial t} + \kappa |\vec{\lambda}|^2 \hat{u} - 0,\qquad \hat{u}(\vec{\lambda}, 0) = \hat{f}(\vec{\lambda}).
    \end{equation*}
    Now we can solve the ordinary differential equation:
    \begin{align*}
        \hat{u}(\vec{\lambda}, t) &= \hat{u}(\vec{\lambda}, 0) e^{-\kappa |\vec{\lambda}|^2 t} \\
        &= \hat{f}(\lambda) \hat{k}(\vec{\lambda};t)
    \end{align*}
    Then the convolution theorem gives the required result.
\end{proof}
\begin{proposition}
    The solution to the heat equation with zero initial data is:
    \begin{equation*}
        u(\vec{x}, t) = \int_{\tau = 0}^{t} \int_{y \in \R^n} k(\vec{x} - \vec{y};t-\tau) F(\vec{y}, \tau)d^n \vec{y} d\tau 
    \end{equation*}
    \label{propHeadUnboundedSolnForced}
\end{proposition}
\begin{proof}
    Taking the Fourier Transform again:
    \begin{equation*}
        \frac{\partial \hat{u}}{\partial t} + \kappa |\vec{\lambda}|^2 \hat{u} = \hat{F}(\vec{\lambda}, t),\qquad \hat{u}(\vec{\lambda}, 0) = 0
    \end{equation*}
    Then we can use an integrating factor $e^{\kappa |\vec{\lambda}|^2 t}$:
    \begin{align*}
        \frac{\partial}{\partial t} \left[\hat{u}(\vec{\lambda}, t) e^{\kappa |\vec{\lambda}|^2 t}\right]&= \hat{F}(\vec{\lambda}, t) e^{\kappa |\vec{\lambda}|^2 t} \\
        \intertext{Integrate with respect to $t$:}
        \hat{u}(\vec{\lambda}, t) e^{\kappa |\vec{\lambda}|^2 t} &= \int_{0}^{t} \hat{F}(\lambda, \tau) e^{\kappa |\vec{\lambda}|^2 \tau} d\tau \\
        \hat{u}(\vec{\lambda}, t) &= \int_{0}^{t} e^{-\kappa |\vec{\lambda}|^2 (t - \tau)}\hat{F}(\lambda, \tau) d\tau \\
        \hat{u}(\vec{\lambda}, t) &= \int_{0}^{t} \hat{k}(\vec{\lambda};t-\tau)\hat{F}(\lambda, \tau) d\tau \\
    \end{align*}
    Then by the convolution theorem, we have the required result.
\end{proof}
Therefore, our finished solution to equation~\ref{eqnHeatUnbounded} is:
\begin{equation}
    u(\vec{x}, t) = \left(k(\cdot; t) \star f\right)(\vec{x}) + \int_{\tau = 0}^{t} \int_{y \in \R^n} k(\vec{x} - \vec{y};t-\tau) F(\vec{y}, \tau)d^n \vec{y} d\tau 
    \label{eqnHeatUnboundedSoln}
\end{equation}
If we focus on the second part, we can define:
\begin{equation*}
    G(\vec{x}, t;\vec{y}, \tau) = H(t - \tau) k(\vec{x} - \vec{y}; t - \tau)
\end{equation*}
Then if we set $F(\vec{x}, t) = 0$ on $t < 0$, the solution to the problem with zero initial data is:
\begin{equation*}
    u(\vec{x}, t) = \int_{\tau = 0}^{t} \int_{y \in \R^n} G(\vec{x}, t;\vec{y}, \tau) F(\vec{y}, \tau)d^n \vec{y} d\tau 
\end{equation*}
then $G$ is a Green's function for the heat equation with zero initial data. We can show that:
\begin{equation*}
    \frac{\partial G}{\partial t} - \kappa \Delta G = \delta(t - s) \delta(\vec{x} - \vec{y})
\end{equation*}
or, equivalently,
\begin{equation*}
    \frac{\partial \hat{G}}{\partial t} + \kappa |\vec{\lambda}|^2 \hat{G} = e^{-\vec{\lambda} \cdot \vec{y}} \delta(t - s)
\end{equation*}
\section{Green's Function for Laplace's Equation}
\subsection{The Free-Space Green's Function}
We want to find $G = G(\vec{x}, \vec{y})$ such that for each fixed $\vec{y} \in \R^n$,
\begin{equation*}
    \Delta(G(\vec{x}, \vec{y})) = \delta(\vec{x} - \vec{y}),\qquad G(\vec{x}, \vec{y}) \to 0 \text{ as } |\vec{x}| \to\infty
\end{equation*}
This is the \underline{Free Space Green's Function} for Laplace's Equation. We can think of $G$ as the electric potential at $\vec{x}$ due to a point charge at point $\vec{y}$. By taking the Fourier Transform:
\begin{equation*}
    \hat{G}(\vec{\lambda}, \vec{y}) = \frac{-e^{i \vec{\lambda} \cdot \vec{y}}}{|\vec{\lambda}|^2}
\end{equation*}
\begin{proposition}
    For $\alpha > 0, \vec{x} \in \R^n$,
    \begin{equation*}
        \FT[|\vec{x}|^{-\alpha}] = C_{n, \alpha} |\vec{\lambda}|^{\alpha - n}
    \end{equation*}
    where $C_{n, \alpha}$ is a constant.
    \label{propFTOfModX}
\end{proposition}
\begin{proof}
    Write $f_\alpha(\vec{x}) = |\vec{x}|^{-\alpha}$.
    \begin{equation*}
        \hat{f}_\alpha(\vec{\lambda}) = \int_{\R^n} e^{-i \vec{\lambda} \cdot \vec{x}} |\vec{x}|^{-\alpha} d^n\vec{x}
    \end{equation*}
    Then we make the substitution $\vec{x} = R^T\vec{x}'$, where $R \in \operatorname{SO}(n)$.
    \begin{align*}
        \hat{f}_\alpha(\vec{\lambda}) &= \int_{\R^n} e^{-i \vec{\lambda} \cdot R^T\vec{x}'}|R^T \vec{x}'|^{-\alpha} d^n \vec{x}' \\
        &= \int_{\R^n} e^{-i (R\vec{\lambda}) \cdot \vec{x}'}|\vec{x}'|^{-\alpha} d^n \vec{x}' \\
        &= \hat{f}_\alpha(R\vec{\lambda}).
    \end{align*}
    Therefore, $\hat{f}_\alpha$ is rotation invariant, so if $\vec{\lambda} = |\vec{\lambda}|\vec{m}$ where $\vec{m}$ is a unit vector, then $\hat{f}_\alpha$ is independent of $\vec{m}$.
    Now make a substitution $|\vec{\lambda}|\vec{x} = \vec{x}'$.
    \begin{align*}
        \hat{f}_\alpha(\vec{\lambda}) &= \int_{\R^n} e^{-i |\vec{\lambda}| \vec{m} \cdot \vec{x}} |\vec{x}|^{-\alpha} d^n \vec{x} \\
        &= \int_{\R^n} e^{-i \vec{m} \cdot \vec{x}'} |\vec{\lambda}|^\alpha|\vec{x}'|^{-\alpha} d^n \vec{x}' |\vec{\lambda}|^{-n} \\
        &= \left(\int_{\R^n} e^{-i \vec{m} \cdot \vec{x}'} |\vec{x}'|^{-\alpha} d^n \vec{x}'\right) |\vec{\lambda}|^{\alpha - n} \\
        &= C_{n, \alpha} |\vec{\lambda}|^{\alpha - n}
    \end{align*}
\end{proof}
From this, we get the important result:
\begin{equation*}
    \frac{1}{|\vec{\lambda}|^2} = \FT[c_n |\vec{x}|^{2-n}]
\end{equation*}
\begin{proposition}
    For $n > 2$,
    \begin{equation*}
        G(\vec{x}, \vec{y}) = -\frac{1}{(n-2) |S^{n-1}|} \frac{1}{|\vec{x} - \vec{y}|^{n-2}}
    \end{equation*}
    where $|S^{n-1}|$ is the area of $S^{n-1}$.
    \label{propFreeGreen}
\end{proposition}
\begin{remark}
    In $\R^3$,
    \begin{equation*}
        G(\vec{x}, \vec{y}) = -\frac{1}{4\pi} \frac{1}{|\vec{x} - \vec{y}|}
    \end{equation*}
\end{remark}
\begin{proof}
    Set $F(\vec{x}) = c_n |\vec{x}|^{2-n}$, so:
    \begin{align*}
        \hat{G}(\vec{\lambda}, \vec{y}) &= e^{-i \vec{\lambda} \cdot \vec{y}} \hat{F}(\lambda) \\
        &= -\FT[F(\vec{x} - \vec{y})]
    \end{align*}
    Therefore $G(\vec{x}, \vec{y}) = -\frac{c_n}{|\vec{x} - \vec{y}|^{n-2}}$. Since also $\Delta G(\vec{x}, \vec{0}) = \delta(\vec{x})$, we have:
    \begin{align*}
        \frac1{c_n} \delta(\vec{x}) &= -\Delta[|\vec{x}|^{2-n}] \\
        \intertext{Integrate over $|\vec{x}| \leq 1$, and use the divergence theorem:}
        \frac1{c_n} &= -\int_{|\vec{x}| \leq 1} \Delta[|\vec{x}|^{2-n}] d^n \vec{x} \\
        &= -\int_{|\vec{x}| = 1} \nabla[|\vec{x}|^{2-n}] \cdot \vec{dS} \\
        &= (n-2)\int_{|\vec{x}| = 1} dS \\
        &= (n-2) |S^{n-1}|
    \end{align*}
\end{proof}
\begin{proposition}
    If $n = 2$, a Green's function for Laplace's equation is:
    \begin{equation*}
        G(\vec{x}, \vec{y}) = \frac1{2\pi} \log\left(|\vec{x} - \vec{y}|\right)
    \end{equation*}
    \label{propFreeGreenR2}
\end{proposition}
\begin{proof}
    Take $\vec{y} = \vec{0}$ without loss of generality (the operator $\Delta$ is translation-invariant).

    \begin{align*}
        \Delta[\log(r)] &= \frac1r \frac{\partial}{\partial r} \left(r \frac{\partial \log(r)}{\partial r}\right) \\
        &= \frac1r \frac{\partial}{\partial r} \left(0\right) \\
        &= 0 \text{ for $r \neq 0$}
    \end{align*}
    Around $r = 0$, set $\epsilon > 0$ and use the divergence theorem:
    \begin{align*}
        \int_{|\vec{x}| \leq \epsilon} \Delta[\log(r)] d^2 \vec{x}&= \int_{|\vec{x}| = \epsilon} \nabla (\log(r)) \cdot \vec{dS} \\
        &= \int_0^{2\pi} \frac{\vec{e_r}}{\epsilon} \cdot \vec{e_r} \epsilon d\theta \\
        &= 2\pi
    \end{align*}
    So we have the requirements, when divided by $2\pi$.
\end{proof}
In summary, the free space Green's function for the Laplace equation is:
\begin{equation}
    G(\vec{x}, \vec{y}) =
    \begin{cases}
        \frac1{2\pi} \log(|\vec{x} - \vec{y}|) & n = 2 \\
        -\frac{1}{(n-2) |S^{n-1}|} \frac{1}{|\vec{x} - \vec{y}|^{n-2}} & n > 2
    \end{cases}
    \label{eqnFreeGreen}
\end{equation}
As an aside, we can get to the $n = 2$ case by taking a limit of the dimension $n \to 2$ of the Green's function:
\begin{equation*}
    \lim_{n \to 2} -\frac{1}{n-2} \frac{1}{|S^{n-1}|} \left[\frac{1}{|\vec{x} - \vec{y}|^{n-2}} - 1\right] = \frac1{2\pi} \log(|\vec{x} - \vec{y}|)
\end{equation*}
\subsection{The Dirichlet Green's Function}
We want to solve, on $\Omega \subseteq \R^n$:
\begin{equation}
    \begin{cases}
        \Delta u = F & \vec{x} \in \Omega \\
        u = 0 & x \in \partial \Omega
    \end{cases}
    \label{eqnLaplacianForcedUnbdd}
\end{equation}
To do this, we want to find the Dirichlet Green's function, defined for each $\vec{y} \in \Omega$ by:
\begin{equation*}
    \begin{cases}
        \Delta \DGrn(\vec{x}, \vec{y}) = \delta(\vec{x} - \vec{y}) & \vec{x} \in \Omega \\
        \DGrn(\vec{x}, \vec{y}) = 0 & \vec{x} \in \partial \Omega
    \end{cases}
\end{equation*}
Then with such a Green's function we have the solution to $u$:
\begin{equation*}
    u(\vec{x}) = \int_{\Omega} \DGrn(\vec{x}, \vec{Y}) F(\vec{y}) d^n \vec{y}
\end{equation*}
To find $\DGrn$ we need a free-space Green's function $G(\vec{x}, \vec{y})$ along with an added harmonic function that satisfies $\DGrn(\vec{x}, \vec{y}) = 0$ on $\partial \Omega$.

We introduce the \underline{Method of Images}. We want to add a series of point charges outside of $\Omega$ such that the total potential due to $\vec{y}$ and the point charges we add is zero on $\partial \Omega$.
\begin{example}
    Consider $\Omega = \subsetselect{\vec{x} \in \R^3}{x_3 > 0}$.
    Considering a point charge at $\vec{y} \in \Omega$,
    \begin{equation*}
        \vec{y} = \begin{pmatrix}y_1 \\ y_2 \\ y_3\end{pmatrix}
    \end{equation*}
    we add another one the other side of the boundary $x_3 = 0$:
    \begin{equation*}
        \vec{y_0} = \begin{pmatrix}y_1 \\ y_2 \\ -y_3\end{pmatrix}
    \end{equation*}
    Therefore we make our guess:
    \begin{equation*}
        \DGrn(\vec{x}, \vec{y}) = G(\vec{x}, \vec{y}) + \left[-G(\vec{x}, \vec{y_0})\right]
    \end{equation*}
    Then for each $\vec{x} \in \Omega$:
    \begin{align*}
        \Delta \DGrn(\vec{x}, \vec{y}) &= \delta(\vec{x} - \vec{y}) - \delta(\vec{x} - \vec{y_0}) \\
        &= \delta(\vec{x} - \vec{y}) \text{ on $x \in \Omega$.}
    \end{align*}
    Here we see the importance of the extra point charges being \underline{outside of $\Omega$}: if not, the $\delta(\vec{x} - \vec{y_0})$ would not disappear.

    We also need to check the boundary conditions. For that we need the result:
    \begin{equation*}
        G(\vec{x}, \vec{y}) = -\frac{1}{4\pi} \frac{1}{|\vec{x} - \vec{y}|}
    \end{equation*}
    For $x \in \partial \Omega$,
    \begin{align*}
        \DGrn(\vec{x}, \vec{y}) &= -\frac{1}{4\pi} \left[\frac{1}{\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (0 - y_3)^2}} \right. \\
        &\left. - \frac{1}{\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + (0 + y_3)^2}}\right] \\
        &= 0
    \end{align*}
\end{example}
\end{document}