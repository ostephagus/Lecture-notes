\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{The Dirac Delta Function}
The Dirac Delta function was introduced in IA Differential Equations with the following properties:
\begin{align*}
    \delta(x) &= 0 \text{ if }x \neq 0 \\
    \forall \epsilon > 0, &\int_{-\epsilon}^{\epsilon} \delta(x) dx = 1
\end{align*}
No such function exists.

However, we can attempt to understand this by considering the Dirac Delta Function as the limit of a sequence of functions:
\begin{equation*}
    \delta_n(x) =
    \begin{cases}
        \frac{n}{\alpha}\exp\left[-\frac{1}{1-n^2x^2}\right] & |x| < \frac1n \\
        0 & |x| \geq \frac1n
    \end{cases}
\end{equation*}
where:
\begin{equation*}
    \alpha = \int_{-1}^{1} \exp\left[-\frac{1}{1-y^2}\right] dy 
\end{equation*}
which is found by taking $y = nx$.

We note that:
\begin{enumerate}
    \item The function $\delta_n$ is smooth for each $n$;
    \item $\delta_n(x)$ is zero on $|x| \geq \frac1n$;
    \item For all $\epsilon > 0,~\exists N > 0$ such that:
        \begin{equation*}
            n \geq N \implies \int_{-\epsilon}^{\epsilon} \delta_x dx = 1
        \end{equation*}
\end{enumerate}
For the third remark, we simply take $N > \frac1\epsilon$. This results in:
\begin{align*}
    \int_{-\epsilon}^{\epsilon} \delta_n(x) dx &= \int_{-\frac1n}^{\frac1n} \delta_n(x) dx \\
    &= \frac1\alpha \int_{-1}^{1} \exp\left[-\frac{1}{1-y^2}\right] dy \text{ by taking }y = nx \\
    &= 1
\end{align*}
Now consider figure~\ref{figDiracDeltaGraph}. We see that $\lim_{n \to \infty} \delta_n(x) = 0$ if $x \neq 0$ and, for $\epsilon > 0$, we have the required integral property.

Therefore, this \underline{almost} gives us the limit:
\begin{equation*}
    \delta(x) = \lim_{n \to \infty} \delta_n(x)
\end{equation*}
However, the pointwise limit does not exist everywhere:
\begin{equation*}
    \lim_{n \to \infty} \delta_n(0) = \lim_{n \to \infty} \frac{n}{e\alpha}
\end{equation*}
which is infinite.

For this course, we will say that the limit does exist, but in a ``weaker sense'' (see III Distribution Theory \& Applications).

The Dirac Delta function rarely appears in isolation. For example, the key ``picking out'' property:
\begin{equation*}
    \int_{-\infty}^{\infty} f(x)\delta(x - a) dx = f(a)
\end{equation*}
It also has important applications in modelling an impulse:
\begin{equation*}
    \ddot{y} + \omega^2 y = \delta(t)
\end{equation*}
To deal with this, we solve $\ddot{y}_n + \omega^2 y_n = \delta_n(t)$. Then we define $y(t)$ as the pointwise limit $\lim_{n \to \infty} t_n(t)$.

Using this limiting procedure, we can also consider $\delta'(x)$:
\begin{align*}
    \int_{-\infty}^{\infty} \delta'(x)f(x) dx &= \lim_{n \to \infty} \int_{-\infty}^{\infty} \delta_n'(x) f(x) dx \\
    &= -\lim_{n \to \infty} \int_{-\infty}^{\infty} \delta_n(x) f'(x) dx \\
    &= -\int_{-\infty}^{\infty} \delta(x) f'(x) dx = -f'(0)    
\end{align*}
It gets irritating writing all these limits, so we usually deal with the Dirac Delta function itself (where we understand that, behind the scenes, a limiting process is occurring).
\subsection{Periodic delta functions}
Let $\delta^L(x)$ denote the $L$-periodic delta function outside the interval $\left[\left.-\frac{L}{2}, \frac{L}{2}\right)\right.$
We can even take the Fourier series:
\begin{equation*}
    \frac{1}{L} \int_{-\frac{L}{2}}^{\frac{L}{2}} \delta(x) e^{-\frac{2\pi}{L}inx} dx
\end{equation*}
This gives:
\begin{equation*}
    \delta^L(x) \sim \frac{1}{L}\sum_{n \in \Z} e^{\frac{2\pi}{L}inx}
\end{equation*}
Plausibility test:
\begin{align*}
    f(0) &= \int_{-\frac{L}{2}}^{\frac{L}{2}} \delta^L(x) f(x) dx \\
    &= \sum_{n \in Z} \left[\frac{1}{L} \int_{-\frac{L}{2}}^{\frac{L}{2}} e^{\frac{2\pi}{L}inx}f(x) dx \right] \\
    &= \sum_{n \in \Z} \hat{f}_{-n} = \sum_{n \in \Z} \hat{f}_{n} \\
    &= f(0)
\end{align*}
This, although we have dealt with non-convergent sums to find it, is our expected result. Note that, since $\delta_n(x)$ is smooth its Fourier coefficients must decay rapidly, so this is slightly less bad than it looks.
\subsection{Eigenfunction Expansion of \texorpdfstring{$\delta(x)$}{the Delta Function}}
Let $L$ be a Sturm-Liouville operator on the vector space $V \subseteq C^2[a, b]$, where we require that $y \in V$ satisfy real, homogeneous boundary conditions at each non-singular endpoint.

Let $\{Y_k\}_{k=1}^\infty$ be a set of normalised eigenfunctions. Fix $\xi \in [a, b]$, and consider functions $\delta_n(x - \xi)$. For $n$ sufficiently large, $\delta_n(x - \xi) \in V$. By completeness,
\begin{align*}
    \delta(x - \xi) &= \sum_{k=1}^{\infty} \inn{\delta_n(t-\xi)}{Y_k(t)}_W Y_k(x) \\
    &= \sum_{k=1}^{\infty} \left[\int_{a}^{b} \delta_n(t - \xi) Y_k(t) w(t) dt \right] Y_k(x)
\end{align*}
Formally, letting $n \to \infty$,
\begin{equation*}
    \delta(x - \xi) = \sum_{k=1}^{\infty} w(\xi) Y_k(\xi) Y_k(x)
\end{equation*}
We can consider another plausibility test:
\begin{align*}
    f(x) &= \int_{a}^{b} \delta(x - \xi)f(\xi) d\xi \\
    &= \sum_{k=1}^{\infty} \int_{a}^{b} f(\xi) Y_k(\xi) w(\xi) d\xi Y_k(x) \\
    &= \sum_{k=1}^{\infty} \hat{f}_k Y_k(x) \sim f(x)
\end{align*}
\section{Green's Functions}
\subsection{Deriving a Green's Function}
We want to solve inhomogeneous problems of the form:
\begin{equation}
    \begin{cases}
        Ly = f(x) & x \in (a, b) \\
        y(x) = 0 & x \in \{a, b\}
    \end{cases}
    \label{eqnInhomogProblem}
\end{equation}
Here, $L$ has the form $\alpha(x) \frac{d^{2}}{dx^{2}} + \beta(x) \frac{d}{dx} + \gamma(x)$ where $\alpha(x) \neq 0$.

Now fix $\xi \in (a, b)$. Suppose we can find a function $G = G(x; \xi)$ satisfying:
\begin{equation*}
    \begin{cases}
        L[G(x;\xi)] = \delta(x - \xi) & x \in (a, b) \\
        G(x, \xi) = 0 & x \in \{a, b\}
    \end{cases}
\end{equation*}
Now consider the new function defined by:
\begin{equation*}
    y(x) = \int_{a}^{b} G(x;\xi) f(\xi) d\xi
\end{equation*}
Clearly $y(a) = y(b) = 0$. Then applying $L$:
\begin{align*}
    L_x[y(x)] &= \int_{a}^{b} L_x[G(x;\xi)]f(\xi) d\xi \\
    &= \int_{a}^{b} \delta(x - \xi)f(\xi) d\xi \\
    &= f(x)
\end{align*}
Therefore, $y$ solves equation~\ref{eqnInhomogProblem}. We call $G$ a \underline{Green's Function} for $L$ with Dirichlet boundary conditions.

We want to try to understand $G$. Note that on the intervals $(a, \xi)$ and $(\xi, b)$ it must satisfy:
\begin{equation*}
    L_x[G(x;\xi)] = 0
\end{equation*}
This is a nice, 2nd-order, homogeneous ordinary differential equation so we expect $G(x;\xi)$ to be well-behaved here. However, in the neighbourhood of $\xi$, we have that $\alpha(x) \frac{d^{2}G}{dx^{2}}$ will be badly behaved, as we know that $\delta(x - \xi)$ is very badly behaved.
\begin{equation*}
    \alpha(x) \frac{d}{dx}\left[\frac{dG}{dx}\right] = \delta(x - \xi) + \text{ more regular terms}
\end{equation*}
So the key question is: what sort of functions have derivatives that look like $\delta(x - \xi)$?

Recall the sequence $\delta_n$:
\begin{equation*}
    \lim_{n \to \infty} \delta_n(x) = 0, x \neq 0, \text{ and } \forall \epsilon > 0 \lim_{n \to \infty} \int_{-\epsilon}^{\epsilon} \delta_n(x) dx = 1
\end{equation*}
It is therefore natural to consider the functions:
\begin{equation*}
    H_n(x) = \int_{-\infty}^{x} \delta_n(x) dx
\end{equation*}
if we fix $X > 0$ then for $N$ sufficiently large ($\frac1n < X$):
\begin{align*}
    H_n(X) &= \int_{-\frac1n}^{\frac1n} \delta_n(t) dt \\
    &= 1
\end{align*}
Similarly fixing $X < 0$, then for $N$ sufficiently large $H_N(X) = 0$. Taking a limit of $H_n$,
\begin{equation*}
    \lim_{n \to \infty} H_n(x) = H(x) =
    \begin{cases}
        1 & x > 0 \\
        0 & x < 0
    \end{cases}
\end{equation*}
Here $H$ is called the \underline{Heaviside function} which satisfies the key property $H'(x) = \delta(x)$.

Armed with this knowledge, we expect to need $\frac{dG}{dx}$ to behave like $H(x - \xi)$ near $x = \xi$. That is, we need $G$ to have a jump discontinuity in its derivative around $x = \xi$. For the behaviour of $G$ we can integrate $H(x)$:
\begin{equation*}
    \int_{a}^{x} H(t - \xi) dt =
    \begin{cases}
        x - \xi & x \geq \xi \\
        0 & x < \xi
    \end{cases}
\end{equation*}
This is a continuous function!

We conclude that we need $G$ to be continuous at $x = \xi$ (and also the rest of the interval), and that $\frac{dG}{dx}$ should have a jump discontinuity at $x = \xi$. To find the magnitude of the jump, integrate:
\begin{equation*}
    \frac{\delta(x - \xi)}{\alpha(x)} = \frac{d^{2}G}{dx^{2}} + \frac{\beta(x)}{\alpha(x)}\frac{dG}{dx} + \frac{\gamma(x)}{\alpha(x)}G
\end{equation*}
over a small region around $\xi$. The LHS is:
\begin{equation*}
    \int_{\xi - \epsilon}^{\xi + \epsilon} \frac{\delta(x) - \xi}{\alpha(x)} dx = \frac{1}{\alpha(\xi)}
\end{equation*}
The RHS is:
\begin{align*}
    \lim_{\epsilon \to 0} &\left.\frac{dG}{dx} \right|_{x = \xi - \epsilon}^{x = \xi + \epsilon} + \int_{\xi - \epsilon}^{\xi + \epsilon} \left[\frac{\beta(x)}{\alpha(x)}\frac{dG}{dx} + \frac{\gamma(x)}{\alpha(x)}G\right] dx \\
    &= \left[\frac{dG}{dx}\right]_{x = \xi-\epsilon}^{x = \xi + \epsilon} = \frac{1}{\alpha(\xi)}
\end{align*}
In summary, $G$ satisfies:
\begin{enumerate}
    \item $\lim_{x \to \xi^+} G(x;\xi) = \lim_{x \to \xi^-} G(x,\xi)$
    \item $\left[\frac{dG}{dx}\right]_{x = \xi - \epsilon}^{x = \xi + \epsilon} = \frac{1}{\alpha(\xi)}$.
\end{enumerate}
\begin{example}
    Let $L = \frac{d^{2}}{dx^{2}} + \omega^2$ on the interval $(0, 1)$.

    We want to solve:
    \begin{equation*}
        \begin{cases}
            \frac{d^{2}G}{dx^{2}} + \omega^2 G = \delta(x - \xi) & x \in (0, 1) \\
            G(x;\xi) & x \in \{0, 1\}
        \end{cases}
    \end{equation*}
    We solve the differential equation on either side of $x = \xi$.
    \begin{equation*}
        G(x;\xi) =
        \begin{cases}
            A(\xi) \sin(\omega x) + B(\xi) \cos(\omega x) & x \in (0, \xi) \\
            C(\xi) \sin(\omega (x-1)) + D(\xi) \cos(\omega (x-1)) & x \in (\xi, 1) \\
        \end{cases}
    \end{equation*}
    Applying the boundary conditions gives $B, D = 0$. This is because we wrote the second bit of $G$ using $x-1$ rather than $x$.

    Use the continuity condition:
    \begin{equation*}
        A(\xi) = C(\xi) \frac{\sin(\omega(\xi-1))}{\sin(\omega\xi)}
    \end{equation*}
    Using the derivative jump equation and substituting for $A$:
    \begin{equation*}
        \frac{C(\xi)\omega}{\sin(\omega\xi)} \left[\cos(\omega(\xi - 1))\sin(\omega\xi) - \cos(\omega\xi)\sin(\omega(\xi - 1))\right] = 1
    \end{equation*}
    We have an addition formula:
    \begin{align*}
        1 &= \frac{C(\xi)\omega}{\sin(\omega\xi)} \left[\sin(\omega \xi - \omega(\xi - 1))\right] \\
        &= \frac{C(\xi) \omega}{\sin(\omega \xi)} \sin(\omega) \\
        C(\xi) &= \frac{\sin(\omega \xi)}{\omega \sin(\omega)} \\
        A(\xi) &= \frac{\sin(\omega(\xi - 1))}{\omega \sin \omega}
    \end{align*}
    Then our Green's function becomes:
    \begin{equation*}
        G(x;\xi) = \frac{1}{\omega \sin(\omega)}
        \begin{cases}
            \sin(\omega x) \sin(\omega(\xi - 1)) & x \in (0, \xi) \\
            \sin(\omega \xi) \sin(\omega(x - 1)) & x \in (\xi, 1)
        \end{cases}
    \end{equation*}
    However, this has the form:
    \begin{equation*}
        \text{const } \times
        \begin{cases}
            y_1(x)y_2(\xi) & x \in (0, \xi) \\
            y_1(\xi)y_2(x) & x \in (\xi, 1)
        \end{cases}
    \end{equation*}
    Note that $y_1, y_2$ are linearly independent solutions to $Ly = 0$ with $y_1(0) = 0$ and $y_2(1) = 0$. This is a very interesting result which hints at a more general easier way to solve for these sorts of functions.

    We have a problem! For some values of $\omega$, our Green's Function is not defined. However, it turns out that these are exactly the eigenvalues of the operator $L$. In fact, this can be used to find the eigenvalues of $L$, and is analogous to setting a determinant to zero.
    \label{exInhomogOscillator}
\end{example}
\subsection{Easier Method for Green's Functions}
We want to find a more general method for finding a Green's function for the problem in equation~\ref{eqnInhomogProblem}.
\begin{proposition}
    Let $y_1, y_2$ be linearly independent solutions to $Ly = 0$ with $y_1(a) = 0$ and $y_2(b) = 0$. The Green's function for the problem in equation~\ref{eqnInhomogProblem} is given by:
    \begin{equation}
        G(x;\xi) = \frac{1}{\alpha(\xi) W(y_1, y_2)(\xi)} \begin{cases}
            y_1(x)y_2(\xi)  & x \in (a, \xi) \\
            y_1(\xi) y_2(x) & x \in (\xi, b)
        \end{cases}
        \label{eqnGreenFuncFormula}
    \end{equation}
    and it satisfies $L[G(x;\xi)] = \delta(x-\xi), G(a;\xi) = G(b;\xi) = 0$.
    \label{propGreenFuncFormula}
\end{proposition}
\begin{proof}
    We have that $L[G(x;\xi)] = 0$ for $x \neq \xi$ because $y_1$ and $y_2$ are solutions. We also have continuity around $\xi$ because the limit is the same from both sides, $G(\xi;\xi) = y_1(\xi)y_2(\xi)$.

    The derivative jump is given by:
    \begin{align*}
        &\left[\frac{dG}{dx}\right]_{x = \xi - \epsilon}^{x = \xi + \epsilon} \\
        &= \frac{1}{\alpha(x) W(y_1, y_2)(\xi)} \left[y_1(\xi) y_2'(\xi) - y_2(\xi) y_1'(\xi)\right] \\
        &=\frac{1}{\alpha(x)}
    \end{align*}
    So this formula for $G(x;\xi)$ has the properties we want.
\end{proof}
Therefore the general solution to equation~\ref{eqnInhomogProblem} is:
\begin{align*}
    y(x) &= \int_{a}^{b} G(x;\xi) f(\xi) d\xi \\
    &= \int_{a}^{x} G(x;\xi) f(\xi) d\xi + \int_{x}^{b} G(x;\xi) f(\xi) d\xi \\
    &= y_2(x) \int_{a}^{x} \frac{y_1(\xi)f(\xi)}{\alpha(\xi)W(y_1, y_2)(\xi)} d\xi + y_1(x) \int_{x}^{b} \frac{y_2(\xi)f(\xi)}{\alpha(\xi)W(y_1, y_2)(\xi)} d\xi 
\end{align*} % TODO: Fix alignment
\begin{example}
    We will now re-do example~\ref{exInhomogOscillator} with this new method.

    We will use:
    \begin{align*}
        y_1(x) &= \sin(\omega x) \\
        y_2(x) &= \sin(\omega(x - 1))
    \end{align*}
    Now $\alpha(\xi)$ is simply 1, and the Wronskian is:
    \begin{align*}
        W(y_1, y_2)(\xi) &= \omega \sin(\omega \xi) \cos(\omega(\xi - 1)) \\
        &- \omega \cos(\omega \xi) \sin(\omega(\xi - 1)) \\
        &= \omega \sin(\omega \xi - \omega(\xi - 1)) \\
        &= \omega \sin(\omega)
    \end{align*}
    Now we have the result we expect:
    \begin{equation*}
        G(x;\xi) = \frac{1}{\omega \sin(\omega)}
        \begin{cases}
            \sin(\omega x) \sin(\omega(\xi - 1)) & x \in (0, \xi) \\
            \sin(\omega \xi) \sin(\omega(x - 1)) & x \in (\xi, 1)
        \end{cases}
    \end{equation*}
    This example is still useful to motivate further ideas. We note that the function out the front is a constant, not a function of $\xi$. This is a more general idea that generalises to all Sturm-Liouville operators.
\end{example}
\subsection{Green's Functions for Sturm-Liouville Operators}
Suppose that the operator $L$ is Sturm-Liouvile:
\begin{equation*}
    L = \frac{1}{W(x)} \left[-\frac{d}{dx}\left(p(x) \frac{d}{dx}\right) + q(x)\right]
\end{equation*}
Since we are interested in the solutions to $Ly = f$, we can set the weight function to $1$ and re-label $W(x) f(x)$ to $f(x)$.

For such an operator our $\alpha(x)$ is $-p(x)$.
\begin{proposition}
    For $L$ a Sturm-Liouville operator with linearly independent solutions $y_1, y_2$ satisfying $y_1(a) = 0 = y_1(b)$,
    \begin{equation*}
        p(x) W(y_1, y_2)(x) \text{ is constant.}
    \end{equation*}
    \label{propSLGreensFunc}
\end{proposition}
\begin{proof}
    \begin{align*}
        y_2 Ly_1 - y_1 Ly_2 &= y_1 \left(-(py_1')' + qy_1\right) - y_1 \left(-(py_1')' + qy_2\right) \\
        &= y_1 (py_2')' - y_2(py_1')' \\
        &= \left[y_1(py_2')' + y_1'(py_2')\right] - \left[y_2(py_1')' + y_2'(py_1')\right] \\
        &= \left[py_1y_2' - py_2 y_1'\right]' \\
        &= \left[p(x) W(y_1, y_2)(x)\right]'
    \end{align*}
    But note now that $Ly_1 = Ly_2 = 0$, so the function $p(x) W(y_1, y_2)(x)$ is constant (its derivative is zero).
\end{proof}
Now we can write out our solution $y(x)$ to $Ly = f$:
\begin{multline*}
    y(x) = \frac{-1}{p(c) W(y_1, y_2)(c)} \left(y_2(x) \int_{a}^{x} f_1(\xi) f(\xi) d\xi \right. \\
    \left.+ y_1(x) \int_{x}^{b} y_2(\xi)f(\xi) d\xi \right)
\end{multline*}
for any constant $c \in (a, b)$.
\section{Eigenfunction Expansions, Revisited}
Recall that if $L$ is a Sturm-Liouville operator on $V = \subsetselect{y \in C^2[a, b]}{y(a) = y(b) = 0}$, then there exist normalised eigenfunctions $\{y_k\}_{k = 1}^\infty$ such that $LY_k = \lambda_k Y_k$ and $\inn{Y_k}{Y_l} = \delta_{kl}$ (orthonormality). Again take the weight function $W(x) = 1$.

For any $f \in V$ then we can consider the eigenfunction expansion:
\begin{equation*}
    f(x) = \sum_{k=1}^{\infty} \hat{f}_k Y_k
\end{equation*}
where $\hat{f}_k = \inn{f}{Y_k}$.

We would really like to write $G(x;\xi)$ in this form, but $G \notin V$ because it is not in $C^2[a, b]$. We apply our usual trick and take:
\begin{equation*}
    G_n(x;\xi) = \delta_n(x - \xi), G_n(a;\xi) = G_n(b;\xi) = 0.
\end{equation*}
Then $G_n \in V$ for each $n \in \N$. That means we can take an eigenfunction expansion and then take a limit of both sides:
\begin{equation*}
    G(x;\xi) = \sum_{k=1}^{\infty} \hat{G}_k(\xi) Y_k(x),~~~\hat{G}_k(\xi) = \inn{G(x;\xi)}{Y_k(x)}
\end{equation*}
\begin{proposition}
    If $\{Y_k\}$ are as above then the Green's Function for the Sturm-Liouville opeartor $L$ satisfies:
    \begin{equation*}
        G(x;\xi) = \sum_{k=1}^{\infty} \frac{Y_k(x)Y_k(\xi)}{\lambda_k}
    \end{equation*}
    \label{propGreenEFuncExp}
\end{proposition}
\begin{proof}
    By completeness,
    \begin{equation*}
        G(x;\xi) = \sum_{k=1}^{\infty} \hat{G}_k(\xi) Y_k(x)
    \end{equation*}
    Then the coefficients are:
    \begin{align*}
        \hat{G}_k(\xi) &= \inn{G}{Y_k} \\
        &= \frac{1}{\lambda_k} \inn{G}{LY_k} \\
        &= \frac{1}{\lambda_k} \inn{LG}{Y_k} \text{ because $L$ is self-adjoint} \\
        &= \frac{1}{\lambda_k} \inn{\delta(x - \xi)}{Y_k(x)} \\
        &= \frac{1}{\lambda_k} \int_{a}^{b} \delta(x - \xi) Y_k(x) dx \\
        &= \frac{Y_k(\xi)}{\lambda_k}
    \end{align*}
    as required.
\end{proof}
\begin{remark}
    This function fails if $L$ has a zero eigenvalue. That is, if there exists a non-zero function $\tilde{y} \in V$ such that $L\tilde{y} = 0$. This is a problem when we want to solve $Ly = f$ beacuse now the problem is ill-defined because the problem is not unique. 
    As an aside, we are trying to find $L^{-1}(f)$ which cannot exist if $L$ has a zero eigenvalue. The analogue to this is trying to find $A^{-1} \vec{v}$ when $A$ is a singular matrix.
\end{remark}
\begin{example}
    Take $L = -\frac{d^{2}}{dx^{2}}$, and $(a, b) = (0, 1)$. The vector space to consider is:
    \begin{equation*}
        V = \subsetselect{y \in C^2[0, 1]}{y(0) = y(1) = 0}
    \end{equation*}
    We know that the normalised eigenfunctions are:
    \begin{equation*}
        Y_k(x) = \sqrt{2} \sin(k \pi x),~~~\lambda_k = (k \pi)^2
    \end{equation*}
    Then we can very easily find our Green's Function:
    \begin{equation*}
        G(x;\xi) = \sum_{k=1}^{\infty} \frac{2\sin(k \pi x) \sin(k \pi \xi)}{(k \pi)^2}
    \end{equation*}
    We can now derive a familar result:
    \begin{align*}
        G(x;\xi) &= \sum_{k=1}^{\infty} \frac{Y_k(x) Y_k(\xi)}{\lambda_k} \\
        \delta(x - \xi) &= L[G(x;\xi)] \\
        &= \sum_{k=1}^{\infty} \frac{Y_k(\xi) LY_k(x)}{\lambda_k} \\
        &= \sum_{k=1}^{\infty} Y_k(\xi) Y_k(x)
    \end{align*}
\end{example}
\section{Initial-Value Problems}
\begin{equation}
    \begin{cases}
        Ly = f(t) & t > 0 \\
        y(0) = \dot{y}(0) = 0 &
    \end{cases}
    \label{eqnInhomogIVP}
\end{equation}
where $L = \alpha(t) \frac{d^2}{dx^2} + \beta(t) \frac{d}{dx} + \gamma(t)$ for $\alpha(t) \neq 0$.

For each $\tau \in (0, \infty)$, suppose we can find $G = G(t;\tau)$ such that:
\begin{align*}
    L[G(t;\tau)] &= \delta(t - \tau),~~t > 0 \\
    G(0; \tau) &= \dot{G}(0;\tau) = 0
\end{align*}
Then we have the function:
\begin{equation*}
    y(t) = \int_{0}^{\infty} G(t;\tau) f(\tau) d\tau 
\end{equation*}
and this satisfies equation~\ref{eqnInhomogIVP}.
\begin{proposition}
    Green's function for equation~\ref{eqnInhomogIVP} is characterised by:
    \begin{enumerate}
        \item $G(t;\tau) = 0$ on $t < \tau$,
        \item $L[G(t, \tau)] = 0$ on $t > \tau$ with $G(\tau^+;\tau) = 0$ and $\dot{G}(\tau^+;\tau) = \frac{1}{\alpha(t)}$.
    \end{enumerate}
    \label{propGreensFuncIVP}
\end{proposition}
\begin{proof}
    The first condition guarantees that $L[G(t;\tau)] = 0$ for $t < \tau$ and $G(0, \tau) = \dot{G}(0, \tau) = 0$.

    The second condition guarantees that $L[G(t;\tau)] = \delta(t - \tau)$ since $G(\tau^+;\tau) = G(\tau^-, \tau) = 0$ and the jump in derivative is:
    \begin{equation*}
        \dot{G}(\tau^+;\tau) - \dot{G}(\tau^-;\tau) = \frac{1}{\alpha(t)} - 0
    \end{equation*}
    as required.
\end{proof}
Note that since $G(t;\tau)$ = 0 on $t < \tau$, we have the following:
\begin{equation*}
    y(t) = \int_{0}^{\infty} G(t;\tau)f(\tau) d\tau = \int_{0}^{t} G(t;\tau)f(\tau) d\tau
\end{equation*}
That is, causality is intact (the solution at time $t$ depends only on the forcing term for times less than $t$).
\begin{example}
    Take $L = \frac{d^{2}}{dx^{2}}$. Then we have that $G(t;\tau) = 0$ when $t < \tau$. To find $G$ for $t > \tau$ we solve the differential equation:
    \begin{equation*}
        \ddot{G}(t;\tau) = 0 = G(\tau^+,t) = 0,~\dot{G}(\tau^+;\tau) = 1
    \end{equation*}
    We will use a similar trick as earlier by making $G$ a function of $t - \tau$:
    \begin{equation*}
        G(t;\tau) = A(\tau) (t - \tau) + B(\tau)
    \end{equation*}
    Then we see that $B = 0$, and $A = 1$ by the boundary conditions at $t \to \tau^+$. Then the solution for $y$ is:
    \begin{equation*}
        y(t) = \int_{0}^{t} (t - \tau)f(\tau) d\tau
    \end{equation*}
    We note that this is consistent with Taylor's Theorem:
    \begin{alignat*}{3}
        y(t) &= y(0) &&+ ty'(0) &&+ \int_{0}^{t} (t - \tau)y''(\tau) d\tau \\
        &= 0 &&+ 0 &&+ \int_{0}^{t} (t - \tau)f(\tau) d\tau
    \end{alignat*}
\end{example}
\end{document}