\documentclass[../Main.tex]{subfiles}

\begin{document}
So far we have only been able to find the equilibrium distribution of a Markov chain by calculating the probabilities explicitly for all times. In this chapter we will provide a thorough analysis of the limiting distribution of Markov Chains.
We want to understand when $\P(X_n = i)$ converges to something, and what it converges to under the right assumptions.

\section{Invariant Distributions}
Note that if $\P(X_n = j) \to \pi(j)$ as $n \to \infty$, for some probability distribution $\pi$, then we intuitively understand that $\pi$ must have the property $X_n \sim \pi \implies X_{n+1} \sim \pi$. That is, $\pi$ is a stable distribution. We find:
\begin{align*}
    \pi(j) &= \P(X_{n+1} = j) = \sum_{i \in I} \P(X_n = i, X_{n+1} = j) \\
    &= \sum_{i \in I} \P(X_n = i) P(i, j) \\
    &= \sum_{i \in I} \pi(i) P(i, j)
\end{align*}
Thus we can gain a nice set of simultaneous equations for $\pi$. We can formalise this property:
\begin{definition}{Invariant distribution}
    A distribution $\pi$ is \underline{invariant} for a transition matrix $P$ if:
    \begin{equation*}
        \pi(j) = \sum_{i \in I} \pi(i) P(i, j)~\forall j \in I
    \end{equation*}
\end{definition}
\begin{theorem}
    Let $\chain{X}$ be a Markov chain on a finite state space. Suppose that, for any initial distribution $\vec{\lambda}$, $p_{i, j}(n) \to \pi(j)$ as $n \to \infty$, the $\pi$ is an invariant distribution.
    \label{thmLimitIsInvariant}
\end{theorem}
\begin{proof}
    Consider a 1-step analysis and take limit as $n \to \infty$. We can interchange this with the sum because $I$ is finite.
    \begin{align*}
        p_{ij}(n+1) &= \sum_{k \in I} p_{ik}(n) P(k, j) \\
        \pi(j) = \sum_{k \in I} \pi(k) P(k, j)
    \end{align*}
    Therefore $\pi$ is invariant. Also:
    \begin{equation*}
        \sum_{j \in I}\pi(j) = \sum_{j \in I} \lim_{n \to \infty} p_{ij}(n) = 1
    \end{equation*}
\end{proof}
\begin{remarks}
    \item The assumption that $I$ is finite is critical here. For example, the SSRW in 1 dimension has $p_{xy}(n) \sim \frac{c}{\sqrt{n}}$ for all $n$ which is not a valid probability distribution.
    \item If $I$ is finite then we can easily show that an invariant distribution always exists. If the state space is infinite, we again run into problems (see later).
\end{remarks}
\begin{proposition}
    If $X \sim \text{Markov}(\pi, P)$ where $\pi$ is an invariant distribution for all $P$, then $X_k \sim \pi$ for all $n$.
    \label{propStayAtInvDist}
\end{proposition}
\begin{proof}
    \induction{$n = 0$}{
        We have that $X_0 \sim \pi$ by the fact that $\pi$ is our initial distrubution.
    }{$n = k$}{}
    {$n = k + 1$}{
        \begin{align*}
            \P(X_{n+1} = j) &= \sum_{i \in I} \P(X_n = i, X_{n+1} = j) \\
            &= \sum_{i \in I} \pi(i) P(i, j)
        \end{align*}
    }
\end{proof}
\begin{example}
    Consider a very simple 2-state Markov chain with transition probabilities:
    \begin{equation*}
        \begin{pmatrix}
            1-\alpha & \alpha \\
            \beta & 1-\beta
        \end{pmatrix}
    \end{equation*}
    Then we saw earlier that:
    \begin{equation*}
        p_{00}(n) = \frac{\beta}{\alpha + \beta} + \frac{\alpha}{\alpha + \beta}(1 - \alpha - \beta)^n
    \end{equation*}
    So we have that our transition probabilities tend to:
    \begin{equation*}
        \begin{pmatrix}
            \frac{\beta}{\alpha + \beta} & \frac{\alpha}{\alpha + \beta} \\
            \frac{\alpha}{\alpha + \beta} & \frac{\beta}{\alpha + \beta} \\
        \end{pmatrix}
    \end{equation*}
    We find that $\pi = \left(\frac{\beta}{\alpha + \beta}, \frac{\alpha}{\alpha + \beta}\right)^T$. This is an invariant distribution for the chain and the chain indeed converges to $\pi$.
\end{example}
\begin{definition}{Measure}
    A \underline{measure} on $I$ is a vector $\vec{\lambda} = (\lambda_i)_{i \in I}$ of non-negative elements.
\end{definition}
We can also apply the definition of invariance to a measure:

$\vec{\lambda}$ is invariant if $\vec{\lambda} = \vec{\lambda}P$, and so for each $n$, $\vec{\lambda} = \vec{\lambda} P^n$.
\begin{definition}{First return time}
    For $k \in I$, the \underline{first return time} to $k$ is:
    \begin{equation*}
        T_k = \inf\subsetselect{n \geq 1}{X_n = k}
    \end{equation*}
    We assume that $X_0 = k$ (or start a new Markov chain starting at the time that $X_n = k$.)
\end{definition}
We define a measure $\nu_k$ on $I$ as:
\begin{equation*}
    \nu_k(i) = \E_k{\left(\sum_{n=0}^{T_k - 1} \ind{X_k = i}\right)}
\end{equation*}
Then this is the expected total number of visits to $i$ during an excursion from $k$ back to itself. Note that:
\begin{equation*}
    \nu_k(i) = \E_k\left(\sum_{n=1}^{T_k} \ind{X_n = i}\right)
\end{equation*}
Further,
\begin{align*}
   \sum_{i \in I} v_k(i) &= \sum_{i \in I} \E_k\left(\sum_{n = 0}^{T_k-1} \ind{X_n = i}\right) \\
    &= \E_k\left(\sum_{n = 0}^{T_k-1} \sum_{i \in I} \ind_{X_n = i}\right) \\
    &= \E_k\left(\sum_{n = 0}^{T_k-1} 1\right) \\
    &= \E_k(T_k)
\end{align*}
\begin{theorem}
    Suppose $P$ is irreducible and recurrent. Then:
    \begin{enumerate}
        \item $0 < \nu_k(i) < \infty$ for all $i$;
        \item $\nu_k(k) = 1$;
        \item $\nu_k$ is an invariant measure for $P$.
    \end{enumerate}
    \label{thmMeasureProps}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item Obvious from the above derivation, the first return time starting at $k$ is always 1.
        \item Fix $i \in I$. now:
            \begin{align*}
                \nu_k(i) &= \E_k\left(\sum_{n=1}^{T_k} \ind{X_n = i}\right) \\
                &= \E_k\left(\sum_{n=1}^{\infty}\ind_{X_n = i} \ind{n \leq T_k}\right) \\
                &= \sum_{n=1}^{\infty} \P(X_n = i, T_k \geq n) \\
                &= \sum_{n=1}^{\infty} \sum_{i \in I} \P_k(X_n = i, X_{n-1} = j, T_k \geq n) \\
                &= \sum_{n=1}^{\infty} \sum_{i \in I} \P_k(X_n = i | X_{n-1} = j, T_k \geq n) \P_k(X_{n-1} = j, T_k \geq n) \\
            \end{align*}
            Note that the event $\{T_k \geq n\} = \{T_k \leq n-1\}^C$ which only depends on the states before time $n$. Therefore, by theorem~\ref{thmStrongMarkov},
            \begin{align*}
                \nu_i(k)&= \sum_{n=1}^{\infty} \sum_{j \in I} P(j, i) \P_k(X_{n-1} = j, T_k \geq n) \\
                \nu_i(k)&= \sum_{j \in I} P(j, i) \sum_{n=1}^{\infty} \E_k\left(\ind{X_{n-1} = j} \ind{T_k \geq n}\right) \\
                \nu_i(k) &= \sum_{j \in I} P(j, i) \E_k\left(\sum_{n=1}^{T_k} \ind{X_{n-1} = j}\right) \\
                \nu_i(k) &= \sum_{j \in I} P(j, i) \E_k\left(\sum_{n=0}^{T_k - 1} \ind{X_{n} = j}\right) \\
                &= \sum_{j \in I} P(j, i) \nu_k(j)
            \end{align*}
            So indeed $\nu_k$ is invariant.
        \setcounter{enumi}{0}
        \item Fix $i \in I$. By invariance, $\nu_k = \nu_kP^n$ for any $n$. Therefore, fix also $n \in \N$.
            \begin{equation*}
                \nu_k(i) = \sum_{j \in I} \nu_k(j) p_{ij}(n) \geq p_{ki}(n)
            \end{equation*}
            But by irreducibility, we must have that there exists $n$ such that $p_{ki}(n) > 0$, so $\nu_k(i)$ is positive.

            For finiteness, observe:
            \begin{align*}
                1 &= \nu_k(k) = \sum_{j \in I} \nu_k(j) p_{jk}(n) \\
                &\geq \nu_k(i) p_{ik}(n)
            \end{align*}
            Now again by irreducibility we can choose $n$ such that $p_{ik}(n) > 0$. Therefore $\nu_k(i) \leq \frac{1}{p_{ik}(n)} < \infty$.
    \end{enumerate}
\end{proof}
\begin{theorem}[$\nu_k$ is unique]
    Let $P$ be a transition matrix for a Markov chain. Suppose that $\lambda$ is an invariant measure for $P$ such that $\lambda_k = 1$. Suppose that $P$ is irreducible.

    Then $\lambda \geq \nu_k$, or $\lambda_i \geq \nu_k(i)$ for all $i \in I$.

    Further, if $P$ is also recurrent then $\lambda = \nu_k$.
    \label{thmInvMeasureUnique}
\end{theorem}
\begin{proof}
    By invariance, 
    \begin{align*}
        \lambda_i &= \sum_{j_1 \in I} \lambda_{j_1} P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} \lambda_{j_1} P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} \sum_{j_2 \in I} P(j_2, j_1) P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} P(k, j_1) P(j_1, i) \sum_{j_1, j_2 \neq k} \lambda_{j_2} P(j_2, j_1) P(j_1, i) \\
        &= \cdots \\
        &= P(k, i) + \sum_{j_1 \neq k}P(k, j_1) P(j_1, i) + \cdots \\
        &+ \sum_{j_1, \cdots, j_{n-1} \neq k} P(k, j_{n-1}) P(j_{n-1} j_{n-2}) \cdots P(j_1, i) \\
        &+ \sum_{j_1, \cdots, j_n \neq k} \lambda_{j_n} P(j_n, j_{n-1}) \cdots P(j_1, i)
    \end{align*}
    Now take $i \neq k$.
    \begin{align*}
        \lambda_i &\geq \P_k(X_1 = i, T_k \geq 1) + \P_k(X_2 = i, T_k \geq 2) \\
        &+ \cdots + \P_k(X_n - i, T_k \geq n)
    \end{align*}
    This holds for all $n$ so take $n \to \infty$.
    \begin{align*}
        \lambda_i &\geq \sum_{n=1}^\infty \P_k(X_n = i, T_k \geq n) \\
        &= \sum_{n=1}^{\infty} \E_k \left(\ind{X_n = i, T_k \geq n}\right) \\
        &= \E_k \left(\sum_{n=1}^{T_k} \ind{X_n = i}\right) \\
        &= \nu_k(i)
    \end{align*}
    Therefore we have shown the required inequality.

    Now assume $P$ is recurrent. $\lambda$ and $\nu_k$ are invariant, so we must also have that their difference, $\lambda - \nu_k$, is invariant. Now fix $i \in I$. By invariance:
    \begin{align*}
        0 &= \lambda_k - \nu_k(k) \\
        &= \sum_{j \in I} (\lambda_j - \nu_k(j))P_{jk}(n) \\
    \end{align*}
    Now take $n$ such that $p_{ik}(n) > 0$, which is valid by recurrence.
    \begin{align*}
        0 &\geq (\lambda_i - \nu_k(i)) p_{ik}(0) \geq 0
    \end{align*}
    therefore we have indeed that $\lambda_i - \nu_k(i) = 0$. This holds for all $i$, so we are done.
\end{proof}
\begin{remark}
    Note the condition $\lambda_k = 1$. If this condition does not hold, then in the irreducible and recurrent case any invariant measure is a multiple of $\nu_k$.
\end{remark}
\section{Positive Recurrence}
We have now found that an irreducible and recurrent Markov chain has an invariant measure $\nu_k$. We therefore want to know if this invariant measure can be normalised to give an \textit{invariant distribution}. Recall that:
\begin{equation*}
    \sum_{i \in I} \nu_k(i) = \E_k(T_k)
\end{equation*}
and so, for normalisability, we require that this is finite.
\begin{definition}{Positive recurrence}
    Let $\chain{X}$ be a Markov chain with transition matrix $P$. Suppose $i$ is a recurrent state for $P$. Then $i$ is \underline{positive recurrent} if $\E_i(T_i) < \infty$.

    We say that a state $i$ is \underline{null recurrent} if $\E_i(T_i)$ is infinite.
\end{definition}
\begin{theorem}
    Suppose $P$ is an irreducible transition matrix of a Markov chain. The following are equivalent:
    \begin{enumerate}
        \item All states are positive recurrent
        \item There exists a positive recurrent state
        \item $P$ has an invariant distribution
    \end{enumerate}
    Further, under any (and therefore all) of these conditions, the invariant distribution is given by:
    \begin{equation*}
        \pi(i) = \frac{1}{\E_i(T_i)}
    \end{equation*}
    \label{thmPosRecurEquivs}
\end{theorem}
\begin{proof}
    \begin{subproof}{Statement 1 implies statement 2}
        This is trivial.
    \end{subproof}
    \begin{subproof}{Statement 2 implies statement 3}
        Since there exists a positive recurrent state $k$, and $P$ is irreducible, we must have that $P$ is recurrent (because $k$ is recurrent). By theorem~\ref{thmInvMeasureUnique}, $\nu_k$ is invariant. We also know:
        \begin{equation*}
            \sum_{i \in I} \nu_k(i) = \E_k(T_k) < \infty
        \end{equation*}
        And therefore we can define an invariant distribution:
        \begin{equation*}
            \pi(i) = \frac{\nu_k(i)}{\sum_{j \in I} \nu_k(i)} = \frac{\nu_k(i)}{\E_k(T_k)}
        \end{equation*}
    \end{subproof}
    \begin{subproof}{Statement 3 implies statement 3}
        Fix $k \in I$. First we show that $\pi(k) > 0$.
        \begin{equation*}
            \pi(k) = \sum_{j \in J} \pi(j) p_{jk}(n) \\
        \end{equation*}
        Then choose an $i$ such that $\pi(i) > 0$ (which is valid because $\pi$ is a distribution), and choose $n$ such that $p_{ik}(n) > 0$ by recurrence. Then $\pi(k) \geq \pi(i) p_{ik}(n) > 0$.

        Since $\pi$ is invariant, for a fixed $k$ we can define a new invariant measure:
        \begin{equation*}
            \lambda_i = \frac{\pi(i)}{\pi(k)}, i \in I
        \end{equation*}
        This has $\lambda_k = 1$. Then by theorem~\ref{thmInvMeasureUnique}, $\lambda \geq \nu_k$. That is,
        \begin{align*}
            \E_k(T_k) &= \sum_{i\in I} \nu_k(i) \\
            &\leq \sum_{i \in I} \lambda_i \\
            &= \frac{1}{\pi(k)}
        \end{align*}
        Since $\pi(k)$ was chosen to be positive this quantity is finite. Now, since $k$ was arbitrary, all states must be recurrent.
    \end{subproof}
    Finally, note that since $P$ is irreducible and recurrent we must have that the above $\lambda$ is in fact equal to $\nu_k$, that is:
    \begin{equation*}
        \E_k(T_k) = \frac{1}{\pi(k)}~\forall k \in I
    \end{equation*}
\end{proof}
\begin{corollary}
    If $\chain{X}$ is an irreducible Markov chain with invariant distribution $\pi$, then:
    \begin{equation*}
        \nu_k(i) = \frac{\pi(i)}{\pi(k)}
    \end{equation*}
    for all $i \in I$
    \label{corInvariantQuotient}
\end{corollary}
\begin{proof}
    We have values for $\pi$:
    \begin{equation*}
        \pi(i) = \frac{\nu_k(i)}{\E_k(T_k)},~~\pi(k) = \frac{1}{\E_k(T_k)}.
    \end{equation*}
    Then the quotient gives the result.
\end{proof}
\begin{example}[Simple symmetric random walk]
    The invariance equations are:
    \begin{align*}
        \pi(i) &= \sum_j \pi(j) P(i, j) \\
        &= \frac12 \pi(i-1) + \frac12 \pi(i + 1)
    \end{align*}
    Then setting $\pi(i) = 1~\forall i \in \Z$ gives a solution. Since this chain is irreducible and recurrent, we have that by proposition~\ref{findtheref} all other invariant distributions are multiples of $\pi$.

    However, this cannot be normalised to an invariant distribution! Therefore we get that $\chain{X}$ has an invariant measure but no invariant distribution. This is because $\chain{X}$ is not positive-recurrent.
\end{example}
\begin{example}[Simple asymmetric random walk]
    Consider a simple asymmetric random walk on $\Z$ with transition probabilities:
    \begin{align*}
        P(i, i+1) &= p\\
        P(i, i-1) &= q = 1-p
    \end{align*}
    for all $i \in \Z$, where $p < q$ and both are non-zero.

    Then the invariance equations are:
    \begin{equation*}
        \pi(i) = p \pi(i-1) + q \pi(i + 1)
    \end{equation*}
    We can find the solution:
    \begin{equation*}
        \pi(i) = a + b \left(\frac{p}{q}\right)^i
    \end{equation*}
    However, not all invariant measures are multiples of each other, and so we conclude that $\chain{X}$ is not recurrent.
\end{example}
\begin{example}
    Consider a random walk on $\N_0$. Transition probabilities are, for $i \in \N$,
    \begin{align*}
        P(i, i+1) &= p,\quad P(i, i-1) = q = 1-p\\
        P(0, 1) &= p,\quad P(0, 0) = q
    \end{align*}
    Then the invariance equations are:
    \begin{align*}
        \pi(i) &= p\pi(i-1) + q \pi(i + 1) \\
        \pi(0) &= q\pi(0) + p\pi(1)
    \end{align*}
    We can find the solution:
    \begin{equation*}
        \pi(i) = \left(\frac{p}{q}\right)^i \pi(0)
    \end{equation*}
    Then this can be normalised:
    \begin{equation*}
        \pi(i) = \left(1 - \frac{p}{q}\right)\left(\frac{p}{q}\right)^i
    \end{equation*}
    And therefore we must have that $\chain{X}$ is positive recurrent because it has an invariant distribution.
\end{example}
\section{Reversibility}
\begin{proposition}
    Suppose that $\chain{X}$ is an irreducible Markov Chain with transition matrix $P$. Further, suppose that it has an invariant distribution $\pi$. Let $N \in \N$ and suppose that $\chain{X}$ is \underline{stationary}: $X_0 \sim \pi$.

    Then $\chain{Y} = (Y_n)_{0 \leq n \leq N}$ defined as $Y_n = X_{N - n}$ is a Markov chain with transition matrix:
    \begin{equation}
        \hat{P}(x, y) = P(Y, x) \frac{\pi(y)}{\pi(x)}
        \label{eqnReverseTransitionMat}
    \end{equation}
    Moreover, $\hat{P}$ is also irreducible and $\pi$ is invariant for $\hat{P}$.
    \label{propReverseChain}
\end{proposition}
\begin{proof}
    \begin{subproof}{$\hat{P}$ is a transition matrix.}
        We must check that its row sum to 1:
        \begin{align*}
            \sum_{y \in I} \hat{P}(x, y) &= \frac{1}{\pi(x)} \sum_{y \in I} \pi(y) P(y, x) \\
            &= 1 \text{ because $\pi$ is invariant.}
        \end{align*}
    \end{subproof}
    \begin{subproof}{$Y$ is a Markov chain with transition matrix $\hat{P}$.}
        \begin{align*}
            &\P(Y_0 = y_0, Y_1 = y_1, \cdots, Y_N = y_n) = \\
            &= \P(X_0 = Y_n, X_1 = Y_{N-1}, \cdots, X_n = y_0) \\
            &= \pi(Y_n) P(y_N, y_{N-1}) \cdots P(y_1, y_0) \\
            &= \pi(y_0) \hat{P}(y_0, y_1) \cdots P(Y_{n-1}, Y_N) \text{ by equation~\ref{eqnReverseTransitionMat}}
        \end{align*}
    \end{subproof}
    The irreducibility of $\hat{P}$ follows from teh fact that:
    \begin{equation*}
        P(x, y) > 0 \iff \hat{P}(y, x) > 0
    \end{equation*}
    \begin{subproof}{$\pi$ is an invariant measure for $\hat{P}$.}
        \begin{align*}
            &\sum_{x \in I} \pi(x) \hat{P}(x, y) = \\
            &= \sum_{x \in I} \pi(x) \frac{\pi(y)}{\pi(x)} P(y, x) \\
            &= \pi(y)
        \end{align*}
    \end{subproof}
\end{proof}
\begin{definition}{Time-reversibility}
    A Markov chain $\chain{X}$ with invariant distribution $\pi$ and transition matrix $P$ is \underline{time-reversible} (or just \underline{reversible}) if $\hat{P} = P$.

    This is equivalent to:
    \begin{equation}
        \pi(x) P(x, y) = \pi(y) P(y, x)~\forall x, y \in I
        \label{eqnDetailedBalance}
    \end{equation}
\end{definition}
Equation~\ref{eqnDetailedBalance} are called the \underline{detailed balance equations}. This is a far easier system of equations to solve than the general invariance equations.
\begin{lemma}
    If $\lambda$ satisfies the detailed balance equations for a transition matrix $P$, i.e.
    \begin{equation*}
        \lambda(x) P(x, y) = \lambda(y) P(y, x)~\forall x, y \in I,
    \end{equation*}
    then $\lambda$ is an invariant measure for $P$.
    \label{eqnDBImpliesInvariant}
\end{lemma}
\begin{proof}
    \begin{align*}
        \sum_{x \in I} \lambda(x) P(x, y) &= \sum_{x \in I} \lambda(y) P(y, x) \\
        &= \lambda(y)
    \end{align*}
\end{proof}
\begin{example}[Asymmetric random walk on the circle $\Z_n$]
    Consider the state space $\Z / n \Z$. Define:
    \begin{align*}
        P(i, i+1 \text{ mod } n) &= \frac23 \\
        P(i, i-1 \text{ mod } n) &= \frac13
    \end{align*}
    We can verify an invariant distribution $\pi(n) = \frac1n$,
    \begin{equation*}
        \frac1n = \frac1n\frac23 + \frac1n \frac13
    \end{equation*}
    But the detailed balance equations are not satisfied:
    \begin{equation*}
        \frac1n \frac23 \neq \frac1n \frac13
    \end{equation*}
\end{example}
\begin{example}[Asymmetric random walk on the line $\Z_n$]
    Consider a random walk with transition probabilities:
    \begin{align*}
        &P(0, 1) = \frac23,\quad P(0, 0) = \frac13 \\
        &P(n-1, n-1) = \frac23,\quad P(n-1, n-2) = \frac13 \\
        &P(i, i+1) = \frac23,\quad P(i, i-1) = \frac13 \text{ otherwise}
    \end{align*}
    Consider the detailed balance equations:
    \begin{equation*}
        \frac23\lambda(i) = \frac13 \lambda(i-2)
    \end{equation*}
    Then we can find a solution $\lambda(i) = 2^i$, which can be normalised.

    This tells us that the chain is reversible as we can find an invariant distribution.

    The intuition behind the reversibility of this chain is that most of the time we are on the right, so we are forced to move left when we are at the right endpoint. This compensates for the fact that we have a higher probability of moving to the right in general.
\end{example}
\begin{example}
    Suppose $G = (E, V)$ is a finite, connected, undirected graph such that the degree of each vertex is $d(x)$. Consider a symmetric random walk on $G$ with transition probabilities:
    \begin{equation*}
        P(x, y) =
        \begin{cases}
            \frac1{d(x)} & (x, y) \in E \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    Then $\pi(x) = \frac{d(x)}{2|E|}$ is an invariant measure that satisfies the detailed balance equations.
    \begin{align*}
        \pi(x) P(x, y) &= \frac{d(x)}{2|E|} \frac{1}{d(x)} \\
        &= \frac{1}{2|E|} \\
        &= \frac{d(y)}{2|E|} \frac{1}{d(y)} = \pi(y) P(y, x)
    \end{align*}
\end{example}
\end{document}