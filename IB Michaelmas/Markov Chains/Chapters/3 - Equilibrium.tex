\documentclass[../Main.tex]{subfiles}

\begin{document}
So far we have only been able to find the equilibrium distribution of a Markov chain by calculating the probabilities explicitly for all times. In this chapter we will provide a thorough analysis of the limiting distribution of Markov Chains.
We want to understand when $\P(X_n = i)$ converges to something, and what it converges to under the right assumptions.

\section{Invariant Distributions}
Note that if $\P(X_n = j) \to \pi(j)$ as $n \to \infty$, for some probability distribution $\pi$, then we intuitively understand that $\pi$ must have the property $X_n \sim \pi \implies X_{n+1} \sim \pi$. That is, $\pi$ is a stable distribution. We find:
\begin{align*}
    \pi(j) &= \P(X_{n+1} = j) = \sum_{i \in I} \P(X_n = i, X_{n+1} = j) \\
    &= \sum_{i \in I} \P(X_n = i) P(i, j) \\
    &= \sum_{i \in I} \pi(i) P(i, j)
\end{align*}
Thus we can gain a nice set of simultaneous equations for $\pi$. We can formalise this property:
\begin{definition}{Invariant distribution}
    A distribution $\pi$ is \underline{invariant} for a transition matrix $P$ if:
    \begin{equation*}
        \pi(j) = \sum_{i \in I} \pi(i) P(i, j)~\forall j \in I
    \end{equation*}
\end{definition}
\begin{theorem}
    Let $\chain{X}$ be a Markov chain on a finite state space. Suppose that, for any initial distribution $\vec{\lambda}$, $p_{i, j}(n) \to \pi(j)$ as $n \to \infty$, the $\pi$ is an invariant distribution.
    \label{thmLimitIsInvariant}
\end{theorem}
\begin{proof}
    Consider a 1-step analysis and take limit as $n \to \infty$. We can interchange this with the sum because $I$ is finite.
    \begin{align*}
        p_{ij}(n+1) &= \sum_{k \in I} p_{ik}(n) P(k, j) \\
        \pi(j) = \sum_{k \in I} \pi(k) P(k, j)
    \end{align*}
    Therefore $\pi$ is invariant. Also:
    \begin{equation*}
        \sum_{j \in I}\pi(j) = \sum_{j \in I} \lim_{n \to \infty} p_{ij}(n) = 1
    \end{equation*}
\end{proof}
\begin{remarks}
    \item The assumption that $I$ is finite is critical here. For example, the SSRW in 1 dimension has $p_{xy}(n) \sim \frac{c}{\sqrt{n}}$ for all $n$ which is not a valid probability distribution.
    \item If $I$ is finite then we can easily show that an invariant distribution always exists. If the state space is infinite, we again run into problems (see later).
\end{remarks}
\begin{proposition}
    If $X \sim \text{Markov}(\pi, P)$ where $\pi$ is an invariant distribution for all $P$, then $X_k \sim \pi$ for all $n$.
    \label{propStayAtInvDist}
\end{proposition}
\begin{proof}
    \induction{$n = 0$}{
        We have that $X_0 \sim \pi$ by the fact that $\pi$ is our initial distrubution.
    }{$n = k$}{}
    {$n = k + 1$}{
        \begin{align*}
            \P(X_{n+1} = j) &= \sum_{i \in I} \P(X_n = i, X_{n+1} = j) \\
            &= \sum_{i \in I} \pi(i) P(i, j)
        \end{align*}
    }
\end{proof}
\begin{example}
    Consider a very simple 2-state Markov chain with transition probabilities:
    \begin{equation*}
        \begin{pmatrix}
            1-\alpha & \alpha \\
            \beta & 1-\beta
        \end{pmatrix}
    \end{equation*}
    Then we saw earlier that:
    \begin{equation*}
        p_{00}(n) = \frac{\beta}{\alpha + \beta} + \frac{\alpha}{\alpha + \beta}(1 - \alpha - \beta)^n
    \end{equation*}
    So we have that our transition probabilities tend to:
    \begin{equation*}
        \begin{pmatrix}
            \frac{\beta}{\alpha + \beta} & \frac{\alpha}{\alpha + \beta} \\
            \frac{\alpha}{\alpha + \beta} & \frac{\beta}{\alpha + \beta} \\
        \end{pmatrix}
    \end{equation*}
    We find that $\pi = \left(\frac{\beta}{\alpha + \beta}, \frac{\alpha}{\alpha + \beta}\right)^T$. This is an invariant distribution for the chain and the chain indeed converges to $\pi$.
\end{example}
\begin{definition}{Measure}
    A \underline{measure} on $I$ is a vector $\vec{\lambda} = (\lambda_i)_{i \in I}$ of non-negative elements.
\end{definition}
We can also apply the definition of invariance to a measure:

$\vec{\lambda}$ is invariant if $\vec{\lambda} = \vec{\lambda}P$, and so for each $n$, $\vec{\lambda} = \vec{\lambda} P^n$.
\begin{definition}{First return time}
    For $k \in I$, the \underline{first return time} to $k$ is:
    \begin{equation*}
        T_k = \inf\subsetselect{n \geq 1}{X_n = k}
    \end{equation*}
    We assume that $X_0 = k$ (or start a new Markov chain starting at the time that $X_n = k$.)
\end{definition}
We define a measure $\nu_k$ on $I$ as:
\begin{equation*}
    \nu_k(i) = \E_k{\left(\sum_{n=0}^{T_k - 1} \ind{X_k = i}\right)}
\end{equation*}
Then this is the expected total number of visits to $i$ during an excursion from $k$ back to itself. Note that:
\begin{equation*}
    \nu_k(i) = \E_k\left(\sum_{n=1}^{T_k} \ind{X_n = i}\right)
\end{equation*}
Further,
\begin{align*}
   \sum_{i \in I} v_k(i) &= \sum_{i \in I} \E_k\left(\sum_{n = 0}^{T_k-1} \ind{X_n = i}\right) \\
    &= \E_k\left(\sum_{n = 0}^{T_k-1} \sum_{i \in I} \ind_{X_n = i}\right) \\
    &= \E_k\left(\sum_{n = 0}^{T_k-1} 1\right) \\
    &= \E_k(T_k)
\end{align*}
\begin{theorem}
    Suppose $P$ is irreducible and recurrent. Then:
    \begin{enumerate}
        \item $0 < \nu_k(i) < \infty$ for all $i$;
        \item $\nu_k(k) = 1$;
        \item $\nu_k$ is an invariant measure for $P$.
    \end{enumerate}
    \label{thmMeasureProps}
\end{theorem}
\begin{proof}
    \begin{enumerate}
        \setcounter{enumi}{1}
        \item Obvious from the above derivation, the first return time starting at $k$ is always 1.
        \item Fix $i \in I$. now:
            \begin{align*}
                \nu_k(i) &= \E_k\left(\sum_{n=1}^{T_k} \ind{X_n = i}\right) \\
                &= \E_k\left(\sum_{n=1}^{\infty}\ind_{X_n = i} \ind{n \leq T_k}\right) \\
                &= \sum_{n=1}^{\infty} \P(X_n = i, T_k \geq n) \\
                &= \sum_{n=1}^{\infty} \sum_{i \in I} \P_k(X_n = i, X_{n-1} = j, T_k \geq n) \\
                &= \sum_{n=1}^{\infty} \sum_{i \in I} \P_k(X_n = i | X_{n-1} = j, T_k \geq n) \P_k(X_{n-1} = j, T_k \geq n) \\
            \end{align*}
            Note that the event $\{T_k \geq n\} = \{T_k \leq n-1\}^C$ which only depends on the states before time $n$. Therefore, by theorem~\ref{thmStrongMarkov},
            \begin{align*}
                \nu_i(k)&= \sum_{n=1}^{\infty} \sum_{j \in I} P(j, i) \P_k(X_{n-1} = j, T_k \geq n) \\
                \nu_i(k)&= \sum_{j \in I} P(j, i) \sum_{n=1}^{\infty} \E_k\left(\ind{X_{n-1} = j} \ind{T_k \geq n}\right) \\
                \nu_i(k) &= \sum_{j \in I} P(j, i) \E_k\left(\sum_{n=1}^{T_k} \ind{X_{n-1} = j}\right) \\
                \nu_i(k) &= \sum_{j \in I} P(j, i) \E_k\left(\sum_{n=0}^{T_k - 1} \ind{X_{n} = j}\right) \\
                &= \sum_{j \in I} P(j, i) \nu_k(j)
            \end{align*}
            So indeed $\nu_k$ is invariant.
        \setcounter{enumi}{0}
        \item Fix $i \in I$. By invariance, $\nu_k = \nu_kP^n$ for any $n$. Therefore, fix also $n \in \N$.
            \begin{equation*}
                \nu_k(i) = \sum_{j \in I} \nu_k(j) p_{ij}(n) \geq p_{ki}(n)
            \end{equation*}
            But by irreducibility, we must have that there exists $n$ such that $p_{ki}(n) > 0$, so $\nu_k(i)$ is positive.

            For finiteness, observe:
            \begin{align*}
                1 &= \nu_k(k) = \sum_{j \in I} \nu_k(j) p_{jk}(n) \\
                &\geq \nu_k(i) p_{ik}(n)
            \end{align*}
            Now again by irreducibility we can choose $n$ such that $p_{ik}(n) > 0$. Therefore $\nu_k(i) \leq \frac{1}{p_{ik}(n)} < \infty$.
    \end{enumerate}
\end{proof}
\begin{theorem}[$\nu_k$ is unique]
    Let $P$ be a transition matrix for a Markov chain. Suppose that $\lambda$ is an invariant measure for $P$ such that $\lambda_k = 1$. Suppose that $P$ is irreducible.

    Then $\lambda \geq \nu_k$, or $\lambda_i \geq \nu_k(i)$ for all $i \in I$.

    Further, if $P$ is also recurrent then $\lambda = \nu_k$.
    \label{thmInvMeasureUnique}
\end{theorem}
\begin{proof}
    By invariance, 
    \begin{align*}
        \lambda_i &= \sum_{j_1 \in I} \lambda_{j_1} P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} \lambda_{j_1} P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} \sum_{j_2 \in I} P(j_2, j_1) P(j_1, i) \\
        &= P(k, i) + \sum_{j_1 \neq k} P(k, j_1) P(j_1, i) \sum_{j_1, j_2 \neq k} \lambda_{j_2} P(j_2, j_1) P(j_1, i) \\
        &= \cdots \\
        &= P(k, i) + \sum_{j_1 \neq k}P(k, j_1) P(j_1, i) + \cdots \\
        &+ \sum_{j_1, \cdots, j_{n-1} \neq k} P(k, j_{n-1}) P(j_{n-1} j_{n-2}) \cdots P(j_1, i) \\
        &+ \sum_{j_1, \cdots, j_n \neq k} \lambda_{j_n} P(j_n, j_{n-1}) \cdots P(j_1, i)
    \end{align*}
    Now take $i \neq k$.
    \begin{align*}
        \lambda_i &\geq \P_k(X_1 = i, T_k \geq 1) + \P_k(X_2 = i, T_k \geq 2) \\
        &+ \cdots + \P_k(X_n - i, T_k \geq n)
    \end{align*}
    This holds for all $n$ so take $n \to \infty$.
    \begin{align*}
        \lambda_i &\geq \sum_{n=1}^\infty \P_k(X_n = i, T_k \geq n) \\
        &= \sum_{n=1}^{\infty} \E_k \left(\ind{X_n = i, T_k \geq n}\right) \\
        &= \E_k \left(\sum_{n=1}^{T_k} \ind{X_n = i}\right) \\
        &= \nu_k(i)
    \end{align*}
    Therefore we have shown the required inequality.

    Now assume $P$ is recurrent. $\lambda$ and $\nu_k$ are invariant, so we must also have that their difference, $\lambda - \nu_k$, is invariant. Now fix $i \in I$. By invariance:
    \begin{align*}
        0 &= \lambda_k - \nu_k(k) \\
        &= \sum_{j \in I} (\lambda_j - \nu_k(j))P_{jk}(n) \\
    \end{align*}
    Now take $n$ such that $p_{ik}(n) > 0$, which is valid by recurrence.
    \begin{align*}
        0 &\geq (\lambda_i - \nu_k(i)) p_{ik}(0) \geq 0
    \end{align*}
    therefore we have indeed that $\lambda_i - \nu_k(i) = 0$. This holds for all $i$, so we are done.
\end{proof}
\begin{remark}
    Note the condition $\lambda_k = 1$. If this condition does not hold, then in the irreducible and recurrent case any invariant measure is a multiple of $\nu_k$.
\end{remark}
\section{Positive Recurrence}
We have now found that an irreducible and recurrent Markov chain has an invariant measure $\nu_k$. We therefore want to know if this invariant measure can be normalised to give an \textit{invariant distribution}. Recall that:
\begin{equation*}
    \sum_{i \in I} \nu_k(i) = \E_k(T_k)
\end{equation*}
and so, for normalisability, we require that this is finite.
\begin{definition}{Positive recurrence}
    Let $\chain{X}$ be a Markov chain with transition matrix $P$. Suppose $i$ is a recurrent state for $P$. Then $i$ is \underline{positive recurrent} if $\E_i(T_i) < \infty$.

    We say that a state $i$ is \underline{null recurrent} if $\E_i(T_i)$ is infinite.
\end{definition}
\begin{theorem}
    Suppose $P$ is an irreducible transition matrix of a Markov chain. The following are equivalent:
    \begin{enumerate}
        \item All states are positive recurrent
        \item There exists a positive recurrent state
        \item $P$ has an invariant distribution
    \end{enumerate}
    Further, under any (and therefore all) of these conditions, the invariant distribution is given by:
    \begin{equation*}
        \pi(i) = \frac{1}{\E_i(T_i)}
    \end{equation*}
    \label{thmPosRecurEquivs}
\end{theorem}
\begin{proof}
    \begin{subproof}{Statement 1 implies statement 2}
        This is trivial.
    \end{subproof}
    \begin{subproof}{Statement 2 implies statement 3}
        Since there exists a positive recurrent state $k$, and $P$ is irreducible, we must have that $P$ is recurrent (because $k$ is recurrent). By theorem~\ref{thmInvMeasureUnique}, $\nu_k$ is invariant. We also know:
        \begin{equation*}
            \sum_{i \in I} \nu_k(i) = \E_k(T_k) < \infty
        \end{equation*}
        And therefore we can define an invariant distribution:
        \begin{equation*}
            \pi(i) = \frac{\nu_k(i)}{\sum_{j \in I} \nu_k(i)} = \frac{\nu_k(i)}{\E_k(T_k)}
        \end{equation*}
    \end{subproof}
    \begin{subproof}{Statement 3 implies statement 3}
        Fix $k \in I$. First we show that $\pi(k) > 0$.
        \begin{equation*}
            \pi(k) = \sum_{j \in J} \pi(j) p_{jk}(n) \\
        \end{equation*}
        Then choose an $i$ such that $\pi(i) > 0$ (which is valid because $\pi$ is a distribution), and choose $n$ such that $p_{ik}(n) > 0$ by recurrence. Then $\pi(k) \geq \pi(i) p_{ik}(n) > 0$.

        Since $\pi$ is invariant, for a fixed $k$ we can define a new invariant measure:
        \begin{equation*}
            \lambda_i = \frac{\pi(i)}{\pi(k)}, i \in I
        \end{equation*}
        This has $\lambda_k = 1$. Then by theorem~\ref{thmInvMeasureUnique}, $\lambda \geq \nu_k$. That is,
        \begin{align*}
            \E_k(T_k) &= \sum_{i\in I} \nu_k(i) \\
            &\leq \sum_{i \in I} \lambda_i \\
            &= \frac{1}{\pi(k)}
        \end{align*}
        Since $\pi(k)$ was chosen to be positive this quantity is finite. Now, since $k$ was arbitrary, all states must be recurrent.
    \end{subproof}
    Finally, note that since $P$ is irreducible and recurrent we must have that the above $\lambda$ is in fact equal to $\nu_k$, that is:
    \begin{equation*}
        \E_k(T_k) = \frac{1}{\pi(k)}~\forall k \in I
    \end{equation*}
\end{proof}
\begin{corollary}
    If $\chain{X}$ is an irreducible Markov chain with invariant distribution $\pi$, then:
    \begin{equation*}
        \nu_k(i) = \frac{\pi(i)}{\pi(k)}
    \end{equation*}
    for all $i \in I$
    \label{corInvariantQuotient}
\end{corollary}
\begin{proof}
    We have values for $\pi$:
    \begin{equation*}
        \pi(i) = \frac{\nu_k(i)}{\E_k(T_k)},~~\pi(k) = \frac{1}{\E_k(T_k)}.
    \end{equation*}
    Then the quotient gives the result.
\end{proof}
\begin{example}[Simple symmetric random walk]
    The invariance equations are:
    \begin{align*}
        \pi(i) &= \sum_j \pi(j) P(i, j) \\
        &= \frac12 \pi(i-1) + \frac12 \pi(i + 1)
    \end{align*}
    Then setting $\pi(i) = 1~\forall i \in \Z$ gives a solution. Since this chain is irreducible and recurrent, we have that by theorem~\ref{thmInvMeasureUnique} all other invariant distributions are multiples of $\pi$.

    However, this cannot be normalised to an invariant distribution! Therefore we get that $\chain{X}$ has an invariant measure but no invariant distribution. This is because $\chain{X}$ is not positive-recurrent.
\end{example}
\begin{example}[Simple asymmetric random walk]
    Consider a simple asymmetric random walk on $\Z$ with transition probabilities:
    \begin{align*}
        P(i, i+1) &= p\\
        P(i, i-1) &= q = 1-p
    \end{align*}
    for all $i \in \Z$, where $p < q$ and both are non-zero.

    Then the invariance equations are:
    \begin{equation*}
        \pi(i) = p \pi(i-1) + q \pi(i + 1)
    \end{equation*}
    We can find the solution:
    \begin{equation*}
        \pi(i) = a + b \left(\frac{p}{q}\right)^i
    \end{equation*}
    However, not all invariant measures are multiples of each other, and so we conclude that $\chain{X}$ is not recurrent.
\end{example}
\begin{example}
    Consider a random walk on $\N_0$. Transition probabilities are, for $i \in \N$,
    \begin{align*}
        P(i, i+1) &= p,\quad P(i, i-1) = q = 1-p\\
        P(0, 1) &= p,\quad P(0, 0) = q
    \end{align*}
    Then the invariance equations are:
    \begin{align*}
        \pi(i) &= p\pi(i-1) + q \pi(i + 1) \\
        \pi(0) &= q\pi(0) + p\pi(1)
    \end{align*}
    We can find the solution:
    \begin{equation*}
        \pi(i) = \left(\frac{p}{q}\right)^i \pi(0)
    \end{equation*}
    Then this can be normalised:
    \begin{equation*}
        \pi(i) = \left(1 - \frac{p}{q}\right)\left(\frac{p}{q}\right)^i
    \end{equation*}
    And therefore we must have that $\chain{X}$ is positive recurrent because it has an invariant distribution.
\end{example}
\section{Reversibility}
\begin{proposition}
    Suppose that $\chain{X}$ is an irreducible Markov Chain with transition matrix $P$. Further, suppose that it has an invariant distribution $\pi$. Let $N \in \N$ and suppose that $\chain{X}$ is \underline{stationary}: $X_0 \sim \pi$.

    Then $\chain{Y} = (Y_n)_{0 \leq n \leq N}$ defined as $Y_n = X_{N - n}$ is a Markov chain with transition matrix:
    \begin{equation}
        \hat{P}(x, y) = P(y, x) \frac{\pi(y)}{\pi(x)}
        \label{eqnReverseTransitionMat}
    \end{equation}
    Moreover, $\hat{P}$ is also irreducible and $\pi$ is invariant for $\hat{P}$.
    \label{propReverseChain}
\end{proposition}
\begin{proof}
    \begin{subproof}{$\hat{P}$ is a transition matrix.}
        We must check that its row sum to 1:
        \begin{align*}
            \sum_{y \in I} \hat{P}(x, y) &= \frac{1}{\pi(x)} \sum_{y \in I} \pi(y) P(y, x) \\
            &= 1 \text{ because $\pi$ is invariant.}
        \end{align*}
    \end{subproof}
    \begin{subproof}{$Y$ is a Markov chain with transition matrix $\hat{P}$.}
        \begin{align*}
            &\P(Y_0 = y_0, Y_1 = y_1, \cdots, Y_N = y_n) = \\
            &= \P(X_0 = Y_n, X_1 = Y_{N-1}, \cdots, X_n = y_0) \\
            &= \pi(Y_n) P(y_N, y_{N-1}) \cdots P(y_1, y_0) \\
            &= \pi(y_0) \hat{P}(y_0, y_1) \cdots P(Y_{n-1}, Y_N) \text{ by equation~\ref{eqnReverseTransitionMat}}
        \end{align*}
    \end{subproof}
    The irreducibility of $\hat{P}$ follows from teh fact that:
    \begin{equation*}
        P(x, y) > 0 \iff \hat{P}(y, x) > 0
    \end{equation*}
    \begin{subproof}{$\pi$ is an invariant measure for $\hat{P}$.}
        \begin{align*}
            &\sum_{x \in I} \pi(x) \hat{P}(x, y) = \\
            &= \sum_{x \in I} \pi(x) \frac{\pi(y)}{\pi(x)} P(y, x) \\
            &= \pi(y)
        \end{align*}
    \end{subproof}
\end{proof}
\begin{definition}{Time-reversibility}
    A Markov chain $\chain{X}$ with invariant distribution $\pi$ and transition matrix $P$ is \underline{time-reversible} (or just \underline{reversible}) if $\hat{P} = P$.

    This is equivalent to:
    \begin{equation}
        \pi(x) P(x, y) = \pi(y) P(y, x)~\forall x, y \in I
        \label{eqnDetailedBalance}
    \end{equation}
\end{definition}
Equation~\ref{eqnDetailedBalance} are called the \underline{detailed balance equations}. This is a far easier system of equations to solve than the general invariance equations.
\begin{lemma}
    If $\lambda$ satisfies the detailed balance equations for a transition matrix $P$, i.e.
    \begin{equation*}
        \lambda(x) P(x, y) = \lambda(y) P(y, x)~\forall x, y \in I,
    \end{equation*}
    then $\lambda$ is an invariant measure for $P$.
    \label{eqnDBImpliesInvariant}
\end{lemma}
\begin{proof}
    \begin{align*}
        \sum_{x \in I} \lambda(x) P(x, y) &= \sum_{x \in I} \lambda(y) P(y, x) \\
        &= \lambda(y)
    \end{align*}
\end{proof}
\begin{example}[Asymmetric random walk on the circle $\Z_n$]
    Consider the state space $\Z / n \Z$. Define:
    \begin{align*}
        P(i, i+1 \text{ mod } n) &= \frac23 \\
        P(i, i-1 \text{ mod } n) &= \frac13
    \end{align*}
    We can verify an invariant distribution $\pi(n) = \frac1n$,
    \begin{equation*}
        \frac1n = \frac1n\frac23 + \frac1n \frac13
    \end{equation*}
    But the detailed balance equations are not satisfied:
    \begin{equation*}
        \frac1n \frac23 \neq \frac1n \frac13
    \end{equation*}
\end{example}
\begin{example}[Asymmetric random walk on the line $\Z_n$]
    Consider a random walk with transition probabilities:
    \begin{align*}
        &P(0, 1) = \frac23,\quad P(0, 0) = \frac13 \\
        &P(n-1, n-1) = \frac23,\quad P(n-1, n-2) = \frac13 \\
        &P(i, i+1) = \frac23,\quad P(i, i-1) = \frac13 \text{ otherwise}
    \end{align*}
    Consider the detailed balance equations:
    \begin{equation*}
        \frac23\lambda(i) = \frac13 \lambda(i-2)
    \end{equation*}
    Then we can find a solution $\lambda(i) = 2^i$, which can be normalised.

    This tells us that the chain is reversible as we can find an invariant distribution.

    The intuition behind the reversibility of this chain is that most of the time we are on the right, so we are forced to move left when we are at the right endpoint. This compensates for the fact that we have a higher probability of moving to the right in general.
\end{example}
\begin{example}
    Suppose $G = (E, V)$ is a finite, connected, undirected graph such that the degree of each vertex is $d(x)$. Consider a symmetric random walk on $G$ with transition probabilities:
    \begin{equation*}
        P(x, y) =
        \begin{cases}
            \frac1{d(x)} & (x, y) \in E \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    Then $\pi(x) = \frac{d(x)}{2|E|}$ is an invariant measure that satisfies the detailed balance equations.
    \begin{align*}
        \pi(x) P(x, y) &= \frac{d(x)}{2|E|} \frac{1}{d(x)} \\
        &= \frac{1}{2|E|} \\
        &= \frac{d(y)}{2|E|} \frac{1}{d(y)} = \pi(y) P(y, x)
    \end{align*}
\end{example}
\section{Convergence of Markov Chains}
\subsection{Conditions for Convergence}
The goal of this section is to understand the conditions required for a Markov chain to converge to a limit distribution. We want to avoid the following problematic cases:
\begin{itemize}
    \item Irreducibility: if a Markov chain is reducible, we can therefore reduce it into irreducible components. Therefore, we can assume irreducibility without loss of generality.
    \item Periodic phenomena: if a random walk has periodic phenomena, such as a state only being reachable at time that are a multiple of 2, we want to discount this case
    \item No limit: We are going to require that the Markov chain has an invariant distribution to converge to.
\end{itemize}
\begin{definition}{Period}
    The \underline{period} of a state $i \in I$ in a Markov chain $\chain{X}$ with transition matrix $P$ is:
    \begin{equation*}
        d_i = \operatorname{hcf}\subsetselect{n \geq 1}{p_{ii}(n) > 0}
    \end{equation*}
\end{definition}
\begin{definition}{Aperiodicity}
    Consider a Markov chain $\chain{X}$ with transition matrix $P$. A state $i$ is \underline{aperiodic} if $d_i = 1$. The transition matrix $P$ is \underline{aperiodic} if all states are aperiodic.
\end{definition}
\begin{lemma}
    Suppose that the set $A$ consisting of non-negative integers is such that $m, n \in A \implies m + n \in A$ (that is, $A$ is closed under addition). If $\operatorname{hcf}(A) = 1$ then there is a number $N$ such that all $n \geq N$ are contained within $A$.
    \label{lemHCFSet}
\end{lemma}
\begin{remark}
    We will not provide a proof of this statement. We also remark that the converse holds, if we have a subset of $\N$ closed under addition, then the $\operatorname{hcf}$ will be $1$ if there exists a number $N \in \N$ such that the subset contains all numbers after $N$.
\end{remark}
\begin{corollary}
    A state $i \in I$ is aperiodic if and only if there exists an $N_i \in \N$ such that $p_{ii}(n) > 0$ for all $n \geq N_i$.
    \label{corAperiodicEquiv}
\end{corollary}
\begin{lemma}
    If $P$ is an irreducible transition matrix and a state $i$ is aperiodic, then $P$ is aperiodic (all states are aperiodic).
    \label{lemOneAperiodicAllAperiodic}
\end{lemma}
\begin{proof}
    Fix $j \in I$. By irreducibility there exists $m \in \N$ such that $p_{ij}(m) > 0$  and there exists $k$ such that $p_{ji}(k) > 0$. Then:
    \begin{align*}
        p_{ij}(k + n + m) &\geq p_{ji}(k) p_{ii}(n) p_{ij}(m) \\
        &> 0 \text{ for $n \geq N_i$ from corollary~\ref{corAperiodicEquiv}.}        
    \end{align*}
\end{proof}
\subsection{Fundamental Convergence Theorem}
\begin{theorem}[Fundamental Convergence Theorem of Markov Chains]
    Suppose that $\chain{X}$ is a Markov chain with transition matrix $P$ and initial distribution $\lambda$. If $P$ is irreducible and aperiodic with invariant distribution $\pi$ then:
    \begin{equation*}
        \P(X_n = j) \to \pi(j) \text{ as $n \to \infty$ for all $j \in I$.}
    \end{equation*}
    In particular, taking $\lambda = \delta_i$ then we get:
    \begin{equation*}
        p_{ij}(n) \to \pi(j) \text{ as $n \to \infty$ for all $i, j \in I$.}
    \end{equation*}
    \label{thmFundamentalConvergence}
\end{theorem}
\begin{proof}
    The proof proceeds by the \textit{coupling method}. We want to take the Markov chain $\chain{X}$, and also a new independent Markov chain $\chain{Y} \sim \operatorname{Markov}(\pi, P)$. Then we will show that after these two Markov chains reach the same state then they have the same distribution.

    Define $\chain{W} = \left(W_n\right)_{n \geq 0}$ as $W_n = (X_n, Y_n)$. Then this is a Markov chain on $I \times I$ with transition matrix:
    \begin{equation*}
        \tilde{P}((x, y), (x', y')) = P(x, x')P(y, y')
    \end{equation*}
    Define also $\tilde{\pi}(x, y) = \pi(x) \pi(y)$.
    \begin{subproof}{$\tilde{\pi}$ is invariant for $\tilde{P}$.}
        Consider the invariance equations:
        \begin{align*}
            \sum_{(x, y) \in I \times I} &\tilde{\pi}(x, y) \tilde{P}((x, y), (x', y')) \\
            &= \sum_{(x, y) \in I \times I} \pi(x) \pi(y) P(x, x') P(y, y') \\
            &= \sum_{x \in I} \pi(x) P(x, x') \sum_{y \in I} \pi(y) P(y, y') \\
            &= \pi(x') \pi(y') = \pi(x', y')
        \end{align*}
    \end{subproof}
    \begin{subproof}{$\chain{W}$ is irreducible.}
        Let $(x, y), (x' ,y')$ be states in $I \times I$. The probability to get between them in $n$ steps is:
        \begin{align*}
            \tilde{P}^n&((x, y), (x', y')) = p_{xx'}(n) p_{yy'}(n) \\
            &\geq p_{xx'}(k) p_{x'x'}(n-k) p_{yy'}(l) p_{y'y'}(n-l)
        \end{align*}
        then by irreducibility of $\chain{X}$, we can choose $k, l$ such that $p_{xx'}(k), p_{yy'}(l) > 0$ and by aperiodicity if $n - k \geq N_{x'}$ and $n - l \geq N_{y'}$ then the other two terms are positive so:
        \begin{equation*}
            \tilde{P}^n((x, y), (x', y')) > 0 \text{ for $n$ large enough.}
        \end{equation*}
    \end{subproof}
    Fix $a \in I$ and define:
    \begin{equation*}
        T = \inf\subsetselect{n \geq 1}{W_n = (a, a)}
    \end{equation*}
    This is the point from which the chains $\chain{X}$ and $\chain{Y}$ are coupled. Since we have an invariant distribution we know, by theorem~\ref{thmPosRecurEquivs}, that $\tilde{P}$ must be positive recurrent, and so the state $(a, a)$ is reached in finite time: $\P(T < \infty) = 1$.

    Define $\chain{Z}$ to be the Markov chain that follows $\chain{X}$ initially, until $\chain{X}$ and $\chain{Y}$ meet, and then follows $\chain{Y}$:
    \begin{equation*}
        Z_n = \begin{cases}
            X_n & n < T \\
            Y_n & n \geq T
        \end{cases}
    \end{equation*}
    \begin{subproof}{$Z \sim \operatorname{Markov}(\lambda, P)$}
        Since $T \geq 1$, by definition $Z_0 = X_0 \sim \lambda$.
        For the Markov property, define the event $B$:
        \begin{equation*}
            B = \{Z_0 = z_0, \cdots, Z_{n-1} = z_{n-1}\}
        \end{equation*}
        We consider transition probabilities for $Z$:
        \begin{align*}
            &\P(Z_{n+1} = y | Z_n = x, B) \\
            &= \P(Z_{n+1} = y, T > n | Z_n = x, B)\\
                &\qquad+ \P(Z_{n+1} = y, T \leq n | Z_n = x, B) \\
            &= \P(Z_{n+1} = y | T > n, Z_n = x, B) \P(T > n | Z_n = x, B) \\
                &\qquad+ \P(Z_{n+1} = y | T \leq n, Z_n = x, B) \P(T \leq n | Z_n = x, B) \\
            \intertext{Define probabilities $C$ and $D$:}
            &= C \times \P(T > n | Z_n = x, B) + D \times \P(T \leq n | Z_n = x, B)
        \end{align*}
        Using the Strong Markov property, we can find $C$.
        \begin{align*}
            C &= \P(Z_{n+1} = y | T > n, Z_n = x, B)\\
            C &= \P(X_{n+1} = y | T > n, X_n = x, B) \text{ because $T \leq n+1$} \\
            &= \sum_{z \in I} \P(X_{n+1} = y | T > n, X_n = x, Y_n = z, B) \\
                &\qquad\times \P(Y_n = z | T > n, X_n = x, B)\\
            \intertext{Then by the Strong Markov property for $W$,}
            &= \sum_{z \in I} P(x, y) \P(Y_n = z | T > n, X_n = x, B)\\
            &= \sum_{z \in I} P(x, y) \P(Y_n = z)\;\parbox{15em}{because none of the conditions apply to $Y_n$}\\
            &= P(x, y)
        \end{align*}
        We can similarly find:
        \begin{align*}
            D &= \P(Z_n = y | T \leq n, Z_n = x, B)\\
            &= P(x, y)
        \end{align*}
        by swapping $X_n$ and $Y_n$.

        Then we return to $\P(Z_{n+1} = y | Z_n = x, B)$:
        \begin{align*}
            &=P(x, y) \P(T > n | Z_n = x, B) + P(x, y) \P(T \leq n | Z_n = x, B) \\
            &= p(x, y)
        \end{align*}
        so indeed $\chain{Z} \sim \operatorname{Markov}(\lambda, P)$.
    \end{subproof}
    We can now tie this all together. Fix $y \in I$. Since $\chain{X}$ and $\chain{Z}$ have the same distribution, and $\chain{Y}$ is a stationary chain,
    \begin{align*}
        |\P(X_n = y) - \pi(y)| &= |\P(Z_n = y) - \pi(y)| \\
        &= |\P(Z_n = y) - \P(Y_n = y)| \\
        &= |\P(Z_n = y, T > n) + \P(Z_n = y, T \leq n)\\
            &\qquad- \P(Y_n = y, T > n) - \P(Y_n = y, T \leq n)| \\
        &= |\P(X_n = y, T > n) + \P(Y_n = y, T \leq n)\\
            &\qquad- \P(Y_n = y, T > n) - \P(Y_n = y, T \leq n)| \\
        &= |\P(X_n = y, T > n) - \P(Y_n = y, T > n)| \\
        &\leq \max\{\P(X_n = y, T > n), \P(Y_n = y, T > n)\} \\
        &\leq \P(T > n)
    \end{align*}
    Therefore, since $\P(T < \infty) = 1$, this final quantity tends to zero. We have therefore found:
    \begin{equation*}
        \P(X_n = y) \to \pi(y) \text{ as $n \to \infty$.}
    \end{equation*}
\end{proof}
\subsection{A Consequence of Null Recurrence}
\begin{theorem}
    Let $P$ be a transition matrix for a Markov chain, and suppose that it is irreducible, aperiodic, and null recurrent. Then:
    \begin{equation*}
        p_{xy}(n) \to 0 \text{ as } n \to \infty
    \end{equation*}
    for all states $x, y$.
    \label{thmNullRecToZero}
\end{theorem}
\begin{proof}
    Consider again the bivariate transition matrix $\tilde{P}$:
    \begin{equation*}
        \tilde{P}((x, y), (x', y')) = P(x, x') P(y, y')
    \end{equation*}
    \begin{case}{$\tilde{P}$ is transient}
        Then by definition:
        \begin{align*}
            \infty &> \sum_{n \geq 0} \tilde{P}^n((x, y), (x, y)) \\
            &= \sum_{n \geq 0} p_{xy}(n)^2
        \end{align*}
        Then $p_{xy}(n) \to 0$ as $n \to \infty$.
    \end{case}
    \begin{case}{$\tilde{P}$ is recurrent}
        Consider, for a fixed $y \in I$,
        \begin{equation*}
            \nu_y(x) \E_y\left[\sum_{n=0}^{T_y-1} \ind{X_n = x}\right]
        \end{equation*}
        Then since $P$ is irreducible and recurrent, $\nu_y$ is invariant for $P$. Since $P$ is also null recurrent, $\E_y(T_y)$ is infinite.
        \begin{align*}
            \E_y(T_y)&= \sum_{x \in I} \nu_y(x) \\
            &= \nu_y(I)
        \end{align*}
        Then for any $M > 0$, there exists a finite subset $A \subseteq I$ such that $\nu_y(A) \geq M$. If not, we could not have $\nu_y(I)$ infinite. Using this, define a new distribution:
        \begin{equation*}
            \mu(x) = \nu_y(x | A) = \frac{\nu_y(x)}{\nu_y(A)} \ind{x \in A}
        \end{equation*}
        so $\mu(x) = 0$ if $x \notin A$.

        Define the independent Markov chains:
        \begin{align*}
            \chain{X} &\sim \operatorname{Markov}(\mu, P) \\
            \chain{Y} &\sim \operatorname{Markov}(\delta_x, P)
        \end{align*}
        define also $T = \inf\subsetselect{n \geq 1}{(X_n, Y_n) = (x, x)}$, the time at which the chains meet. As in the proof of theorem~\ref{thmFundamentalConvergence}, $T < \infty$ with probability 1. Define, as before,
        \begin{equation*}
            Z_n = \begin{cases}
                X_n & n < T \\
                Y_n & n \geq T
            \end{cases}
        \end{equation*}
        and we have already seen that $Z \sim \operatorname{Markov}(\mu, P)$.

        \begin{align*}
            (\mu P^n)_y &= \sum_{z \in I} \mu(z) P^n(z, y) \\
            &= \sum_{z \in I} \frac{\nu_y(z)}{\nu_y(A)} \ind{z \in A} P^n(z, y) \\
            &\leq \sum_{z \in I} \frac{\nu_y(z)}{\nu_y(A)} P^n(z, y) \\
            &= \frac{1}{\nu_y(A)} \nu_y(z) \text{ because $\nu_y$ is invariant for $P$} \\
            &= \frac{1}{\nu_y(A)} \leq \frac{1}{M}
        \end{align*}
        Finally, 
        \begin{align*}
            p_{xy}(n) & = \P(Y_n = y) \\
            &= \P(Y_n = y, T > n) + \P(Y_n = y, T \leq n) \\
            &\leq \P(T > n) + \P(X_n = y) \\
            &\leq \P(T > n) + \frac{1}{M}
        \end{align*}
        Taking the limit $n \to \infty$:
        \begin{align*}
            \lim_{n \to \infty} p_{xy}(n) &\leq \frac{1}{M} \\
            \implies p_{xy}(n) &= 0 \text{ because we can take $M \to \infty$.}
        \end{align*}
    \end{case}
\end{proof}
\nonexaminablesection{Ergodic Theorem for Markov Chains}
We can prove a theorem that is analogous to the Law of Large Numbers for a Markov chain:
\begin{theorem}[Ergodic Theorem for Markov Chains]
    Suppose $\chain{X} \sim \operatorname{Markov}(\vec{\lambda}, P)$ is irreducible and positive recurrent with invariant distribution $\pi$. Then:
    \begin{equation*}
        \frac1n \sum_{i=0}^{n-1} \ind{X_i = x} \to \pi(x) \text{ for all $x \in I$ as $n \to \infty$}
    \end{equation*}
    with probability 1.
    \label{thmErgodic}
\end{theorem}
\begin{remark}
    This gives us the Law of Large Numbers in its usual form. Suppose $f : I \mapsto \R$. Then:
    \begin{align*}
        \frac1n \sum_{i=0}^{n-1} f(X_i)&= \frac1n \sum_{i=0}^{n-1} \sum_{a \in I} \ind{X_i = a} f(a) \\
        &= \sum_{a \in I} \left[\frac1n \sum_{i=-}^{n-1} \ind{X_i = a}\right] f(a)
        &= \sum_{a \in I} \pi(a) f(a) \text{ as $n \to \infty$}
    \end{align*}
    Therefore we have:
    \begin{equation*}
        \frac1n \sum_{i = 0}^{n -1} f(X_i) \to \sum_{a \in I} \pi(a) f(a) = \E_\pi[f(X)]
    \end{equation*}
    with probability $1$.
\end{remark}
\begin{proof}
    Recall the notation, for $x$ fixed:
    \begin{equation*}
        T_x^{(0)} = 0,\qquad T_x^{(k+1)} = \inf\subsetselect{n > T_x^{(k)}}{X_n = x}
    \end{equation*}
    And let $V_n(x)$ be the total number of visits to $x$ by time $n$:
    \begin{equation*}
        V_n(x) = \sum_{i=0}^{n-1} \ind{X_i = x}
    \end{equation*}
    So we are interested in the limit of $\frac1n V_n(x)$ as $n \to \infty$.

    Since $\chain{X}$ is positive recurrent, $E_x(T_x) < \infty$, where $T_x = T_x^{(1)}$. By the Strong Markov Property, 
    \begin{equation*}
        (X_0, X_1, \cdots, X_{T_x}) \text{ are independent of } (X_{T_x + n})_{n \geq 0}
    \end{equation*}
    And so $(X_{T_x + n}) \sim \operatorname{Markov}(\delta_x, P)$.

    Therefore, without loss of generality, we can consider only the case $\vec{\lambda} = \delta_x$.

    Define the sequences:
    \begin{equation*}
        S_x^{(k)} =
        \begin{cases}
            T_x^{(k)} = T_x^{(k-1)} & T_x^{(k-1)} \text{ is finite} \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    Again by the strong Markov property, we have that the sequences are independent and identically distributed, with:
    \begin{equation*}
        \E(S_x^{(k)}) = \E(T_x) = \frac1{\pi(x)}
    \end{equation*}
    and let this be $m_x$.

    By the definitions, %TODO: check
    we have:
    \begin{equation*}
        S_x^{(1)} + \cdots + S_x^{(V_n(x) - 1)} \leq n \leq S_x^{(1)} + \cdots + S_x^{(V_n(x))}
    \end{equation*}
    Then by the standard Strong Law of Large Numbers we know, as $n \to \infty$,
    \begin{equation*}
        \frac1n \sum_{k=1}^{n} S_x^{(k)} \to m_x \text{ with probability $1$.}
    \end{equation*}
    We also know that $V_n(x) \to \infty$ as $n \to \infty$, so:
    \begin{align*}
        \frac{S_x^{(1)} + \cdots + S_x^{(V_n(x) - 1)}}{V_n(x)} &\leq \frac{n}{V_n(x)} \leq \frac{S_x^{(1)} + \cdots + S_x^{(V_n(x))}}{V_n(x)} \\
        \frac{V_n(x)}{n} &= \frac{1}{n} \sum_{i=0 }^{n-1} \ind{X_i = x} \to \frac{1}{m_\lambda} = \pi(x)
    \end{align*}
    as $n \to \infty$ with probability $1$.
\end{proof}
\end{document}