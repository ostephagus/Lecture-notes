\documentclass[../Main.tex]{subfiles}

\begin{document}
This chapter will be concerned with ordinary differential equations of the form:
\begin{equation}
    \begin{cases}
        \frac{d\vec{y}}{dt} = \vec{f}(t, \vec{y})  & \vec{y}(t) \in \R^n, t \in \R \\
        \vec{y}(0) = \vec{y_0} & 
    \end{cases}
    \label{eqnGeneralODE}
\end{equation}
\section{One-Step Explicit Methods}
In this section we will try to find a formula that predicts $\vec{y}$ at $t_n$ based on $\vec{y}(t_{n-1})$. We split the $t$ domain into discrete points $t_n$ based on $t_n = nh$ for a step size $h$. Let $t_n$ run from $0$ to a final value $t^\star$.

Denote the true solution of the ODE as $\vec{y}(t)$, and denote the output of a numerical algorithm $\vec{y_n}$ for the approximation of the function of $t_n$.
\subsection{Quantifying Convergence}
Consider a method that produces, for each $h > 0$, a sequence:
\begin{equation*}
    (y_n)_{n = 0}^{\lfloor t^\star  / h \rfloor}
\end{equation*}
that approximates the true solution $y(t_n) = y(nh)$.
\begin{definition}{Convergent method}
    We say that a method as given above is \underline{convergent} if:
    \begin{equation*}
        \max_{n = 0 \text{ to } \lfloor \frac{t^\star}{h}\rfloor} \norm{y_n - y(nh)} \xrightarrow[h \to 0]{} 0
    \end{equation*}
\end{definition}
\subsection{Euler's Method}
The simplest one-step method is Euler's Method. Consider a Taylor expansion:
\begin{align*}
    \vec{y}(t_{n+1}) &= \vec{y}(t_n) + ht'(t_n) + O(h^2) \\
    &= \vec{y}(t_n) + h \vec{f}(t_n, \vec{y}(t_n)) O(h^2)
\end{align*}
So we take the approximation:
\begin{equation}
    \vec{y_{n+1}} = \vec{y_n} + h \vec{f}(t_n, \vec{y_n})
    \label{eqnEulerMethod}
\end{equation}
\begin{definition}{Lipschitz continuity}
    A function $f : \C^n \mapsto \C^n$ is \underline{$\lambda$-Lipschitz continuous} if, for all $\vec{x}, \vec{y} \in \C^n$,
    \begin{equation*}
        \norm{f(x) - f(y)} \leq \lambda \norm{x - y}
    \end{equation*}
\end{definition}
\begin{theorem}[Convergence of Euler's Method]
    Consider an ODE of the form in \eqnref{eqnGeneralODE} and assume that $f$ is $\lambda$-Lipschitz continuous:
    \begin{equation*}
        \norm{f(t, \vec{u}) - f(t, \vec{w})} \leq \lambda \norm{\vec{u} - \vec{w}}
    \end{equation*}
    for all $t \in [0, t^\star]$ and $\vec{u}, \vec{w} \in \R^N$.

    Then Euler's method is convergent.
    \label{thmEulerConvergence}
\end{theorem}
\begin{proof}
    Let $h > 0$ and let $\vec{e_n} = \vec{y_n} - \vec{y}(t_n)$.
    \begin{align*}
        \vec{e_{n+1}} &= \vec{y_{n+1}} - \vec{y}(t_{n+1}) \\
        &= \vec{y_n} + h \vec{f}(t_n, \vec{y_n}) - \vec{y}(t_{n+1}) \\
        &= \vec{y_n} - \vec{y}(t_n) + h \vec{f}(t_n, \vec{y_n}) - \left(\vec{y}(t_{n+1}) - \vec{y}(t_n)\right) \\
        &= \vec{e_n} + h \vec{f}(t_n, \vec{y_n}) - \left(h \vec{y}'(t_n) + \frac{h^2}{2} \vec{y}''(\xi)\right) \\
        \intertext{by using the Lagrange remainder, $\xi \in [t_n, t_{n+1}]$}
        &= \vec{e_n} + h \left(\vec{f}(t_n, \vec{y_n}) - \vec{f}(t_n, \vec{y}(t_n))\right) - \frac{h^2}{2} \vec{y}''(\xi) \\
        \intertext{and so take norms, assuming $\vec{y}$ has a bounded second derivative:}
        \norm{\vec{e_{n+1}}} &\leq \norm{\vec{e_n}} + h \norm{\vec{f}(t_n, \vec{y_n}) - \vec{f}(t_n, \vec{y}(t_n))} + ch^2 \\
        &\leq \norm{\vec{e_n}} + h \lambda\norm{\vec{y_n} - \vec{y}(t_n)} + ch^2 \text{ by Lipschitz continuity} \\
        &= \norm{\vec{e_n}} + h \lambda\norm{\vec{e_n}} + ch^2 \\
        &= \left(1 + h \lambda\right)\norm{\vec{e_n}} + ch^2
    \end{align*}
    Then we can recursively use this inequality:
    \begin{align*}
        \norm{\vec{e_n}} &=(1 + h \lambda) \norm{\vec{e_{n-1}}} + ch^2 \\
        &\leq (1 + h\lambda) \left((1 + h\lambda) \norm{\vec{e_{n-2}}} + ch^2\right) + ch^2 \\
        &\leq (1 + h\lambda) \underbrace{\norm{\vec{e_0}}}_0 + ch^2 \sum_{i=1}^{n} (1 + h\lambda)^{i-1} \\
        &= ch^2 \frac{(1 + h\lambda)^n - 1}{h\lambda} \\
        &\leq \frac{ch}{\lambda} (1 + h\lambda)^n \\
        &\leq \frac{ch}{\lambda} e^{nh\lambda} \\
        &\leq \frac{ch}{\lambda} e^{\lambda t^\star}
    \end{align*}
    Then finally we conclude that this tends to $0$ as $h \to 0$.
\end{proof}
\begin{remark}
    The term $ch^2$ in the recursive error formula is called the \underline{local truncation}\\\underline{error}, it is a factor that is added to the error after every new timestep. We saw that this must be $O(h^2)$ or smaller for convergence. We will prove this more generally with a theorem.
\end{remark}
Often we perform this on a computer, and so we need to consider a rounding error. Let $\epsilon$ be the rounding error of the computer, and so we find:
\begin{equation*}
    \norm{\vec{e_n}} \leq (1 + \lambda h) \norm{\vec{e_{n-1}}} + ch^2 + \epsilon
\end{equation*}
For a computer using double-precision floating-point arithmetic, $\epsilon \approx 10^{-16}$.

Following the computation through to find the magnitude of the error,
\begin{equation*}
    \norm{\vec{e_n}} \leq \left(\frac{ch}{\lambda} + \frac{\epsilon}{\lambda h}\right)e^{\lambda t^\star}.
\end{equation*}
This looks approximately linear for $h \gg \epsilon$, but as $h$ gets closer to $0$ the error increases again. The minimal value is attained when $h = \sqrt{\frac{\epsilon}{c}}$. This means $h \approx 10^{-8}$ is a good timestep.
\section{Multistep Methods}
\subsection{Definition and Examples}
Consider a method that depends on $s$ previous timesteps:
\begin{equation*}
    \vec{y_{n+s}} = \vec{\phi}(\vec{y_n}, \cdots, \vec{y_{n+s-1}}) \\
\end{equation*}
We will consider only methods of the form:
\begin{equation*}
    \vec{y_{n + s}} = -\sum_{l = 0}^{s-1} \rho_l \vec{y_{n + l}} + h\left(\sum_{l = 0}^{s-1} \sigma_l \vec{f}(t_{n+l}, \vec{y_{n+l}}) + \sigma_s f(t_{n+s}, \vec{y_{n+s}})\right)
\end{equation*}
Then if $\sigma_s = 0$, the method is \underline{explicit}. If $\sigma_s \neq 0$, the method is \underline{implicit} and we need to solve a (possibly nonlinear) algebraic equation to get $\vec{y_{n+s}}$. This looks like:
\begin{equation*}
    \vec{y_{n+s}} - h \sigma_s \vec{f}(t_{n + s}, \vec{y_{n + s}}) = -\sum_{l=0}^{s-1} \rho_l \vec{y_{n + l}} + h \sum_{l = 0}^{s - 1}\sigma_l \vec{f}(t_{n + l}, \vec{y_{n + l}})
\end{equation*}
We can also write this multistep method as:
\begin{equation}
    \sum_{l=0}^{s} \rho_l \vec{y_{n + l}} = h\sum_{l=0}^{s} \sigma_l \vec{f}(t_{n + l}, \vec{y_{n + l}})
    \label{eqnMultiStep}
\end{equation}
with the convention $\rho_s = 1$.

We can give a few examples for methods:

\begin{tabular}{|c|c|}
    \hline
    Name & Formula \\
    \hline
    Explicit Euler & $\vec{y_{n + 1}} - \vec{y_n} = h\vec{f}(t_n, \vec{y_n})$ \\
    Implicit Euler & $\vec{y_{n + 1}} - \vec{y_n} = h\vec{f}(t_{n+1}, \vec{y_{n+1}})$ \\
    Theta rule & $\vec{y_{n+1}} - \vec{y_n} = h\left[\theta \vec{f}(t_n, \vec{y_n}) + (1-\theta) \vec{f}(t_{n+1}, \vec{y_{n+1}})\right]$ \\
    Adams-Bashforth & $\vec{y_{n+2}} - \vec{y_{n+1}} = h\left[\frac32 \vec{f}(t_{n+1}, \vec{y_{n+1}}) - \frac12 \vec{f}(t_n, \vec{y_{n}})\right]$ \\
    \hline
\end{tabular}
then the first three methods are 1-step, and Adams-Bashforth is a 2-step method. The theta rule with $\theta = \frac12$ gives the trapezoid rule.
\subsection{Local Truncation Error}
Consider an explicit $s$-step method:
\begin{equation*}
    \vec{y_{n+s}} = -\sum_{l=0}^{s-1} \rho_l \vec{y_{n+l}} + h\sum_{l=0}^{s-1} \sigma_l \vec{f}(t_{n + l}, \vec{y_{n + l}})
\end{equation*}
\begin{definition}{Local truncation error}
    For a numerical method evaluated at time $t_{n + s}$, $\vec{y_{n + s}}$, and a true solution $\vec{y}(t_{n + s})$, the \underline{local truncation error} is the difference:
    \begin{equation*}
        \vec{y}(t_{n + s}) - \vec{y_{n + s}}
    \end{equation*}
    where the numerical method computation is completed using exact values for $\vec{y_{n + l}}, l \in \{0, \cdots, s-1\}$.
\end{definition}
We can compute this error:
\begin{align*}
    \vec{y}(t_{n + s}) - \vec{y_{n + s}} &= \vec{y}(t_{n + s}) -\\
    &\qquad \left(-\sum_{l=0}^{s-1} \rho_l \vec{y}(t_{n + l}) + h \sum_{l=0}^{s-1} \sigma_l \vec{f}(t_{n + l}, \vec{y}(t_{n + l}))\right) \\
    &= \sum_{l=0}^{s} \rho_l \vec{y}(t_{n + l}) - h \sum_{l = 0}^{s-1} \sigma_l \underbrace{\vec{f}(t_{n + l}, \vec{y}(t_{n + l}))}_{\vec{y}'(t_{n + l})}
\end{align*}
\begin{definition}{Order}
    A convergent $s$-step method with the form in equation \eqnref{eqnMultiStep} has \underline{order} $p$ if the local truncation error is $O(h^{p+1})$. That is,
    \begin{equation*}
        \sum_{l=0}^{s} \rho_l \vec{y}(t_{n + l}) - h \sum_{l=0}^{s} \sigma_l \vec{y}'(t_{n + l}) = O(h^{p+1})
    \end{equation*}
\end{definition}
\begin{examples}{
        Consider the following methods and their orders:
    }
    \item Euler's Method:
        \begin{align*}
            \vec{y}(t_{n + 1}) - \vec{y}(t_n) &= h \vec{y}'(t_n)\\
            &= \left(\vec{y}(t_n) + h \vec{y}'(t_n) + O(h^2)\right)\\
            &\qquad- \left(\vec{y}(t_n) - h \vec{y}'(t_n)\right)
        \end{align*}
        and this is $O(h^2)$, so Euler's Method has order $1$.
    \item Consider the $\theta$ rule:
        \begin{equation*}
            \vec{y_{n + 1}} - \vec{y_n} = \vec{h}\left[\theta \vec{f}(t_n, \vec{y_n}) + (1 - \theta)\vec{f}(t_{n+1}, \vec{y_{n+1}})\right]
        \end{equation*}
        Then we need to analyse the order of:
        \begin{equation*}
            \vec{y}(t_{n + 1}) - \vec{y}(t_n) -h\theta \vec{y}'(t_n) - h(1 - \theta) \vec{y}'(t_{n+1}) \\
        \end{equation*}
        Taylor expanding around $t_{n}$ gives:
        \begin{align*}
            &= \left(\vec{y}(t_n) + h \vec{y}'(t_n) + \frac{h^2}{2} \vec{y}''(t_n) + \frac{h^3}{6} \vec{y}'''(t_n) + O(h^6)\right)\\
            &\qquad- \vec{y}(t_n) - h\theta \vec{y}'(t_n) \\
            &\qquad-h(1-\theta) \left(\vec{y}'(t_n) + h\vec{y}''(t_n) + \frac{h^2}{2} \vec{y}'''(t_n) + O(h^3)\right) \\
            &= (0) + (0)h + \left(\vec{y}''(t_n) \left[\frac12 - (1 - \theta)\right]\right)h^2\\
            &\qquad+ \left(\vec{y}'''(t_n) \left[\frac16 - \frac{1-\theta}{2}\right]\right)h^3 + O(h^4) \\
            &= h^2 \vec{y}''(t_n) \left(\theta - \frac12\right) + h^3 \vec{y}'''(t_n) \left(-\frac13 + \frac{\theta}{2}\right) + O(h^4)
        \end{align*}
        Then if $\theta = \frac12$ (trapezoidal rule), the method is order 2. Otherwise, it is of order 1.
\end{examples}
We can identify multistep methods with polynomials. For a method given by:
\begin{equation*}
    \sum_{l=0}^{s} \rho_l \vec{y_{n + l}} = h\sum_{l=0}^{s} \sigma_l \vec{f}(t_{n + l}, \vec{y_{n + l}})
\end{equation*}
then we can define polynomials:
\begin{align*}
    \rho(w) = \sum_{l=0}^{s} \rho_l w^l && \sigma(w) = \sum_{l=0}^{s} \sigma_l w^l
\end{align*}
This leads us to the following theorem:
\begin{theorem}
    For a multistep method specified with polynomials $\rho$ and $\sigma$ like above, the method has order $p$ if and only if:
    \begin{equation*}
        \rho(e^z) - z\sigma(e^z) = O(z^{p+1}) \text{ as } z \to 0
    \end{equation*}
    \label{thmODEMethodOrderExp}
\end{theorem}
\begin{remark}
    If $\sigma(e^z) \neq 0$ when $z \to 0$, so if $\sigma(1) \neq 0$, then the condition is equivalent to:
    \begin{equation*}
        \frac{\rho(w)}{\sigma(w)} = \log(w) + O\left((w-1)^{p+1}\right) \text{ as } w \to 1
    \end{equation*}
\end{remark}
\begin{proof}
    \begin{align*}
        &\sum_{l = 0}^s \rho_l \vec{y}(t_{n + l}) - h \sum_{l=0}^{s} \sigma_l \vec{y}'(t_{n + l}) \\
        &= \sum_{l = 0}^{s} \rho_l \sum_{k=0}^{\infty} \frac{(lh)^k}{k!} \vec{y}^{(k)}(t_n) - h\sum_{l=0}^{s} \sigma_l \sum_{k=0}^{\infty} \frac{(lh)^k}{k!} \vec{y}^{(k+1)}(t_n) \\
        &= \left(\sum_{l=0}^{s} p_l \right) \vec{y}(t_n) + \sum_{k=1}^{\infty} \frac{h^k \vec{y}^{(k)}(t_n)}{k!} \left[\sum_{l=0}^{s} \rho_l l^k - k \sum_{l=0}^{s} \sigma_l l^{k-1}\right]
    \end{align*}
    Then in order for the method to have order $p$,
    \begin{align}
        \sum_{l=0}^{s} \rho_l = 0&& \sum_{l = 0}^{s} (\rho_l l^k - k \sigma^l l^{k-1}) = 0 && \forall k = 1 \text{ to } p
        \label{eqnExpConvergenceSubConditions}
    \end{align}
    Then to prove the theorem, we need to show that these conditions are equivalent to the conditions in terms of exponentials. 
    \begin{align*}
        \rho(e^z) - z\sigma(e^z) &= \sum_{l=0}^{s} \rho_l e^{lz} - z \sum_{l=0}^{s} \sigma_l e^{lz} \\
        \rho(e^z) - z\sigma(e^z) &= \sum_{l=0}^{s} \rho_l \sum_{k=0}^{\infty} \frac{(lz)^k}{k!} - z \sum_{l=0}^{s} \sigma_l \sum_{k=0}^{\infty} \frac{(lz)^k}{k!} \\
        &= \left(\sum_{l=0}^{s}\rho_l\right) + \sum_{k=1}^{\infty} \frac{z^k}{k!} \sum_{l=0}^{s} \left(\rho_l l^k - k \sigma_l l^k\right)
    \end{align*}
    and so for this to be of order $k$, the conditions are equivalent to those in \eqnref{eqnExpConvergenceSubConditions}
\end{proof}
\subsection{Convergence of Multistep Methods}
\begin{definition}{Root condition}
    A polynomial $\rho$ satisfies the \underline{root condition} if:
    \begin{enumerate}
        \item all the roots of $\rho$ lie inside the disc $\subsetselect{z \in \C}{|z| \leq 1}$;
        \item any roots with $|z| = 1$ are simple.
    \end{enumerate}
\end{definition}
\begin{theorem}[Dahlquist Equivalence Theorem]
    Consider a multistep method specified by polynomials $\rho$ and $\sigma$. Let the degree of these polynomials be $s$ (so we have an $s$-step method) and require $\rho$ monic. The multistep method is convergent if and only if:
    \begin{enumerate}
        \item the order $p$ of the method is greater than 1;
        \item $\rho$ satisfies the root condition.
    \end{enumerate}
    \label{thmDahlquistEquiv}
\end{theorem}
\begin{remarks}
    \item This gives us a nice characterisation of convergence in terms of things that are not too difficult to check. We simply find the order by Taylor expansion, and we can easily calculate the roots of a polynomial.
    \item The root condition on $\rho$ is to control how the local truncation errors accumulate and to ensure that they do not blow up.
\end{remarks}
\begin{proof}
    The proof is not provided in this course.
\end{proof}
\begin{example}[Convergence of Adams-Bashforth]
    The Adams-Bashforth method is a 2-step method defined:
    \begin{equation*}
        \vec{y_{n+2}} - \vec{y_{n+1}} = h \left[\frac32 \vec{f}(t_{n+1}, \vec{y_{n+1}} - \frac12 \vec{f}(t_n, \vec{y_n}))\right]
    \end{equation*}
    We see that $\rho(w) = w^2 - w$. We factorise to get $\rho(w) = w(w-1)$, and so we have that the roots are $0$ and $1$ which satisfies the root condition.

    $\sigma(w) = \frac32 w - \frac12$. Considering \thmref{thmODEMethodOrderExp}, consider:
    \begin{align*}
        \rho(e^z) - z \sigma(e^z) &= e^{2z} - e^z - z\left(\frac32 e^z - \frac12\right) \\
        &= 1 + 2z + \frac{(2z)^2}{2} + O(z^3) - \left(1 + z + \frac{z^2}{2}+ O(z^3)\right) \\
        &\qquad- z\left(\frac32\left(1 + z + O(z^2) - \frac12\right)\right) \\
        &= 0 + 0z + 0z^2 + O(z^3)
    \end{align*}
    therefore \thmref{thmODEMethodOrderExp} tells us that the order of the method is at least 2 (in fact it is 2).

    Since the order is greater than $1$ and $\rho$ satisfies the root condition, the Adams-Bashforth method is convergent by \thmref{thmDahlquistEquiv}.
\end{example}
\begin{example}[Failure of Root Condition 1]
    Consider the following method:
    \begin{equation*}
        \vec{y_{n+2}} + 5\vec{y_{n+1}} - 4\vec{y_n} = h\left(4 \vec{f}(t_{n+1}, \vec{y_{n+1}}) + 2 \vec{f}(t_n, \vec{y_n})\right)
    \end{equation*}
    This gives:
    \begin{align*}
        \rho(w) &= w^2 + 4w - 5 \\
        \sigma(w) &= 4w + 2
    \end{align*}
    Then we can check that this method has order 3. However,
    \begin{equation*}
        \rho(w) = (w - 1)(w + 5)
    \end{equation*}
    so the roots are $1$ and $-5$, which does not satisfy the root condition. Therefore, by \thmref{thmDahlquistEquiv}, this is not convergent.

    In order to see this in a simple case, let $f = 0$ and so we want to solve:
    \begin{equation*}
        \vec{y_{n+2}} + 5\vec{y_{n+1}} - 4\vec{y_n} = 0
    \end{equation*}
    Then we know how to solve this, we consider each root raised to a power:
    \begin{equation*}
        \vec{y_n} = \vec{c_1} (1)^n + \vec{c_2} (-5)^n
    \end{equation*}
    Therefore even if $c_2$ is very small we would get exponential growth in $\vec{y_n}$.
\end{example}
\begin{example}[Failure of Root Condition 2]
    Consider:
    \begin{equation*}
        \vec{y_{n+2}} - 2\vec{y_{n+1}} + \vec{y_n} = 0
    \end{equation*}
    as a fairly silly example. The order of this method is $1$, and we find:
    \begin{equation*}
        \rho(w) = w^2 - 2w + 1 = (w - 1)^2
    \end{equation*}
    which does not satisfy the root condition. We see that the analytic solution to the discrete equation is:
    \begin{equation*}
        \vec{y_n} = (c_0 + n c_1) (1)^n
    \end{equation*}
    and so this increases linearly with $n$, so is not convergent.
\end{example}
\subsection{Constructing Convergent Methods}
We already have \thmref{thmDahlquistEquiv}, so we need to find $\rho(w)$ that satisfies the root condition and has $\rho(1) = 0$. Then, we choose $\sigma$ such that:
\begin{equation*}
    \sigma(w) = \frac{\rho(w)}{\log(w)} + O((w-1)^{s})
\end{equation*}
for an $s$-step explicit method, or:
\begin{equation*}
    \frac{\rho(w)}{\sigma(w)} - \log(w) + O((w-1)^{s + 1})
\end{equation*}
for an $s$-step implicit which has order $s+1$.

\begin{example}[Adams Methods]
    Adams methods correspond to the choice:
    \begin{equation*}
        \rho(w) = w^{s-1}(w-1)
    \end{equation*}
    which satisfies the root condition and $\rho(1) = 0$. For $s = 2$, we get:
    \begin{equation*}
        \frac{w(w-1)}{\log(w)} = \frac{(1 + x)(x)}{\log(1+x)} \text{ where $x = w - 1$} \\
    \end{equation*}
    Then we find:
    \begin{align*}
        \sigma(x - 1) &= 1 + \frac32 x + \frac5{12} x^2 \\
        \sigma(w) &= -\frac{1}{12} + \frac23 w + \frac{5}{12} w^2
    \end{align*}
    and we get to the 2-step Adams method as previously seen.
\end{example}
\begin{example}[Backward-differentition formula]
    The $s$-step backward differentiation formula (BDF) is the $s$-order method with:
    \begin{equation*}
        \sigma(w) = \sigma_s w^s
    \end{equation*}
    That is, it has the form:
    \begin{equation*}
        \sum_{l=0}^{s} \rho_l \vec{y_{n + l}} = \rho_s \vec{f}(t_{n+s}, \vec{y_{n+s}})
    \end{equation*}

    Now, we need to find $\rho(w)$ of degree $s$ such that:
    \begin{equation*}
        \rho(w) = \sigma_s w^s \log(w) + O(|w - 1|^{s+1})
    \end{equation*}
    Note that if we expand $\log(w)$ with the usual expansion, we will end up with a polynomial of degree greater than $s$. We instead consider:
    \begin{align*}
        \log(w) &= - \log\left(\frac{1}{w}\right) \\
        &= -\log\left(1 - \frac{w-1}{w}\right) \\
        &= \sum_{l=0}^{\infty} \left(\frac{w - 1}{w}\right)^l \frac1l
    \end{align*}
    Next,
    \begin{align*}
        w^s \log(w) &= \sum_{l=1}^{\infty} w^{s-l} (w-1)^l \frac1l \\
        &= \underbrace{\sum_{l=1}^{s} \frac1l w^{s-l} (w-1)^l}_{\rho(w)} + O\left((w-1)^{s+1}\right) \\
    \end{align*}
    Therefore, choose $\rho$ to the polynomial given. We can find the constant $\sigma_s$ by ensuring that the coefficient of $w^s$ is $1$ in $\rho$ (by convention). This fixes $\sigma_s$:
    \begin{equation*}
        \sigma_s = \left(\sum_{l=1}^{s} \frac1l\right)^{-1}
    \end{equation*}
    Note that in general, this $\rho$ will not satisfy the root condition. One can show that $\rho$ satisfies the root condition for $s \leq 6$. The BDF method is convergent in this case.
\end{example}
\section{Runge-Kutta Methods}
Recall the form of a general ODE:
\begin{equation}
    \begin{cases}
        \frac{d\vec{y}}{dt} = \vec{f}(t, \vec{y})  & \vec{y}(t) \in \R^n, t \in \R \\
        \vec{y}(0) = \vec{y_0} & 
    \end{cases}
    \tag{\ref{eqnGeneralODE}}
\end{equation}
Then the Runge-Kutta methods attempt to link numerical solution of such an ODE with integrals. We know:
\begin{align*}
    \vec{y}(t_{n+1}) &= \vec{y}(t_n) + \int_{t_n}^{t_{n+1}} \vec{y}'(\tau) d\tau \\
    &= \vec{y}(t_n) + \int_{t_n}^{t_{n+1}} \vec{f}(\tau, \vec{y}(\tau)) d\tau \\
    &= \vec{y}(t_n) + h \int_0^1 \vec{f}(t_n + hs, \vec{y}(t_n + hs)) ds
\end{align*}
Then the Runge-Kutta method uses a quadrature formula to approximate this integral:

\begin{equation*}
    \vec{y}(t_{n+1}) = \vec{y}(t_n) + h \sum_{l=1}^{\nu} b_l \vec{f}(t_n + c_l h, \vec{y}(t_n + c_l h))
\end{equation*}
The problem with this formula is that we find $\vec{y}(t_n + c_l h)$ on the RHS. This we do not know, and so we will do this by solving at intermediate points between $t_n$ and $t_{n+1}$ and aggregating them to find the next value of $\vec{y}$.

Let $\vec{k_l} = \vec{f}(t_n + h c_l, \vec{y}(t_n + h c_l))$. We want to approximate this in order to evaluate:
\begin{equation*}
    \sum_{l=1}^{\nu} b_l \vec{k_l}
\end{equation*}
We can simply choose $\vec{k_1}$ to be $f$ evaluated at the previous timestep, but for the others we will need an explicit Euler approximation for the values of $\vec{y}$ between $t_n$ and $t_{n+1}$.
\begin{align*}
    \vec{k_1} &= \vec{f}(t_n + \vec{y_n}) \\
    \vec{k_2} &= \vec{f}(t_n + h c_2, \vec{y_n} + h c_2 \vec{k_1}) \\
    \vec{k_3} &= \vec{f}(t_n + h c_3, \vec{y_n} + h(a_{3,1} \vec{k_1} + a_{3,2} \vec{k_2})) \\
    \vdots~~&~~~~~\vdots \\
    \vec{k_\nu} &= \vec{f}(t_n + h c_\nu, \vec{y_n} + h \sum_{j=1}^{\nu - 1} a_{\nu,j} k_j)
\end{align*}
This gives the general form of a $\nu$-stage explicit Runge-Kutta method. We can also consider implicit Runge-Kutta method, given by:
\begin{equation*}
    \vec{k_l} = \vec{f}(t_n + h c_l, \vec{y_n} + h \sum_{j=1}^{\nu} a_{l, j} k_j)
\end{equation*}
where we now allow the diagonal elements $a_{l, l}$ to be non-zero. This gives a linear system of equations that must be solved.
\begin{example}[Explicit 2-stage RK Method]
    Consider a Runge-Kutta method defined by:
    \begin{align*}
        k_1 &= f(t_n, y_n) \\
        k_2 &= f(t_n + h c_2, y_n + h c_2 k_1)
    \end{align*}
    and we want to find ts truncation error. Consider $y(t_{n+1}) - y_{n+1}$, assuming $y_{n+1}$ was computing using $y_n = y(t_n)$. This gives:
    \begin{align*}
        k_1 &= f(t_n, y_n) = y'(t_n) \\
        k_2 &= f(t_n + h c_2, y_n + h c_2 k_1) \\
        &= f(t_n, y_n) + \left(\frac{\partial f}{\partial t} h c_2 + \frac{\partial f}{\partial y} h c_2 k_1\right) + O(h^2) \\
        &= y'(t_n) + h c_2 \left(\frac{\partial f}{\partial t} + \frac{\partial f}{\partial y}y'(t_n)\right) + O(h^2) \\
        &= y'(t_n) + h c_2 y''(t_n) + O(h^2)
    \end{align*}
    Putting this together,
    \begin{align*}
        y_{n+1} &= y_n + h \left(b_1 y'(t_n) + b_2 \left(y'(t_n) + h c_2 y''(t_n)\right) + O(h^2)\right) \\
        &= y(t_n) + h \left(b_1 + b_2\right) y'(t_n) + h^2 y''(t_n) b_2 c_2 + O(h^3)
    \end{align*}
    and so the error is:
    \begin{align*}
        y(t_{n+1}) - y_{n+1} &= h\left(1 - b_1 - b_2\right)y'(t_n) + h^2 \left(\frac12 - b_2 c_2\right)y''(t_n) + O(h^3) \\
    \end{align*}
    therefore if $b_1 + b_2 = 1$, the order is $\geq 1$. If further $b_2 c_2 = \frac12$, then the order is $\geq 2$. We can show that in fact the order is exactly 2 in this case.
\end{example}
\section{Absolute Stability}
Consider the scalar ODE:
\begin{equation}
    \begin{cases}
        y' = \lambda y & t \in \R \\
        y(0) = 1 & \lambda \in \C
    \end{cases}
    \label{eqnStabilityODE}
\end{equation}
Then we know that the solution is $y(t) = e^{\lambda t}$. If $\Re(\lambda) < 0$, then $y(t) \to 0$ as $t \to \infty$. We will be interested in whether a given method converges (to 0) as $t \to \infty$ when numerically solving this ODE.

In order to illustrate the concepts, consider the explicit Euler method.
\begin{equation*}
    y_{n+1} = y_n + h \lambda y_n = (1 + h \lambda) y_n \implies y_n = (1 + h \lambda)^n
\end{equation*}
Then we can clearly see that this will converge as long as the exponentiated quantity is less than $1$. Consider this quantity, as graphed in figure~\ref{figModGraph}

\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1, mark size=0]
        \begin{axis}[
                axis lines=middle,
                xmin=0,xmax=3,
                xlabel={$h$},
                xtick={0, 1, 2},
                xticklabels={$0$, $\frac{1}{\abs{\lambda}}$, $\frac{2}{\abs{\lambda}}$},
                ylabel={$|1 + h \lambda|$}
            ]
            \addplot+[samples at={0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3}] {abs(1 - x)};
            \draw[dashed] (0, 1) -- (3, 1);
            \draw[dashed] (2, 1) -- (2, 0);
        \end{axis}
    \end{tikzpicture}
    \caption{Plot of stability condition for explicit Euler}
    \label{figModGraph}
\end{figure}

Then we see that $y_n \to 0$ if and only if $h < \frac{2}{-\lambda}$.

For the implicit Euler method, $y_{n + 1} = y_n + \lambda y_{n+1}$, se see that this has solution
\begin{equation*}
    y_n = \left(\frac{1}{1 - h \lambda}\right)
\end{equation*}
and so we find a similar convergence condition.
\begin{definition}{Linear stability domain}
    Suppose that a numerical method applied to the ODE given in \eqnref{eqnStabilityODE} produces the sequence $y_n$ for a given step size $h > 0$. Then the \underline{linear stability domain} of the method is:
    \begin{equation*}
        \dom = \subsetselect{h\lambda \in \C}{y_n \xrightarrow[n \to \infty]{} 0}
    \end{equation*}
\end{definition}
\begin{examples}{
        We find linear stability domains for various methods
    }
    \item We have already seen the explicit Euler method, it has LSD:
        \begin{equation*}
            \dom = \subsetselect{z \in \C}{\abs{1 + z} < 1}
        \end{equation*}
    \item The Implicit Euler method gives:
        \begin{equation*}
            \dom = \subsetselect{z \in \C}{\frac{1}{1-z} < 1}
        \end{equation*}
    \item Trapezoidal rule:
        \begin{align*}
            y_{n+1} &= y_n + h \left(\frac{1}{2} f(t_n, y_n) + \frac12 f(t_{n+1}, y_{n+2})\right) \\
            &= y_n + h (\frac12 \lambda y_n + \lambda y_{n+1})
        \end{align*}
        and so:
        \begin{equation*}
            \left(1 - \frac{h}{2} \lambda\right) y_{n+1} = \left(1 + \frac{h}{2} \lambda\right) y_n
        \end{equation*}
        which gives the solution:
        \begin{equation*}
            y_n = \left(\frac{1 + \frac12 h \lambda}{1 - \frac12 h \lambda}\right)^n
        \end{equation*}
        and then the linear stability domain is the left-half plane.
\end{examples}
\begin{definition}{A-stable}
    A method is \underline{A-stable} if:
    \begin{equation*}
        \subsetselect{z \in \C}{\Re(z) < 0} \subseteq \dom
    \end{equation*}
\end{definition}
\end{document}