\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{A Note on Computational Complexity}
\begin{definition}{Computational Complexity}
    The \underline{computational complexity} of an algorithm is defined as the number of elementary operations ($+$, $-$, $\times$, $\div$).
\end{definition}
In general, the complexity is a function of some metric $n$ of the size of the input to the algorithm $C(n)$. We are interested only in the leading term of $C(n)$ as $n \to \infty$, and therefore we use ``big O'' notation: $O(f(n))$ meaning that $C(n)$ is bounded by fixed multiples of $f(n)$.
\section{Polynomial Interpolation}
Let $\P_n[x]$ be the vector space of polynomials of degree at most $n$ in $x$. This space has $(n+1)$ dimensions. Consider the following problem:

\begin{equation}
    \begin{split}
        \text{ Given $n+1$ distinct points } (x_i)_{i=0}^n \text{ and } (f_i)_{i=0}^n \\
        \text{Find } p \in P_n[x] \text{ such that } p(x_i) = f_i~\forall 0 \leq i \leq n.
    \end{split}
    \label{eqnInterpProblem}
\end{equation}
\subsection{Lagrange Polynomials}
\begin{definition}{Lagrange polynomial}
    The $k$th \underline{Lagrange polynomial} is defined to be:
    \begin{equation}
        L_k(x) = \prod_{\substack{l = 0\\l \neq k}}^{n} \frac{x - x_l}{x_k - x_l} \in \P_n[x]
        \label{eqnLagrangePoly}
    \end{equation}
\end{definition}
This has the property:
\begin{equation}
    L_k(x_l) =
    \begin{cases}
        1 & i = k \\
        0 & i \neq k
    \end{cases}
    \label{eqnLagrangeProperty}
\end{equation}
Then we find that the solution to \eqnref{eqnInterpProblem} is given by:
\begin{equation}
    p(x) = \sum_{k=0}^{n} f_k L_k(x) \in \P_n[x]
    \label{eqnInterpWithLagrange}
\end{equation}
\begin{proposition}
    The solution to the problem in equation~\ref{eqnInterpProblem} is unique.
    \label{propInterpUnique}
\end{proposition}
\begin{proof}
    Consider $p$, $q$ polynomials that satisfy $p(x_i) = q(x_i) = f_i$.

    Then consider the polynomials $p - q$. At the $x_i$, $(p - q)(x_i) = 0$. however, this is $(n+1)$ distinct zeroes and so the only such polynomial in $P_n[x]$ is the zero polynomial.
\end{proof}
We have now found a valid solution the the interpolation problem. What is the time complexity of the solution in \eqnref{eqnInterpWithLagrange}?

We must calculate $(n+1)$ multiplications of $f_k$ with $L_k(x)$. Then to find $L_k$ we require 3 elementary operations for each term and there are $n$ terms (which also need to be multiplied), giving a complexity of $4n$. Then the total complexity is:
\begin{align*}
    C(n) &= 4n(n+1) + n + 1 \\
    &= 4n^2 + 5n + 1 = O(n^2)
\end{align*}
Therefore we say that the computation via Lagrange polynomials has complexity $O(n^2)$.
\subsection{Error in Polynomial Interpolation}
Let $C[a, b]$ be the space of continuous functions on $[a, b]$. Let $C^s[a, b]$ be the space of $s$-times continuously differentiable functions on $[a, b]$.

\begin{theorem}[Polynomial Interpolation Error Theorem]
    Let $f \in C^{n+1}[a, b]$, and $x_0, \cdots, x_n$ be distinct points in $[a, b]$. Let $p \in \P_n[x]$ be the polynomial interpolation such that $p(x_i) = f(x_i)$.

    Then for all $x \in [a, b]$, there exists $\xi \in [a, b]$ such that:
    \begin{equation}
        f(x) - p(x) = \frac{1}{(n+1)!} f^{(n+1)}(\xi) \prod_{i=0}^{n} (x - x_i)
        \label{eqnInterpError}
    \end{equation}
    \label{thmInterpError}
\end{theorem}
\begin{proof}
    If $x = x_i$ for some $i$, then both sides of \eqnref{eqnInterpError} are trivially zero. Therefore, assume not.

    Define an auxiliary function:
    \begin{equation*}
        \phi(t) = f(t) - p(t) - (f(x) - p(x)) \frac{\prod_{i=0}^{n} (t - x_i)}{\prod_{i=0}^{n} (x - x_i)}
    \end{equation*}
    Then we want to understand the zeroes of this function. Note that $\phi(x_i) = 0$ and also $\phi(x) = 0$. Therefore $\phi$ has at least $(n+2)$ distinct zeroes. Then, since $\phi \in C^{n+1}[a, b]$, we have the conditions required for Rolle's theorem and so $\phi'$ must have at least $(n+1)$ distinct zeroes (one between each zero of $\phi$). Applying Rolle's theorem $n$ more times, we find that $\phi^{(n+1)}$ has at least one zero, $\phi^{(n+1)}(\xi) = 0$.
    \begin{equation*}
        0 = \phi^{(n+1)}(\xi) = f^{(n+1)}(\xi) - (f(x) - p(x)) \frac{(n+1)!}{\prod_{i=0}^{n} (x - x_i)}
    \end{equation*}
    and this rearranges to \eqnref{eqnInterpError}.
\end{proof}
\begin{example}
    Consider $f(x) = \frac{1}{1 + x^2}$, interpolated over equally spaced points in $[-5, 5]$: $x_j = -5 + 10\frac{d}{n}$.

    \begin{figure}
        \centering
        \begin{tikzpicture}[mark size=0]
            \begin{axis}[
                    axis lines=middle,
                    xlabel={$x$},
                    ylabel={$y$},
                    legend entries={Function, $n = 5$, $n = 15$}
                ]
                \addplot table {Plot data/ExampleFunction.dat};
                \addplot table {Plot data/ExampleFunctionInterp1.dat};
                \addplot table {Plot data/ExampleFunctionInterp2.dat};
            \end{axis}
        \end{tikzpicture}
        \caption{Interpolation of example function with different values of $n$}
        \label{figInterpExp}
    \end{figure}
    \begin{figure}
        \centering
        \begin{tikzpicture}[mark size=0]
            \begin{axis}[
                    axis lines=middle,
                    xlabel={$x$},
                    ylabel={$y$},
                    legend entries={$n = 5$, $n = 15$}
                ]
                \addplot table {Plot data/ExampleFunctionError1.dat};
                \addplot table {Plot data/ExampleFunctionError2.dat};
            \end{axis}
        \end{tikzpicture}
        \caption{Errors in interpolation of example function with different values of $n$}
        \label{figInterpExpErr}
    \end{figure}

    Figure~\ref{figInterpExp} gives the output of polynomial interpolation with different values of $n$. We see that it breaks down near the endpoints, and this is clearly illustrated in the error plot of figure~\ref{figInterpExpErr}. These large spikes in error are called Runge spikes (or Runge's phenomena).

    We see that, in general, equally spaced points give poor output.
\end{example}
In order to provide good approximations, \thmref{thmInterpError} tells us that we should minimise:
\begin{equation*}
    \max_{x \in [a, b]} \left|\prod_{i=0}^{n} (x - x_i)\right|
\end{equation*}
The solution is given by the \textit{Chebyshev points}:
\begin{equation*}
    x_j = 5\cos\left(\frac{(n - j)\pi}{n}\right)
\end{equation*}
In the general case of a function on $[a, b]$, we would use:
\begin{equation}
    x_j = \frac{b + a}{2} + \frac{b - a}{2}\cos\left(\frac{(n - j)\pi}{n}\right)
    \label{eqnChebyshevPoints}
\end{equation}
\section{Divided Differences}
\subsection{Definition and Properties}
\begin{definition}{Divided difference}
    Given a function $f : \R \mapsto \R$ and a polynomial $p$ constructed as above, the \underline{divided difference} of $f$ at $\{x_0, \cdots, x_n\}$ is the leading coefficient (in $x^n$) in $p$. This is:
    \begin{equation*}
        f[x_0, \cdots, x_n] = \sum_{i=0}^{n} f(x_i) \prod_{\substack{j=0\\j \neq i}}^{n} \frac{1}{x_i - x_j}
    \end{equation*}
\end{definition}
\begin{example}
    For $n = 0$, we simply find $f[x_0] = f(x_0)$. When $n = 1$,
    \begin{equation*}
        f[x_0, x_1] = f(x_0) \frac{1}{x_0-x_1} + f(x_1) \frac{1}{x_1-x_0} = \frac{f(x_1) - f(x_0)}{x_1-x_0}
    \end{equation*}
    Then this $n = 1$ case gives some intuition as to why this is called the divided difference. This is furthered by the following theorem.
    \label{expDivDiff}
\end{example}
\begin{theorem}
    Suppose that $x_0, \cdots, x_{n+1}$ are distinct point in $\R$. Let $f : \R \mapsto \R$. Then the divided differences have the form:
    \begin{equation}
        f[x_0, \cdots, x_{n+1}] = \frac{f[x_1, \cdots, x_{n+1}] - f[x_0, \cdots, x_n]}{x_{n+1} - x_0}
        \label{eqnDivDiffRecurse}
    \end{equation}
    \label{thmDivDiffRecurse}
\end{theorem}
\begin{proof}
    \induction{$n = 1$}{
        Seen in example~\ref{expDivDiff}.
    }{$n = k$}{
        Assume that \eqnref{eqnDivDiffRecurse} holds for $k$ points.
    }{$n = k + 1$}{
        Let $p, q \in \P_k[x]$ be the polynomials that interpolate $[x_0, \cdots, x_k]$ and $[x_1, \cdots, x_{n+1}]$, respectively. Let $r \in \P_{k+1}[x]$ be defined by:
        \begin{equation*}
            r(x) = \frac{(x_{k+1} - x) p(x) + (x - x_0) q(x)}{x_{k+1}} \in \P_{k+1}[x]
        \end{equation*}
        Now note that $r(x_i) = f(x_i)~\forall i \in \{0, \cdots, k+1\}$. Therefore, by definition, $f[x_0, \cdots, x_{k+1}]$ is the leading coefficient of $r$. Then given the formula for $r$, this is:
        \begin{align*}
            f[x_0, \cdots, x_{k+1}] &= \frac{-(\text{leading coeff't of }p) + (\text{leading coeff't of }q)}{x_{k+1} - x_0} \\
            &= \frac{f[x_1, \cdots, x_{k+1}] - f[x_0, \cdots, x_k]}{x_{k+1} - x_0}
        \end{align*}
        by induction, as required.
    }
\end{proof}
\begin{theorem}
    Let $f \in C^n[a, b]$ and let $x_0, \cdots, x_n$ be distinct points in $[a, b]$. Then there exists $\xi \in [a, b]$ such that:
    \begin{equation*}
        f[x_0, \cdots, x_n] = \frac{f^{(n)}(\xi)}{n!}
    \end{equation*}
    \label{thmDivDiffDerivs}
\end{theorem}
\begin{proof}
    Let $p$ be the polynomial that interpolates $f$ at $x_0, \cdots, x_n$. Then $\phi = f - p$ has $n+1$ zeroes in $[a, b]$. By Rolle's theorem applied $n$ times, $\phi^{(n)}$ has at least one zero in $[a, b]$, and let this be $\xi$. Therefore, $f^{(n)}(\xi) = p^{(n)}(\xi)$.

    However, the $n$th derivative of $p$ is $n!$ times the leading coefficient,
    \begin{align*}
        n! f[x_0, \cdots, x_n] &= f^{(n)}(\xi) \\
        f[x_0, \cdots, x_n] &= \frac{f^{(n)}(\xi)}{n!} \\
    \end{align*}
\end{proof}
\begin{theorem}[Newton Interpolation Formula]
    Let $x_0, \cdots, x_n$ be distinct points in $[a, b]$, $f \in C[a, b]$. Let $p \in \P_n[x]$ be the polynomial that interpolates $f$ at the points $x_i$. Then:
    \begin{equation}
        \begin{split}
            p(x) = f[x_0] + f[x_0, x_n](x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1) + \cdots \\
            +f[x_0, \cdots, x_n] \prod_{j=1}^{n-1} (x - x_j)
        \end{split}
        \label{eqnNewtonInterp}
    \end{equation}
    \label{thmNewtonInterp}
\end{theorem}
\begin{proof}
    For $k \in \{0, \cdots, n\}$, let $p_k \in \P_k[x]$ be the polynomial that interpolates $f$ at $\{x_0, \cdots, x_k\}$. Note first that $p_0(x) = f(x_0)$.
    
    Consider $p_{k+1}(x) - p_k(x)$. We know that this vanishes at $x_0, \cdots, x_k$ ($k + 1$ roots), and must have degree at most $k + 1$.

    We also know the leading coefficient, this must be the leading coefficient of $p_{k+1}$ which is a divided difference:
    \begin{equation*}
        p_{k+1} = f[x_0, \cdots, x_{k+1}] \prod_{j=0}^{k} (x - x_j)
    \end{equation*}
    Then we can use a telescoping sum for $p(x)$:
    \begin{align*}
        p(x) &= p_n(x) = p_0(x) + (p_1 - p_0)(x) + \cdots + (p_n - p_{n-1})(x) \\
        &= f[x_0] + f[x_0, x_1] (x - x_0) + \cdots + f[x_0, \cdots, x_{n+1}] \prod_{j=0}^{n-1} (x - x_j)
    \end{align*}
\end{proof}
\subsection{Computing Divided Differences}
\Thmref{thmDivDiffRecurse} gives a natural method to compute divided differences. Consider figure~\ref{figDivDiff}. Each arrow in this diagram represents 3 operations, so the complexity is:
\begin{align*}
    C(n) &= 3\sum_{i=1}^{n} 1 \\
    &= 3 \frac{n(n+1)}{2} \\
    &= O(n^2)
\end{align*}
\begin{figure}
    \centering
    \begin{tikzpicture}[x=1.5cm, y=0.5cm]
        % Nodes
        \node (A1) at (0, 8) {$f(x_0)$};
        \node (A2) at (0, 6) {$f(x_1)$};
        \node (A3) at (0, 4) {$f(x_2)$};
        \node (A4) at (0, 2) {$\vdots$};
        \node (A5) at (0, 0) {$f(x_n)$};

        \node (B1) at (2, 7) {$f[x_0, x_1]$};
        \node (B2) at (2, 5) {$f[x_1, x_2]$};
        \node (B3) at (2, 3) {$\vdots$};
        \node (B4) at (2, 1) {$f[x_{n-1}, x_n]$};

        \node (C1) at (4, 6) {$f[x_0, x_1, x_2]$};
        \node (C2) at (4, 4) {$\vdots$};
        \node (C3) at (4, 2) {$f[x_{n-2}, x_{n-1}, x_n]$};

        \node at (5.5, 4) {$\cdots$};
        \node (D) at (7, 4) {$f[x_0, \cdots, x_n]$};

        % Arrows
        \begin{scope}[->, shorten <=2pt, shorten >= 7pt]
            \draw (A1.east) -- (B1.west);
            \draw (A2.east) -- (B1.west);
            \draw (A2.east) -- (B2.west);
            \draw (A3.east) -- (B2.west);
            \draw (A5.east) -- (B4.west);

            \draw (B1.east) -- (C1.west);
            \draw (B2.east) -- (C1.west);
            \draw (B4.east) -- (C3.west);
        \end{scope}
    \end{tikzpicture}
    \caption{Diagram of recursive calculation of divided differences}
    \label{figDivDiff}
\end{figure}
However, this is just to calculate the divided differences. In order to find $p(x)$, we need to compute:
\begin{align*}
    p(x) &= f[x_0] + f[x_0, x_1] (x - x_0) + \cdots + f[x_0, \cdots, x_n] \prod_{j=1}^{n} (x - x_j) \\
    \intertext{We can factorise this to give a simpler computation:}
    p(x) &= f[x_0] + (x - x_0) \left(f[x_0, x_1] + (x - x_1) \left(f[x_0, x_1, x_2] + (x - x_2)\left(\cdots\right)\right)\right)
\end{align*}
This is the Horner scheme, with total complexity of $O(n)$.
\section{Orthogonal Polynomials}
\subsection{Orthogonality and Inner Products}
\begin{definition}{Inner product}
    Let $V$ be a vector space over $\R$. Then an \underline{inner product} is a function $\inn{\cdot}{\cdot} : V \times V \mapsto \R$ satisfying, for $u, v \in V$,:
    \begin{itemize}
        \item Symmetry: $\inn{u}{v} = \inn{v}{u}$
        \item Linearity: $\inn{ax + by}{z} = a\inn{x}{z} + b\inn{y}{z}$
        \item Positivity: $\inn{x}{x} = 0 \iff x = \vec{0}$.
    \end{itemize}
\end{definition}
\begin{example}[Examples of inner products]
    We introduce the two canonical examples of inner products.
    On $\R^n$:
    \begin{align*}
        \inn{\cdot}{\cdot} : \R^n \times \R^n &\mapsto \R\\
        (\vec{u}, \vec{v}) &\mapsto \vec{u}^T \vec{v}
    \end{align*}

    On $C[a,b]$:
    \begin{align*}
        \inn{\cdot}{\cdot}_w : C[a, b] &\mapsto \R\\
        (f, g) &\mapsto \int_{a}^{b} f(x)g(x) w(x) dx 
    \end{align*}
    where $w$ is a \underline{weight function}, that must be strictly positive on $(a, b)$.
\end{example}
\begin{definition}{Orthogonal sequence}
    Given an inner product $\inn\cdot\cdot$, a sequence $p_0, p_1, \cdots$ is a  \underline{sequence of orthogonal polynomials} if:
    \begin{enumerate}
        \item $\inn{p_i}{p_j} = 0$
        \item $\deg(p_i) = i$
    \end{enumerate}
\end{definition}
\begin{theorem}
    For any $n \geq 0$, there exists a unique monic orthogonal polynomial of degree $n$.
    \label{thmUniqueOrthoSequence}
\end{theorem}
\begin{proof}
    \induction{$n = 0$}{
        There is a unique monic polynomial of degree $0$, $p_0(x) = 1$.
    }{$n = k$}{
        Assume that $p_0, \cdots, p_k$ are a sequence of orthogonal monic polynomials.
    }{$n = k + 1$}{
        We use a Gram-Schmidt process on the polynomial $q(x) = x^{k+1}$. Define:
        \begin{equation*}
            p_{k+1} = q - \sum_{m=0}^{k} \frac{\inn{q}{p_m}}{\inn{p_m}{p_m}}
        \end{equation*}
        We know that $p_{k+1}$ is monic because the $x^{k+1}$ coefficient only comes from $q$, so is $1$.

        For $i \leq k$,
        \begin{align*}
            \inn{p_{k+1}}{p_i} &= \inn{q}{p_i} - \sum_{m=0}^{k} \frac{\inn{q}{p_m}}{\inn{p_m}{p_m}} \inn{p_m}{p_i} \\
            &= \inn{q}{p_i} - \frac{\inn{q}{p_i}}{\inn{p_i}{p_i}} \inn{p_i}{p_i} \\
            &= \inn{q}{p_i} - \inn{q}{p_i} \\
            &= 0
        \end{align*}
        \begin{subproof}{$p_{k+1}$ is unique}
            Assume that $\tilde{p}_{k+1}$ is another monic orthogonal polynomial. Let $p = \tilde{p}_{k+1} - p_{k+1}$. Then subtracting these two polynomials gives a polynomial of degree $k$ because both are monic.

            That is, $p \in \P_k[x]$, but also we know that $p$ is orthogonal to $p_0, \cdots, p_k$. That is, it is orthogonal to a basis of $\P_k[x]$, so it must be the zero vector (it resides in a space of dimension 0).
        \end{subproof}
    }
\end{proof}
\begin{example}
    Considering the inner product:
    \begin{equation*}
        \inn{p}{q} = \int_{-1}^{1} p(x) q(x) dx 
    \end{equation*}
    We find the orthogonal polynomials are the \underline{Legendre polynomials}.

    \begin{tabular}{|c c|}
        \hline
        $i$ & $p_i$ \\
        \hline
        $0$ & $1$ \\
        $1$ & $x$ \\
        $2$ & $x^2 - \frac13$ \\
        $3$ & $x^3 - \frac35 x$ \\
        \hline
    \end{tabular}
\end{example}
\begin{example}
    Now consider a more general inner product,
    \begin{equation*}
        \inn{p}{q} = \int_{a}^{b} p(x) q(x) w(x) dx 
    \end{equation*}
    where $w(x) > 0$ is a weight function.

    \begin{tabular}{|c c c|}
        \hline
        $w(x)$ & Interval & Polynomials \\
        \hline
        $1$ & $[-1, 1]$ & Legendre Polynomials \\
        $\frac{1}{\sqrt{1 - x^2}}$ & $[-1, 1]$ & Chebyshev polynomials \\
        $e^{-x}$ & $[0, \infty)$ & Laguerre polynomials \\
        $e^{-x^2}$ & $\R$ & Hermite polynomials \\
        \hline
    \end{tabular}
\end{example}
\subsection{Three-Term Recurrence Relation}
Under mild assumptions on the inner product, we can prove an interesting result.
\begin{theorem}[Three-Term Recurrence]
    Suppose that an inner product $\inn\cdot\cdot$ satisfies, for all $p, q \in \P_n[x]$,
    \begin{equation*}
        \inn{xp}{q} = \inn{p}{xq}.
    \end{equation*}
    Let $(p_i)_{i = 1}^\infty$ be the sequence of monic polynomials for this inner product. Then for all $n \in \N$,

    \begin{equation}
        p_{n+1} = (x - \alpha_n) p_n - \beta_n p_{n-1}
        \label{eqn3TermRecur}
    \end{equation}
    where:
    \begin{align*}
        p_0 &= 1, p_{-1} = 0 \text{ (notationally)} \\
        \alpha_n &= \frac{\inn{xp_n}{p_n}}{\inn{p_n}{p_n}},\quad \beta_n = \frac{\inn{p_n}{p_n}}{\inn{p_{n-1}}{p_{n-1}}} > 0
    \end{align*}    
    \label{thm3TermRecur}
\end{theorem}
\begin{remark}
    All weighted integral inner products of the form above have this property, so we already have a large class of inner products on which this theorem holds.
\end{remark}
\begin{proof}
    Let $\psi = p_{n+1} - (x - \alpha_n) p_n + \beta_n p_{n-1}$. Note that since the $p_i$ are monic, the $x^{n+1}$ term cancels and $\psi \in \P_n[x]$. Now we show that $\inn{\psi}{p_m} = 0~\forall m \leq n$.

    \begin{align}
        \inn{\psi}{p_m} &= \inn{p_{n+1}}{p_m} - \inn{(x - \alpha_n) p_n}{p_m} + \beta_n \inn{p_{n-1}}{p_m} \nonumber \\
        &= -\inn{(x - \alpha_n) p_n}{p_m} + \beta_n \inn{p_{n-1}}{p_m} \label{eqn3TermInnProd}
    \end{align}
    Consider first $m < n-1$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)p_n}{p_m} \\
        &= -\inn{xp_n}{p_m} + \alpha_n \inn{p_n}{p_m} \\
        &= -\inn{p_n}{xp_m}
    \end{align*}
    Then since $xp_m \in \P_{n-1}[x]$, the inner product is zero.

    Consider now $m = n-1$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)p_n}{p_{n-1}} + \beta_n \inn{p_{n-1}}{p_{n-1}} \\
        &= -\inn{x p_n}{p_{n-1}} + \inn{p_n}{p_n} \\
        &= -\inn{p_n}{x p_{n-1}} + \inn{p_n}{p_n} \\
        \intertext{Note that $x p_{n-1}$ is a monic polynomial. Write as a linear combination of $p_i$}
        &= -\inn{p_n}{p_n} + \inn{p_n}{p_n} + \sum_{i=0}^{n-1} \inn{c_i p_i}{p_n} \\
        &= 0
    \end{align*}

    Now consider $m = n$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)}{p_n} + \beta_n \inn{p_{n-1}}{p_n} \\
        &= -\inn{xp_n}{p_n} + \alpha_n \inn{p_n}{p_n} \\
        &= -\inn{xp_n}{p_n} + \inn{x p_n}{p_n} \\
        &= 0
    \end{align*}
\end{proof}
\begin{example}[Chebyshev polynomials]
    Consider the inner product:
    \begin{equation*}
        \inn{p}{q} = \int_{-1}^{1} \frac{p(x) q(x)}{\sqrt{1 + x^2}} dx
    \end{equation*}
    Let $T_0, T_1, \cdots$ be the a set of orthogonal polynomials with respect to this inner product. These are the \underline{Chebyshev polynomials}, defined by:
    \begin{equation*}
        T_n(\cos\theta) = \cos(n\theta)
    \end{equation*}
    and have leading coefficients $2^{n-1}$.
    \begin{align*}
        &\cos((n+1)\theta) + \cos((n-1)\theta) = 2\cos(n\theta) \cos(\theta) \\
        \therefore &T_{n+1}(x) + T_{n-1}(x) = 2T_n(x) x \\
        &T_{n+1} = 2x T_n(x) - T_{n-1}
    \end{align*}
    Now let $\hat{T}_n(x)$ be the monic versions. Note that $\hat{T}_n(x) = 2^{1-n} T_n(x)$. The 3-term recurrence is:
    \begin{equation*}
        \hat{T}_{n+1}(x) = x \hat{T}_n(x) - \frac14 \hat{T}_{n-1}(x)
    \end{equation*}
    and therefore $\alpha_n = 0, \beta_n = \frac14$.
\end{example}
\subsection{Least-Squares Polynomial Fitting}
We now return to the study of polynomial approximation. Now we have an inner product on $C[a, b]$, we want to find the polynomial $p$ of degree $n$ that minimises $\norm{f - p}$ for any $f \in C[a, b]$.
\begin{theorem}[Least-Squares Polynomial Theorem]
    The polynomial $p \in \P_n[x]$ that minimises, for $f \in C[a, b]$, $\norm{f - p}$ is given by:
    \begin{equation}
        p = \sum_{k=0}^{n} \frac{\inn{f}{p_k}}{\inn{p_k}{p_k}} p_k
        \label{eqnLeastSquaresPoly}
    \end{equation}
    where $(p_k)_{i=0}^n$ are the first $n$ monic orthogonal polynomials for the given inner product.
    \label{thmLeastSquaresPoly}
\end{theorem}
\end{document}