\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{A Note on Computational Complexity}
\begin{definition}{Computational Complexity}
    The \underline{computational complexity} of an algorithm is defined as the number of elementary operations ($+$, $-$, $\times$, $\div$).
\end{definition}
In general, the complexity is a function of some metric $n$ of the size of the input to the algorithm $C(n)$. We are interested only in the leading term of $C(n)$ as $n \to \infty$, and therefore we use ``big O'' notation: $O(f(n))$ meaning that $C(n)$ is bounded by fixed multiples of $f(n)$.
\section{Polynomial Interpolation}
Let $\P_n[x]$ be the vector space of polynomials of degree at most $n$ in $x$. This space has $(n+1)$ dimensions. Consider the following problem:

\begin{equation}
    \begin{split}
        \text{ Given $n+1$ distinct points } (x_i)_{i=0}^n \text{ and } (f_i)_{i=0}^n \\
        \text{Find } p \in P_n[x] \text{ such that } p(x_i) = f_i~\forall 0 \leq i \leq n.
    \end{split}
    \label{eqnInterpProblem}
\end{equation}
\subsection{Lagrange Polynomials}
\begin{definition}{Lagrange polynomial}
    The $k$th \underline{Lagrange polynomial} is defined to be:
    \begin{equation}
        L_k(x) = \prod_{\substack{l = 0\\l \neq k}}^{n} \frac{x - x_l}{x_k - x_l} \in \P_n[x]
        \label{eqnLagrangePoly}
    \end{equation}
\end{definition}
This has the property:
\begin{equation}
    L_k(x_l) =
    \begin{cases}
        1 & i = k \\
        0 & i \neq k
    \end{cases}
    \label{eqnLagrangeProperty}
\end{equation}
Then we find that the solution to \eqnref{eqnInterpProblem} is given by:
\begin{equation}
    p(x) = \sum_{k=0}^{n} f_k L_k(x) \in \P_n[x]
    \label{eqnInterpWithLagrange}
\end{equation}
\begin{proposition}
    The solution to the problem in equation~\ref{eqnInterpProblem} is unique.
    \label{propInterpUnique}
\end{proposition}
\begin{proof}
    Consider $p$, $q$ polynomials that satisfy $p(x_i) = q(x_i) = f_i$.

    Then consider the polynomials $p - q$. At the $x_i$, $(p - q)(x_i) = 0$. however, this is $(n+1)$ distinct zeroes and so the only such polynomial in $P_n[x]$ is the zero polynomial.
\end{proof}
We have now found a valid solution the the interpolation problem. What is the time complexity of the solution in \eqnref{eqnInterpWithLagrange}?

We must calculate $(n+1)$ multiplications of $f_k$ with $L_k(x)$. Then to find $L_k$ we require 3 elementary operations for each term and there are $n$ terms (which also need to be multiplied), giving a complexity of $4n$. Then the total complexity is:
\begin{align*}
    C(n) &= 4n(n+1) + n + 1 \\
    &= 4n^2 + 5n + 1 = O(n^2)
\end{align*}
Therefore we say that the computation via Lagrange polynomials has complexity $O(n^2)$.
\subsection{Error in Polynomial Interpolation}
Let $C[a, b]$ be the space of continuous functions on $[a, b]$. Let $C^s[a, b]$ be the space of $s$-times continuously differentiable functions on $[a, b]$.

\begin{theorem}[Polynomial Interpolation Error Theorem]
    Let $f \in C^{n+1}[a, b]$, and $x_0, \cdots, x_n$ be distinct points in $[a, b]$. Let $p \in \P_n[x]$ be the polynomial interpolation such that $p(x_i) = f(x_i)$.

    Then for all $x \in [a, b]$, there exists $\xi \in [a, b]$ such that:
    \begin{equation}
        f(x) - p(x) = \frac{1}{(n+1)!} f^{(n+1)}(\xi) \prod_{i=0}^{n} (x - x_i)
        \label{eqnInterpError}
    \end{equation}
    \label{thmInterpError}
\end{theorem}
\begin{proof}
    If $x = x_i$ for some $i$, then both sides of \eqnref{eqnInterpError} are trivially zero. Therefore, assume not.

    Define an auxiliary function:
    \begin{equation*}
        \phi(t) = f(t) - p(t) - (f(x) - p(x)) \frac{\prod_{i=0}^{n} (t - x_i)}{\prod_{i=0}^{n} (x - x_i)}
    \end{equation*}
    Then we want to understand the zeroes of this function. Note that $\phi(x_i) = 0$ and also $\phi(x) = 0$. Therefore $\phi$ has at least $(n+2)$ distinct zeroes. Then, since $\phi \in C^{n+1}[a, b]$, we have the conditions required for Rolle's theorem and so $\phi'$ must have at least $(n+1)$ distinct zeroes (one between each zero of $\phi$). Applying Rolle's theorem $n$ more times, we find that $\phi^{(n+1)}$ has at least one zero, $\phi^{(n+1)}(\xi) = 0$.
    \begin{equation*}
        0 = \phi^{(n+1)}(\xi) = f^{(n+1)}(\xi) - (f(x) - p(x)) \frac{(n+1)!}{\prod_{i=0}^{n} (x - x_i)}
    \end{equation*}
    and this rearranges to \eqnref{eqnInterpError}.
\end{proof}
\begin{example}
    Consider $f(x) = \frac{1}{1 + x^2}$, interpolated over equally spaced points in $[-5, 5]$: $x_j = -5 + 10\frac{d}{n}$.

    \begin{figure}
        \centering
        \begin{tikzpicture}[mark size=0]
            \begin{axis}[
                    axis lines=middle,
                    xlabel={$x$},
                    ylabel={$y$},
                    legend entries={Function, $n = 5$, $n = 15$}
                ]
                \addplot table {Plot data/ExampleFunction.dat};
                \addplot table {Plot data/ExampleFunctionInterp1.dat};
                \addplot table {Plot data/ExampleFunctionInterp2.dat};
            \end{axis}
        \end{tikzpicture}
        \caption{Interpolation of example function with different values of $n$}
        \label{figInterpExp}
    \end{figure}
    \begin{figure}
        \centering
        \begin{tikzpicture}[mark size=0]
            \begin{axis}[
                    axis lines=middle,
                    xlabel={$x$},
                    ylabel={$y$},
                    legend entries={$n = 5$, $n = 15$}
                ]
                \addplot table {Plot data/ExampleFunctionError1.dat};
                \addplot table {Plot data/ExampleFunctionError2.dat};
            \end{axis}
        \end{tikzpicture}
        \caption{Errors in interpolation of example function with different values of $n$}
        \label{figInterpExpErr}
    \end{figure}

    Figure~\ref{figInterpExp} gives the output of polynomial interpolation with different values of $n$. We see that it breaks down near the endpoints, and this is clearly illustrated in the error plot of figure~\ref{figInterpExpErr}. These large spikes in error are called Runge spikes (or Runge's phenomena).

    We see that, in general, equally spaced points give poor output.
\end{example}
In order to provide good approximations, \thmref{thmInterpError} tells us that we should minimise:
\begin{equation*}
    \max_{x \in [a, b]} \left|\prod_{i=0}^{n} (x - x_i)\right|
\end{equation*}
The solution is given by the \textit{Chebyshev points}:
\begin{equation*}
    x_j = 5\cos\left(\frac{(n - j)\pi}{n}\right)
\end{equation*}
In the general case of a function on $[a, b]$, we would use:
\begin{equation}
    x_j = \frac{b + a}{2} + \frac{b - a}{2}\cos\left(\frac{(n - j)\pi}{n}\right)
    \label{eqnChebyshevPoints}
\end{equation}
\section{Divided Differences}
\subsection{Definition and Properties}
\begin{definition}{Divided difference}
    Given a function $f : \R \mapsto \R$ and a polynomial $p$ constructed as above, the \underline{divided difference} of $f$ at $\{x_0, \cdots, x_n\}$ is the leading coefficient (in $x^n$) in $p$. This is:
    \begin{equation*}
        f[x_0, \cdots, x_n] = \sum_{i=0}^{n} f(x_i) \prod_{\substack{j=0\\j \neq i}}^{n} \frac{1}{x_i - x_j}
    \end{equation*}
\end{definition}
\begin{example}
    For $n = 0$, we simply find $f[x_0] = f(x_0)$. When $n = 1$,
    \begin{equation*}
        f[x_0, x_1] = f(x_0) \frac{1}{x_0-x_1} + f(x_1) \frac{1}{x_1-x_0} = \frac{f(x_1) - f(x_0)}{x_1-x_0}
    \end{equation*}
    Then this $n = 1$ case gives some intuition as to why this is called the divided difference. This is furthered by the following theorem.
    \label{expDivDiff}
\end{example}
\begin{theorem}
    Suppose that $x_0, \cdots, x_{n+1}$ are distinct point in $\R$. Let $f : \R \mapsto \R$. Then the divided differences have the form:
    \begin{equation}
        f[x_0, \cdots, x_{n+1}] = \frac{f[x_1, \cdots, x_{n+1}] - f[x_0, \cdots, x_n]}{x_{n+1} - x_0}
        \label{eqnDivDiffRecurse}
    \end{equation}
    \label{thmDivDiffRecurse}
\end{theorem}
\begin{proof}
    \induction{$n = 1$}{
        Seen in example~\ref{expDivDiff}.
    }{$n = k$}{
        Assume that \eqnref{eqnDivDiffRecurse} holds for $k$ points.
    }{$n = k + 1$}{
        Let $p, q \in \P_k[x]$ be the polynomials that interpolate $[x_0, \cdots, x_k]$ and $[x_1, \cdots, x_{n+1}]$, respectively. Let $r \in \P_{k+1}[x]$ be defined by:
        \begin{equation*}
            r(x) = \frac{(x_{k+1} - x) p(x) + (x - x_0) q(x)}{x_{k+1}} \in \P_{k+1}[x]
        \end{equation*}
        Now note that $r(x_i) = f(x_i)~\forall i \in \{0, \cdots, k+1\}$. Therefore, by definition, $f[x_0, \cdots, x_{k+1}]$ is the leading coefficient of $r$. Then given the formula for $r$, this is:
        \begin{align*}
            f[x_0, \cdots, x_{k+1}] &= \frac{-(\text{leading coeff't of }p) + (\text{leading coeff't of }q)}{x_{k+1} - x_0} \\
            &= \frac{f[x_1, \cdots, x_{k+1}] - f[x_0, \cdots, x_k]}{x_{k+1} - x_0}
        \end{align*}
        by induction, as required.
    }
\end{proof}
\begin{theorem}
    Let $f \in C^n[a, b]$ and let $x_0, \cdots, x_n$ be distinct points in $[a, b]$. Then there exists $\xi \in [a, b]$ such that:
    \begin{equation*}
        f[x_0, \cdots, x_n] = \frac{f^{(n)}(\xi)}{n!}
    \end{equation*}
    \label{thmDivDiffDerivs}
\end{theorem}
\begin{proof}
    Let $p$ be the polynomial that interpolates $f$ at $x_0, \cdots, x_n$. Then $\phi = f - p$ has $n+1$ zeroes in $[a, b]$. By Rolle's theorem applied $n$ times, $\phi^{(n)}$ has at least one zero in $[a, b]$, and let this be $\xi$. Therefore, $f^{(n)}(\xi) = p^{(n)}(\xi)$.

    However, the $n$th derivative of $p$ is $n!$ times the leading coefficient,
    \begin{align*}
        n! f[x_0, \cdots, x_n] &= f^{(n)}(\xi) \\
        f[x_0, \cdots, x_n] &= \frac{f^{(n)}(\xi)}{n!} \\
    \end{align*}
\end{proof}
\begin{theorem}[Newton Interpolation Formula]
    Let $x_0, \cdots, x_n$ be distinct points in $[a, b]$, $f \in C[a, b]$. Let $p \in \P_n[x]$ be the polynomial that interpolates $f$ at the points $x_i$. Then:
    \begin{equation}
        \begin{split}
            p(x) = f[x_0] + f[x_0, x_n](x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1) + \cdots \\
            +f[x_0, \cdots, x_n] \prod_{j=1}^{n-1} (x - x_j)
        \end{split}
        \label{eqnNewtonInterp}
    \end{equation}
    \label{thmNewtonInterp}
\end{theorem}
\begin{proof}
    For $k \in \{0, \cdots, n\}$, let $p_k \in \P_k[x]$ be the polynomial that interpolates $f$ at $\{x_0, \cdots, x_k\}$. Note first that $p_0(x) = f(x_0)$.
    
    Consider $p_{k+1}(x) - p_k(x)$. We know that this vanishes at $x_0, \cdots, x_k$ ($k + 1$ roots), and must have degree at most $k + 1$.

    We also know the leading coefficient, this must be the leading coefficient of $p_{k+1}$ which is a divided difference:
    \begin{equation*}
        p_{k+1} = f[x_0, \cdots, x_{k+1}] \prod_{j=0}^{k} (x - x_j)
    \end{equation*}
    Then we can use a telescoping sum for $p(x)$:
    \begin{align*}
        p(x) &= p_n(x) = p_0(x) + (p_1 - p_0)(x) + \cdots + (p_n - p_{n-1})(x) \\
        &= f[x_0] + f[x_0, x_1] (x - x_0) + \cdots + f[x_0, \cdots, x_{n+1}] \prod_{j=0}^{n-1} (x - x_j)
    \end{align*}
\end{proof}
\subsection{Computing Divided Differences}
\Thmref{thmDivDiffRecurse} gives a natural method to compute divided differences. Consider figure~\ref{figDivDiff}. Each arrow in this diagram represents 3 operations, so the complexity is:
\begin{align*}
    C(n) &= 3\sum_{i=1}^{n} 1 \\
    &= 3 \frac{n(n+1)}{2} \\
    &= O(n^2)
\end{align*}
\begin{figure}
    \centering
    \begin{tikzpicture}[x=1.5cm, y=0.5cm]
        % Nodes
        \node (A1) at (0, 8) {$f(x_0)$};
        \node (A2) at (0, 6) {$f(x_1)$};
        \node (A3) at (0, 4) {$f(x_2)$};
        \node (A4) at (0, 2) {$\vdots$};
        \node (A5) at (0, 0) {$f(x_n)$};

        \node (B1) at (2, 7) {$f[x_0, x_1]$};
        \node (B2) at (2, 5) {$f[x_1, x_2]$};
        \node (B3) at (2, 3) {$\vdots$};
        \node (B4) at (2, 1) {$f[x_{n-1}, x_n]$};

        \node (C1) at (4, 6) {$f[x_0, x_1, x_2]$};
        \node (C2) at (4, 4) {$\vdots$};
        \node (C3) at (4, 2) {$f[x_{n-2}, x_{n-1}, x_n]$};

        \node at (5.5, 4) {$\cdots$};
        \node (D) at (7, 4) {$f[x_0, \cdots, x_n]$};

        % Arrows
        \begin{scope}[->, shorten <=2pt, shorten >= 7pt]
            \draw (A1.east) -- (B1.west);
            \draw (A2.east) -- (B1.west);
            \draw (A2.east) -- (B2.west);
            \draw (A3.east) -- (B2.west);
            \draw (A5.east) -- (B4.west);

            \draw (B1.east) -- (C1.west);
            \draw (B2.east) -- (C1.west);
            \draw (B4.east) -- (C3.west);
        \end{scope}
    \end{tikzpicture}
    \caption{Diagram of recursive calculation of divided differences}
    \label{figDivDiff}
\end{figure}
However, this is just to calculate the divided differences. In order to find $p(x)$, we need to compute:
\begin{align*}
    p(x) &= f[x_0] + f[x_0, x_1] (x - x_0) + \cdots + f[x_0, \cdots, x_n] \prod_{j=1}^{n} (x - x_j) \\
    \intertext{We can factorise this to give a simpler computation:}
    p(x) &= f[x_0] + (x - x_0) \left(f[x_0, x_1] + (x - x_1) \left(f[x_0, x_1, x_2] + (x - x_2)\left(\cdots\right)\right)\right)
\end{align*}
This is the Horner scheme, with total complexity of $O(n)$.
\section{Orthogonal Polynomials}
\subsection{Orthogonality and Inner Products}
\begin{definition}{Inner product}
    Let $V$ be a vector space over $\R$. Then an \underline{inner product} is a function $\inn{\cdot}{\cdot} : V \times V \mapsto \R$ satisfying, for $u, v \in V$,:
    \begin{itemize}
        \item Symmetry: $\inn{u}{v} = \inn{v}{u}$
        \item Linearity: $\inn{ax + by}{z} = a\inn{x}{z} + b\inn{y}{z}$
        \item Positivity: $\inn{x}{x} = 0 \iff x = \vec{0}$.
    \end{itemize}
\end{definition}
\begin{example}[Examples of inner products]
    We introduce the two canonical examples of inner products.
    On $\R^n$:
    \begin{align*}
        \inn{\cdot}{\cdot} : \R^n \times \R^n &\mapsto \R\\
        (\vec{u}, \vec{v}) &\mapsto \vec{u}^T \vec{v}
    \end{align*}

    On $C[a,b]$:
    \begin{align*}
        \inn{\cdot}{\cdot}_w : C[a, b] &\mapsto \R\\
        (f, g) &\mapsto \int_{a}^{b} f(x)g(x) w(x) dx 
    \end{align*}
    where $w$ is a \underline{weight function}, that must be strictly positive on $(a, b)$.
\end{example}
\begin{definition}{Orthogonal sequence}
    Given an inner product $\inn\cdot\cdot$, a sequence $p_0, p_1, \cdots$ is a  \underline{sequence of orthogonal polynomials} if:
    \begin{enumerate}
        \item $\inn{p_i}{p_j} = 0$
        \item $\deg(p_i) = i$
    \end{enumerate}
\end{definition}
\begin{theorem}
    For any $n \geq 0$, there exists a unique monic orthogonal polynomial of degree $n$.
    \label{thmUniqueOrthoSequence}
\end{theorem}
\begin{proof}
    \induction{$n = 0$}{
        There is a unique monic polynomial of degree $0$, $p_0(x) = 1$.
    }{$n = k$}{
        Assume that $p_0, \cdots, p_k$ are a sequence of orthogonal monic polynomials.
    }{$n = k + 1$}{
        We use a Gram-Schmidt process on the polynomial $q(x) = x^{k+1}$. Define:
        \begin{equation*}
            p_{k+1} = q - \sum_{m=0}^{k} \frac{\inn{q}{p_m}}{\inn{p_m}{p_m}}
        \end{equation*}
        We know that $p_{k+1}$ is monic because the $x^{k+1}$ coefficient only comes from $q$, so is $1$.

        For $i \leq k$,
        \begin{align*}
            \inn{p_{k+1}}{p_i} &= \inn{q}{p_i} - \sum_{m=0}^{k} \frac{\inn{q}{p_m}}{\inn{p_m}{p_m}} \inn{p_m}{p_i} \\
            &= \inn{q}{p_i} - \frac{\inn{q}{p_i}}{\inn{p_i}{p_i}} \inn{p_i}{p_i} \\
            &= \inn{q}{p_i} - \inn{q}{p_i} \\
            &= 0
        \end{align*}
        \begin{subproof}{$p_{k+1}$ is unique}
            Assume that $\tilde{p}_{k+1}$ is another monic orthogonal polynomial. Let $p = \tilde{p}_{k+1} - p_{k+1}$. Then subtracting these two polynomials gives a polynomial of degree $k$ because both are monic.

            That is, $p \in \P_k[x]$, but also we know that $p$ is orthogonal to $p_0, \cdots, p_k$. That is, it is orthogonal to a basis of $\P_k[x]$, so it must be the zero vector (it resides in a space of dimension 0).
        \end{subproof}
    }
\end{proof}
\begin{example}
    Considering the inner product:
    \begin{equation*}
        \inn{p}{q} = \int_{-1}^{1} p(x) q(x) dx 
    \end{equation*}
    We find the orthogonal polynomials are the \underline{Legendre polynomials}.

    \begin{tabular}{|c c|}
        \hline
        $i$ & $p_i$ \\
        \hline
        $0$ & $1$ \\
        $1$ & $x$ \\
        $2$ & $x^2 - \frac13$ \\
        $3$ & $x^3 - \frac35 x$ \\
        \hline
    \end{tabular}
\end{example}
\begin{example}
    Now consider a more general inner product,
    \begin{equation*}
        \inn{p}{q} = \int_{a}^{b} p(x) q(x) w(x) dx 
    \end{equation*}
    where $w(x) > 0$ is a weight function.

    \begin{tabular}{|c c c|}
        \hline
        $w(x)$ & Interval & Polynomials \\
        \hline
        $1$ & $[-1, 1]$ & Legendre Polynomials \\
        $\frac{1}{\sqrt{1 - x^2}}$ & $[-1, 1]$ & Chebyshev polynomials \\
        $e^{-x}$ & $[0, \infty)$ & Laguerre polynomials \\
        $e^{-x^2}$ & $\R$ & Hermite polynomials \\
        \hline
    \end{tabular}
\end{example}
\subsection{Three-Term Recurrence Relation}
Under mild assumptions on the inner product, we can prove an interesting result.
\begin{theorem}[Three-Term Recurrence]
    Suppose that an inner product $\inn\cdot\cdot$ satisfies, for all $p, q \in \P_n[x]$,
    \begin{equation*}
        \inn{xp}{q} = \inn{p}{xq}.
    \end{equation*}
    Let $(p_i)_{i = 1}^\infty$ be the sequence of monic polynomials for this inner product. Then for all $n \in \N$,

    \begin{equation}
        p_{n+1} = (x - \alpha_n) p_n - \beta_n p_{n-1}
        \label{eqn3TermRecur}
    \end{equation}
    where:
    \begin{align*}
        p_0 &= 1, p_{-1} = 0 \text{ (notationally)} \\
        \alpha_n &= \frac{\inn{xp_n}{p_n}}{\inn{p_n}{p_n}},\quad \beta_n = \frac{\inn{p_n}{p_n}}{\inn{p_{n-1}}{p_{n-1}}} > 0
    \end{align*}    
    \label{thm3TermRecur}
\end{theorem}
\begin{remark}
    All weighted integral inner products of the form above have this property, so we already have a large class of inner products on which this theorem holds.
\end{remark}
\begin{proof}
    Let $\psi = p_{n+1} - (x - \alpha_n) p_n + \beta_n p_{n-1}$. Note that since the $p_i$ are monic, the $x^{n+1}$ term cancels and $\psi \in \P_n[x]$. Now we show that $\inn{\psi}{p_m} = 0~\forall m \leq n$.

    \begin{align}
        \inn{\psi}{p_m} &= \inn{p_{n+1}}{p_m} - \inn{(x - \alpha_n) p_n}{p_m} + \beta_n \inn{p_{n-1}}{p_m} \nonumber \\
        &= -\inn{(x - \alpha_n) p_n}{p_m} + \beta_n \inn{p_{n-1}}{p_m} \label{eqn3TermInnProd}
    \end{align}
    Consider first $m < n-1$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)p_n}{p_m} \\
        &= -\inn{xp_n}{p_m} + \alpha_n \inn{p_n}{p_m} \\
        &= -\inn{p_n}{xp_m}
    \end{align*}
    Then since $xp_m \in \P_{n-1}[x]$, the inner product is zero.

    Consider now $m = n-1$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)p_n}{p_{n-1}} + \beta_n \inn{p_{n-1}}{p_{n-1}} \\
        &= -\inn{x p_n}{p_{n-1}} + \inn{p_n}{p_n} \\
        &= -\inn{p_n}{x p_{n-1}} + \inn{p_n}{p_n} \\
        \intertext{Note that $x p_{n-1}$ is a monic polynomial. Write as a linear combination of $p_i$}
        &= -\inn{p_n}{p_n} + \inn{p_n}{p_n} + \sum_{i=0}^{n-1} \inn{c_i p_i}{p_n} \\
        &= 0
    \end{align*}

    Now consider $m = n$.
    \begin{align*}
        (\ref{eqn3TermInnProd}) &= -\inn{(x - \alpha_n)}{p_n} + \beta_n \inn{p_{n-1}}{p_n} \\
        &= -\inn{xp_n}{p_n} + \alpha_n \inn{p_n}{p_n} \\
        &= -\inn{xp_n}{p_n} + \inn{x p_n}{p_n} \\
        &= 0
    \end{align*}
\end{proof}
\begin{example}[Chebyshev polynomials]
    Consider the inner product:
    \begin{equation*}
        \inn{p}{q} = \int_{-1}^{1} \frac{p(x) q(x)}{\sqrt{1 + x^2}} dx
    \end{equation*}
    Let $T_0, T_1, \cdots$ be the a set of orthogonal polynomials with respect to this inner product. These are the \underline{Chebyshev polynomials}, defined by:
    \begin{equation*}
        T_n(\cos\theta) = \cos(n\theta)
    \end{equation*}
    and have leading coefficients $2^{n-1}$.
    \begin{align*}
        &\cos((n+1)\theta) + \cos((n-1)\theta) = 2\cos(n\theta) \cos(\theta) \\
        \therefore &T_{n+1}(x) + T_{n-1}(x) = 2T_n(x) x \\
        &T_{n+1} = 2x T_n(x) - T_{n-1}
    \end{align*}
    Now let $\hat{T}_n(x)$ be the monic versions. Note that $\hat{T}_n(x) = 2^{1-n} T_n(x)$. The 3-term recurrence is:
    \begin{equation*}
        \hat{T}_{n+1}(x) = x \hat{T}_n(x) - \frac14 \hat{T}_{n-1}(x)
    \end{equation*}
    and therefore $\alpha_n = 0, \beta_n = \frac14$.
\end{example}
\subsection{Least-Squares Polynomial Fitting}
We now return to the study of polynomial approximation. Now we have an inner product on $C[a, b]$, we want to find the polynomial $p$ of degree $n$ that minimises $\norm{f - p}$ for any $f \in C[a, b]$.
\begin{theorem}[Least-Squares Polynomial Theorem]
    Let $f \in C[a, b]$. The polynomial $p \in \P_n[x]$ that minimises $\norm{f - p}$ is given by:
    \begin{equation}
        p = \sum_{k=0}^{n} \frac{\inn{f}{p_k}}{\inn{p_k}{p_k}} p_k
        \label{eqnLeastSquaresPoly}
    \end{equation}
    where $(p_k)_{i=0}^n$ are the first $n$ monic orthogonal polynomials for the given inner product.
    \label{thmLeastSquaresPoly}
\end{theorem}
\begin{remark}
    We understand this as an orthogonal projection of $f$ in $C[a, b]$ onto the subspace $\P_n[x]$, since this minimises the norm.
\end{remark}
\begin{proof}
    Any $p \in \P_n[x]$ can be expressed as:
    \begin{equation*}
        p = \sum_{k=0}^{n} c_k p_k
    \end{equation*}
    Then the goal is to find the coefficients $c_i$ such that $\norm{f - p}$ is minimal. Expanding out $\norm{f - p}^2$:
    \begin{align*}
        \norm{f - \sum_{k-0}^{n} c_k p_k}^2 &= \inn{f - \sum_{k=0}^n c_k p_k}{f - \sum_{k=0}^n c_k p_k} \\
        &= \inn{f}{f} - 2 \inn{f}{\sum_{k=0}^{n} c_k p_k} - \sum_{k=0}^{n} \sum_{j=0}^{n} c_k c_j \inn{p_k}{p_j} \\
        &= \inn{f}{f} - 2 \sum_{k=0}^{n}c_k \inn{f}{p_k} + \sum_{k=0}^{n} c_k^2 \inn{p_k}{p_k} \\
        &= \inn{f}{f} \sum_{k=0}^{n}\left(c_k^2 \inn{p_k}{p_k} - 2 c_k \inn{f}{p_k}\right)
    \end{align*}
    The terms in the sum are independent of each other (with respect to the $c_i$), so we have $n$ independent minimisation problems:
    \begin{equation*}
        \text{Minimise } c_k^2 \inn{p_k}{p_k} - 2 c_k \inn{f}{p_k}
    \end{equation*}
    Which gives the result:
    \begin{equation*}
        c_k = \frac{\inn{f}{p_k}}{\inn{p_k}{p_k}}.
    \end{equation*}
\end{proof}
The value of $\norm{f - p}$ is:
\begin{equation*}
    \norm{f - p} = \inn{f}{f} - \sum_{k=0}^{n} \frac{\inn{f}{p_k}^2}{\inn{p_k}{p_k}}
\end{equation*}
We would like to know that this error terms goes to $0$ as $n \to \infty$.

\begin{theorem}
    Let $[a, b]$ be a finite interval. Let $f \in C[a, b]$. Then for any $\epsilon > 0$, there exists a polynomial of sufficiently high degree such that:
    \begin{equation*}
        \norm{f - p}_{\infty} = \sup_{x \in [a, b]} \abs{f(x) - p(x)} \leq \epsilon
    \end{equation*}
    \label{thmLeastSquareInfNorm}
\end{theorem}
This theorem will be used without proof.
\begin{theorem}
    Given a finite interval $[a, b]$ and a function $f \in C[a, b]$, the least-squares polynomial $\hat{p}_n$ of degree $n$ has:
    \begin{equation*}
        \norm{f - \hat{p}_n}^2 \to 0 \text{ as } n \to \infty
    \end{equation*}
    \label{thmLeastSquareError}
\end{theorem}
\begin{proof}
    Note that the norms are bounded as follows:
    \begin{equation*}
        \norm{f - p}^2 = \int_a^b w(x) (f(x) - p(x))^2 dx \leq \left(\sup_{x \in [a, b]} \abs{f(x) - p(x)}\right)^2
    \end{equation*}
    Now let $\delta > 0$. Then by \thmref{thmLeastSquareInfNorm} we have a polynomial of high enough degree $n$ such that
    \begin{equation*}
        \norm{f - p}_{\infty} \leq \sqrt{\frac{\delta}{\int_{a}^{b} w(x) dx }}.
    \end{equation*}
    Then let $\hat{p}_n$ be the polynomial given by \thmref{thmLeastSquaresPoly}. We know:
    \begin{align*}
        \norm{f - \hat{p}_n}^2 &\leq \norm{f - p}^2 \\
        &\leq \norm{f - p}_\infty^2 \int_{a}^{b} w(x) dx \leq \delta
    \end{align*}
    Then since also $\norm{f - \hat{p}_n}^2$ is decreasing with $n$, it must go to $0$.
\end{proof}
\subsection{Least-Squares Fitting on Discrete Function Values}
Consider the problem:
\begin{equation*}
    \min_{p \in \P_n[x]} \sum_{k=1}^{m} w_k (p(x_k) - f(x_k))^2 \text{ for $m > n$}
\end{equation*}
Then we define a pseudo-inner product:
\begin{equation*}
    \inn{g}{h} = \sum_{k=1}^{m} w_k g(x_k) h(x_k)
\end{equation*}
Then this does define a valid inner product, but only on $\P_{m-1}$. %TODO: What?

Therefore, assuming $m > n$, we can still construct orthogonal polynomials $p_0, \cdots, p_n$ using \thmref{thm3TermRecur} and use the formula given in \thmref{thmLeastSquaresPoly}.
\section{Gaussian Quadrature}
Here we want to compute quantities of the form:
\begin{equation}
    \int_{a}^{b} w(x) f(x) dx \approx \sum_{k=1}^{\nu} b_k f(c_k)
    \label{eqnGaussQuadrat}
\end{equation}
where we say $b_k$ are the weights, and $c_k$ are the nodes.

We want to find a method to obtain the weights $b_k$ and the nodes $c_k$. We also want to find the ``best'' approximation. We will do this by ensuring equality for polynomials up to degree $n$, and trying to find the largest value for $n$.
\subsection{Finding the Largest Order}
\begin{definition}{Order}
    If \eqnref{eqnGaussQuadrat} is an equality for all $f \in \P_m[x]$, and $m$ is the largest such, $m$ is the \underline{order} of the quadrature formula.
\end{definition}
\begin{proposition}
    The order of a quadrature formula must be less than $2\nu$.
    \label{propOrderNotDouble}
\end{proposition}
\begin{proof}
    We simply construct a polynomial of degree $2\nu$ that does not satisfy an equality in \eqnref{eqnGaussQuadrat}.

    Define:
    \begin{equation*}
        f(x) = \prod_{k=1}^{\nu} (x - c_k)^2
    \end{equation*}
    We see that this has degree $2\nu$, and the integral must be positive (the function $f$ is non-zero and everywhere non-negative). However, sampling only at the nodes $c_k$ gives $0$.
\end{proof}
\begin{lemma}
    Given a weighted inner product:
    \begin{equation*}
        \inn{p}{q} = \int_{a}^{b} w(x) p(x) q(x) dx,
    \end{equation*}
    let $(p_i)_{i=1}^n$ be the orthogonal polynomials with respect to this inner product.

    Then for any $n \geq 1$, $p_n$ has $n$ distinct roots in $(a, b)$.
    \label{lemOrthogNRoots}
\end{lemma}
\begin{proof}
    Let $\xi_1, \cdots, \xi_m$ be the points in $(a, b)$ where $p_n$ changes sign. We want to prove that $m = n$. Define:
    \begin{equation*}
        q(x) = \prod_{i=1}^{m} (x - \xi_i) \in \P_m[x].
    \end{equation*}
    Note that the product $p(x) q(x)$ must be either non-positive or non-negative, since both polynomials change sign at the same point.
    \begin{equation*}
        \inn{p_n}{q} = \int_{a}^{b} \underbrace{w(x)}_{>0} \underbrace{p_n(x) q(x)}_{\geq 0 \text{ or } \leq 0} dx \neq 0  \\
    \end{equation*}
    Then since $\inn{p_n}{q} \neq 0$, we must have that $q \notin \P_{n-1}[x]$ because $p_n$ is orthogonal to this subspace.
    Therefore $m \geq n$. However, $p_n$ is an $n$ degree polynomial so can have at most $n$ roots, so we conclude $m = n$.
\end{proof}
\begin{theorem}[Gaussian Quadrature]
    Let $\nu \in \N$ and $w \in C[a, b]$ with $w(x) > 0~\forall x \in (a, b)$.
    \begin{enumerate}
        \item Let $c_1, \cdots, c_\nu$ be distinct points in $(a, b)$ and define the weights as follows:
            \begin{equation*}
                b_k = \int_{a}^{b} w(x) L_k(x) dx \text{ where } L_k(x) = \prod_{\substack{i=1\\i\neq k}}^{\nu} \frac{x - c_i}{c_k - c_i}
            \end{equation*}
            Then for all $f \in \P_{\nu - 1}[x]$, we have:
            \begin{equation*}
                \int_{a}^{b} w(x) f(x) dx = \sum_{k=1}^{\nu} b_k f(c_k)
            \end{equation*}
        \item If we choose the $c_i$ to be the $\nu$ roots of $p_{\nu}$, then:
            \begin{equation*}
                \int_{a}^{b} w(x) f(x) dx = \sum_{k=1}^{\nu} b_k f(c_k)
            \end{equation*}
            for all $f \in \P_{2\nu - 1}[x]$.
    \end{enumerate}
    \label{thmGaussQuadrat}
\end{theorem}
\begin{proof}
    Let $f \in \P_{\nu - 1}[x]$. Consider:
    \begin{equation*}
        \tilde{f}(x) = \sum_{k=1}^{n} f(c_k) L_k(x)
    \end{equation*}
    and we want to show that $f = \tilde{f}$. Therefore, consider $f - \tilde{f}$. Since both $f$ and $\tilde{f}$ have degree $\nu - 1$, $\deg(f - \tilde{f}) \leq \nu - 1$. However, we can find $\nu$ roots of $f - \tilde{f}$, namely the $c_i$. Therefore, $f - \tilde{f}$ must be the zero polynomial, and $f = \tilde{f}$. We have therefore:
    \begin{equation*}
        \int_{a}^{b} w(x) f(x) dx = \int_{a}^{b} w(x) \sum_{k=1}^{\nu} f(c_k) L_k(x) dx  = \sum_{k=1}^{\nu} f(c_k) \underbrace{\int_{a}^{b} w(x) L_k(x) dx}_{b_k}
    \end{equation*}
    and we have proven point 1.

    Now assume that $c_1, \cdots, c_\nu$ are the distinct roots of $p_{\nu}$. Let $f \in \P_{2\nu - 1}$. We apply the polynomial division algorithm and get:
    \begin{equation*}
        f = p_\nu q + r
    \end{equation*}
    where $q$ is the quotient with $\deg(q) \leq \nu - 1$, and $r$ is the remainder which also has $\deg(r) \leq \nu - 1$.
    \begin{align*}
        \int_{a}^{b} w(x) f(x) dx &= \int_{a}^{b} w(x) p_\nu(x) q(x) dx + \int_{a}^{b} w(x) r(x) dx \\
        &= \underbrace{\inn{p_\nu}{q}}_{0} + \int_{a}^{b} w(x) r(x) dx \\
        &= \sum_{k=1}^{\nu} b_k r(c_k) \text{ by point 1 of the theorem} \\
        &= \sum_{k=1}^{\nu} b_k f(c_k) \text{ since $p_\nu(c_k) = 0$ so $f(c_k) = r(c_k)$}
    \end{align*}
    which is as required.
\end{proof}
\subsection{Peano Kernel Theorem}
Consider the error of a quadrature formula:
\begin{equation*}
    L(f) = \int_{a}^{b} w(x) f(x) dx - \sum_{k=1}^{\nu} b_k f(c_k)
\end{equation*}
Then we want to find a bound for $L$ when $f \notin \P_n[x]$ (where $n$ is the order of the quadrature formula).
Consider $f \in C^{n+1}[a, b]$. Then we use Taylor's Theorem with integral remainder:
\begin{align*}
    f(x) &= \underbrace{f(a) + (x - a) f'(a) + \frac{(x-a)^2}{2} f''(a) + \cdots + \frac{(x-a)^n}{n!} f^{(n)} (a)}_{g(x)}\\
    &\qquad+ \underbrace{\frac{1}{n!} \int_{a}^{x} f^{n+1}(\theta)(x - \theta)^n d\theta}_{h(x)}
\end{align*}
then $g(x)$ is a polynomial of degree $n$. Therefore $L(g(x)) = 0$. So, by linearity of $L$, $L(f) = L(h)$. Similarly, we can simplify $h$:
\begin{equation*}
    \frac{1}{n!} \int_{a}^{b} f^{(n+1)}(\theta) \underbrace{\left(\max\{(x - \theta), 0\}\right)^n}_{h_\theta(x)} d\theta
\end{equation*}
where $h_\theta$ is another function of $\theta$ and $x$.

At this point, we will provide a claim that uses the concept of infinite summation as integration, but the proof will not be provided.
\begin{align*}
    L(f) &= L(h) \\
    &= \frac{1}{n!} f^{(n+1)}(\theta) \underbrace{L(h_\theta)}_{K(\theta)} d\theta
\end{align*}
here $K(\theta)$ is the \underline{Peano Kernel}.
\begin{theorem}
    Assume $L$ is a linear functional such that $L(f) = 0$ for all $f \in \P_n[x]$. Then for all $f \in C^{n+1}[a, b]$, we have:
    \begin{equation}
        L(f) = \frac{1}{n!} \int_{a}^{b} f^{(n+1)}(\theta) K(\theta) d\theta
        \label{eqnPeanoKernel}
    \end{equation}
    where $K(\theta) = L\left(\left(\max\{x - \theta, 0\}\right)^n\right)$.
    \label{thmPeanoKernel}
\end{theorem}
\begin{corollary}
    \begin{equation*}
        |L(f)| \leq \frac{1}{n!} \int_{a}^{b} \abs{f^{(n+1)}(\theta)} \abs{K(\theta)} d\theta \leq \frac{\norm{f^{(n+1)}}_\infty}{n!} \int_{a}^{b} \abs{K(\theta)} d\theta 
    \end{equation*}
    \label{corPeanoKernelBound}
\end{corollary}
\begin{example}
    Consider Simpson's Rule, given by:
    \begin{equation*}
        \int_{-1}^{1} f(x) dx \approx \frac13 (f(-1) + 4f(0) + f(1))
    \end{equation*}
    Therefore our error functional is:
    \begin{equation*}
        L(f) = \int_{-1}^{1} f(x) dx - \frac13 (f(-1) + 4f(0) + f(1))
    \end{equation*}
    We can check that $L(f) = 0$ for $f \in \P_2[x]$.

    Computing $K(\theta)$:
    \begin{align*}
        K(\theta) &= L\left(\left(\max\{x - \theta, 0\}\right)^2\right) \\
        &= \int_{-1}^{1} h_\theta(x) dx - \frac13 \left(h_\theta(-1) + 4 h_\theta(0) + h_\theta(1)\right)
    \end{align*}
    Now, $h_\theta(-1) = 0$ and $h_\theta(1) = (1 - \theta)^2$.
    \begin{equation*}
        h_\theta(0) =
        \begin{cases}
            0 & \theta \geq 0 \\
            \theta^2 & \theta \leq 0
        \end{cases}
    \end{equation*}
    \begin{align*}
        \int_{-1}^{1} h_\theta(x) dx &= \int_\theta^1 (x - \theta)^2 dx \\
        &= \left[\frac{(x - \theta)^3}{3}\right]_\theta^1 \\
        &= \frac{(1 - \theta)^3}{3}
    \end{align*}
    Putting all this together,
    \begin{align*}
        K(\theta) &=
        \begin{cases}
            \frac{(1 - \theta)^3}{3} - \frac13(4\theta^2 + (1 - \theta)^2) & \theta \leq 0 \\
            \frac{(1 - \theta)^3}{3} - \frac13(1 - \theta)^2 & \theta \geq 0 \\
        \end{cases}\\
        &= \begin{cases}
            -\frac13 \theta(1 + \theta)^2 & \theta \leq 0 \\
            -\frac13 \theta(1 - \theta)^2 & \theta \geq 0
        \end{cases}
    \end{align*}
    Then computing the integral to bound the error:
    \begin{align*}
        L(f) &= \frac{1}{2!} \int_{-1}^{1} f^{(3)}(\theta) K(\theta) d\theta \\
        &\leq \norm{f^{(3)}}_\infty \frac12 \int_{-1}^{1} \abs{K(\theta)} d\theta \\
        &= \frac{1}{36} \norm{f^{(3)}}_\infty
    \end{align*}
\end{example}
\begin{example}
    We can consider $L$ to be something other than a quadrature error.
    \begin{equation*}
        L(f) = f'(0) - \left(-frac32 f(0) + 2f(1) - frac12 f(2)\right)
    \end{equation*}
    Then indeed $L$ is a linear functional. We want to know what $L$ does to low-degree polynomials:
    \begin{align*}
        L(1) &= 0 - \left(-frac32 + 2 - \frac12\right) = 0 \\
        L(x) &= 1 - \left(-\frac32 (0) + 2(1) - \frac12(2)\right) = 0
        L(x^2) &= 0 - \left(-\frac32(0) + 2(1) - \frac12(4)\right) = 0
        L(x^3) &= 0 - \left(-\frac32(0) + 2(1) - \frac12(8)\right) = -4 \neq 0
    \end{align*}
    Therefore we take $n = 2$. Therefore $L(f) = 0$ for $f \in \P_2[x]$. To compute the Peano kernel, we need $L(h_\theta)$:
    \begin{align*}
        h_\theta'(0) &= 0 \text{ if $\theta \geq 0$} \\
        h_\theta(0) &= 0 \text{ if $\theta \geq 0$} \\
        h_\theta(1) &=
        \begin{cases}
            0 & \theta \geq 1 \\
            (1 - \theta)^2 & \theta \leq 1
        \end{cases}\\
        h_\theta(2) &=
        \begin{cases}
            0 & \theta \geq 2 \\
            (2 - \theta)^2 & \theta \geq 2
        \end{cases}
    \end{align*}
    \begin{align*}
        L(h_\theta) &=
        \begin{cases}
            0 - \left(-\frac32(0) + 2(1 - \theta)^2 - \frac12 (2 - \theta)^2\right) & \theta \in [0, 1] \\
            0 - \left(-\frac32(0) + 2(0) - \frac12 (2 - \theta)^2\right) & \theta \in [1, 2] \\
            0 & \theta \geq 2
        \end{cases}\\
        &=
        \begin{cases}
            2\theta - \frac32\theta^2 & \theta \in [0, 1] \\
            \frac12 (2-\theta)^2 & \theta \in [1, 2] \\
            0 & \theta \geq 2
        \end{cases}
    \end{align*}
\end{example}
\end{document}