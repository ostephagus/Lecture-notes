\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Definitions}
\begin{definition}{Hypothesis}
    A \underline{hypothesis} is an assumption about the distribution of data $\vec{X}$ taking values in $\samplespace$.
\end{definition}
\begin{definition}{Null hypothesis}
    The \underline{null hypothesis}, $H_0$ is the base case or ``no effect'' hypothesis.
\end{definition}
\begin{definition}{Alternative hypothesis}
    The \underline{alternative hypothesis}, $H_1$ is the hypothesis of a positive/negative effect.
\end{definition}
\begin{example}
    Consider flipping coins, $X_i \sim \text{Ber}(\theta)$. The null hypothesis is $H_0 : \theta = \frac12$. Examples of alternative hypotheses could be:
    \begin{itemize}
        \item $\theta = \frac34$
        \item $\theta \neq \frac12$
        \item $\theta < \frac12$
    \end{itemize}
\end{example}
\begin{example}[Goodness of Fit Test]
    Suppose that $X_i$ are independent and identically distributed random variable for $i = 1$ to $n$.
    We can then make hypotheses about the distribution function:
    \begin{align*}
        H_0:&X_i \text{ have pdf } f_1 \\
        H_1:&X_i \text{ have pdf } f_2 \\
    \end{align*}
\end{example}
\begin{example}
    Suppose that $\vec{X}$ has a pdf $f_{\vec{X}}(\vec{x}, \theta)$ for some $\theta \in \Theta$. We can make hypotheses about $\theta$:
    \begin{align*}
        H_0:&\theta \in \Theta_0 \subset \Theta \\
        H_1:&\theta \notin \Theta_0
    \end{align*}
\end{example}
\begin{definition}{Simple hypothesis}
    A hypothesis is \underline{simple} if it fully specifies the distribution of the data $\vec{X}$.

    The opposite of a simple hypothesis is a \underline{composite hypothesis}.
\end{definition}
\begin{definition}{Hypothesis test}
    A \underline{test} of a hypothesis $H_0$ is defined by a \underline{critical region} $C$. When $\vec{X} \in C$, we reject $H_0$. When $\vec{X} \notin C$, we do not reject $H_0$.
\end{definition}
\begin{remark}
    Note that when $\vec{X} \notin C$, we do not \textit{accept} $H_0$. We are simply saying that there is not enough data to reject it.
\end{remark}
\begin{definition}{Type I/II errors}
    A \underline{type I error} is the case of rejecting $H_0$ when $H_0$ was in fact true.
    A \underline{type II error} is the case of failing to reject $H_0$ when $H_1$ was true.
\end{definition}
\begin{definition}{Power/size}
    When $H_0$ and $H_1$ are simple hypotheses, the \underline{size} of the test is the probability of a type I error,
    \begin{equation*}
        \alpha = \P_{H_0}(H_0 \text{ rejected}) = \P_{H_0}(\vec{X} \in C).
    \end{equation*}
    The \underline{power} of the test is the probability of no type II errors. The power is $1 - \beta$ where:
    \begin{equation*}
        \beta = \P_{H_1}(H_0 \text{ not rejected}) = \P_{H_1}(\vec{X} \notin C)
    \end{equation*}
\end{definition}
Usually, we set $\alpha$ at an acceptable level and choose a test that minimises $\beta$ subject to the condition on $\alpha$.
\section{Neyman-Pearson Lemma}
\begin{definition}{Likelihood ratio statistic}
    Let $H_0$ and $H_1$ be simple, with $\vec{X}$ having pdf $f_i$ under $H_i$ ($i = 0$ or $1$). The \underline{likelihood ratio statistic} is:
    \begin{equation*}
        \Lambda_{\vec{X}}(H_0, H_1) = \frac{f_1(\vec{X})}{f_0(\vec{X})}
    \end{equation*}
\end{definition}
\begin{definition}{Likelihood ratio test}
    A \underline{likelihood ratio test} rejects $H_0$ when $\vec{X} \in C = \subsetselect{\vec{x}}{\Lambda_{\vec{X}}(H_0, H_1) > k}$.
\end{definition}
\begin{theorem}[Neyman-Pearson Lemma]
    Suppose $f_0$ and $f_1$ are non-zero on the same set and there exists $k$ such that the critical region:
    \begin{equation*}
        C_k = \subsetselect{\vec{x}}{\frac{f_1(\vec{x})}{f_2(\vec{x})} > k}
    \end{equation*}
    has size $\alpha$. Then for all tests with size $\leq \alpha$, the LRT is the test with smallest $\beta$.
    \label{thmNeymanPearson}
\end{theorem}
\begin{proof}
    Let $\overline{C}$ be the complement of $C$.
    \begin{align*}
        \alpha &= \P_{H_0}(\vec{X} \in C) = \int_C f_0(\vec{x}) d^n \vec{x} \\
        \beta &= \P_{H_1}(\vec{X} \in \overline{C}) = \int_{\overline{C}} f_1(\vec{x}) d^n \vec{x}
    \end{align*}
    Then let $C^\star$ be the critical region of another test of size $\alpha^\star \leq \alpha$. We want to show that $\beta \leq \beta^\star$.
    \begin{align*}
        \beta - \beta^\star &= \int_{\overline{C}} f_1(\vec{x}) d^n \vec{x} - \int_{\overline{C^\star}} f_1(\vec{x}) d^n \vec{x}\\
        &= \int_{\overline{C} \cap C^\star} f_1(\vec{x}) d^n\vec{x} - \int_{\overline{C^\star} \cap C} f_1(\vec{x}) d^n\vec{x} \\
        &= \int_{\overline{C} \cap C^\star} \underbrace{\frac{f_1(\vec{x})}{f_0(\vec{x})}}_{\text{$\leq k$ on $\overline{C}$}} f_0(\vec{x}) d^n\vec{x} - \int_{\overline{C^\star} \cap C} \underbrace{\frac{f_1(\vec{x})}{f_0(\vec{x})}}_{\text{$> k$ on $C$}} f_0(\vec{x}) d^n\vec{x} \\
        &\leq k\left(\int_{\overline{C} \cap C^\star} f_0(\vec{x}) d^n\vec{x} - \int_{\overline{C^\star} \cap C} f_0(\vec{x}) d^n\vec{x}\right) \\
        &= k\left(\int_{\overline{C} \cap C^\star} f_0(\vec{x}) d^n\vec{x} + \int_{C \cap C^\star} f_0(\vec{x}) d^n\vec{x} - \int_{C \cap C^\star} f_0(\vec{x}) d^n\vec{x}\right.\\
        &\qquad\left. - \int_{\overline{C^\star} \cap C} f_0(\vec{x}) d^n\vec{x}\right) \\
        &= k\left(\int_{C^\star} f_0(\vec{x}) d^n\vec{x} - \int_C f_0(\vec{x}) d^n\vec{x}\right) \\
        &= k(\alpha^\star - \alpha)
    \end{align*}
    Therefore, since we define $\alpha^\star \leq \alpha$, we find $\beta^\star \geq \beta$.
\end{proof}
\begin{remark}
    A likelihood ratio test with size $\alpha$ does not always exist for any given $\alpha$. However, we can always define a ``randomised'' test with exact size $\alpha$. That is, we define a region where $H_0$ is always rejected, and define a region where there is a certain probability that $H_0$ is rejected.
\end{remark}
\begin{example}
    Suppose $X_i \sim N(\mu, \sigma_0^2)$ for $i = 1$ to $n$. Let $\sigma_0^2$ be fixed and known. Consider the hypotheses:
    \begin{align*}
        H_0: & \mu = \mu_0 \\
        H_1: & \mu = \mu_1
    \end{align*}
    for fixed $\mu_1 > \mu_0$. The likelihood ratio is:
    \begin{align*}
        \Lambda_{\vec{X}}(H_0, H_1) &= \frac{(2\pi\sigma_0^2)^{-n / 2} \exp\left(-\frac{1}{2\sigma_0^2} \sum_{i=1}^{n} (X_i - \mu_1)^2\right)}{(2\pi\sigma_0^2)^{-n / 2} \exp\left(-\frac{1}{2\sigma_0^2} \sum_{i=1}^{n} (X_i - \mu_0)^2\right)} \\
        &= \exp\left(\frac{\mu_1 - \mu_0}{\sigma_0^2} n \overline{X} + \frac{n(\mu_0^2 - \mu_1^2)}{2\sigma_0^2}\right)
    \end{align*}
    then since $\Lambda_{\vec{X}}(H_0, H_1)$ is monotone in $\overline{X}$,
    \begin{equation*}
        \Lambda_{\vec{X}}(H_0, H_1) > k \iff \overline{X} > c
    \end{equation*}
    By the same logic, define $Z = \frac{\sqrt{n}(\overline{X} - \mu_0)}{\sigma_0}$ which is standard normal under $\mu_0$. Therefore, the rejection region is $\{Z > c'\}$ Therefore, with size $\alpha$, we choose:
    \begin{equation*}
        C = \subsetselect{\vec{X}}{Z > \Phi^{-1}(1-\alpha)}
    \end{equation*}
\end{example}
\section{\texorpdfstring{$p$}{p}-Values}
\begin{definition}{$p$-value}
    For a test with critical region of the form $\subsetselect{\vec{x}}{T(\vec{x}) > k}$, where $T$ is some statistic, the \underline{$p$-value} is:
    \begin{equation*}
        p = \P_{H_0}(T(\vec{X}) > T(\vec{x}^*))
    \end{equation*}
    where $\vec{x}^*$ is the observed data.
\end{definition}
\begin{remark}
    This is the probability of observing more extreme data under $H_0$.
\end{remark}
\begin{proposition}
    Under $H_0$, $p \sim U[0, 1]$.
    \label{propPValUnif}
\end{proposition}
\begin{proof}
    Let $F$ be the probability distribution function of $T$. Then:
    \begin{align*}
        \P_{H_0}(p < u) &= \P_{H_0}(1-F(T(\vec{X})) < u) \\
        &= \P_{H_0}(F(T(\vec{X})) > 1-u) \\
        &= \P_{H_0}(T(\vec{X}) > F^{-1}(1-u)) \\
        &= 1 - F(F^{-1}(1-u)) = u.
    \end{align*}
\end{proof}
\section{Tests and Confidence Intervals}
\begin{definition}{Acceptance region}
    The \underline{acceptance region} $A$ of a test is the complement of the critical region.
\end{definition}
\begin{theorem}
    Let $\vec{X}$ have pdf $f_{\vec{X}}(\vec{x}, \theta)$ for some $\theta \in \Theta$.
    \begin{enumerate}
        \item Suppose that for each $\theta_0 \in \Theta$, there exists a test of $H_0: \theta = \theta_0$ of size $\alpha$ with acceptance region $A(\theta_0)$. Then the set
            \begin{equation*}
                I(\vec{X}) = \subsetselect{\theta}{\vec{X} \in A(\theta)}
            \end{equation*}
            is a $100(1-\alpha)\%$ confidence set.
        \item Suppose that $I(\vec{X})$ is a $100(1-\alpha)$ confidence set for $\theta$. Then:
            \begin{equation*}
                A(\theta_0) = \subsetselect{\vec{x}}{\theta_0 \in I(\vec{x})}
            \end{equation*}
            is the acceptance region of a test of size $\alpha$ for $H_0 : \theta = \theta_0$.
    \end{enumerate}
    \label{thmTestsCIs}
\end{theorem}
\begin{proof}
    In both cases, $\theta_0 \in I(\vec{X}) \iff \vec{X} \in A(\theta_0)$.

    \begin{subproof}{$\P_{\theta_0}(\theta_0 \in I(\vec{X})) = 1-\alpha$}
        Using the equivalence, the LHS is:
        \begin{equation*}
            \P_{\theta_0}(\vec{X} \in A(\theta_0)) = \P_{\theta_0}(\text{do not reject $H_0$}) = 1-\alpha
        \end{equation*}
    \end{subproof}
    \begin{subproof}{$\P_{\theta_0}(\vec{X} \notin A(\theta_0)) = \alpha$}
        Using the equivalence again,
        \begin{equation*}
            \P_{\theta_0}(\vec{X} \notin A(\theta_0)) = \P_{\theta_0}(\theta_0 \notin I(\vec{X})) = \alpha
        \end{equation*}
    \end{subproof}
\end{proof}
\begin{example}
    Suppose $X_1, \cdots, X_n \sim N(\mu, \sigma_0^2)$ where $\sigma_0$ is known. We know a $100(1-\alpha)\%$ confidence interval for $\mu$:
    \begin{equation*}
        I(\vec{X}) = \overline{X} \pm \frac{\sigma_0^2}{\sqrt{n}} Z_{\frac{\alpha}{2}}
    \end{equation*}
    therefore we can use the second part of \thmref{thmTestsCIs} to construct a test of size $\alpha$ for $H_0: \mu = \mu_0$.
    \begin{equation*}
        A(\mu_0) = \subsetselect{\vec{x}}{\mu_0 \in I(\vec{x})}
    \end{equation*}
    and we can write instead that we reject $H_0$ when:
    \begin{equation*}
        \frac{\sqrt{n}|\mu_0-\overline{X}|}{\sigma_0} > z_{\alpha / 2}
    \end{equation*}
    Then this is a 2-sided LRT.
\end{example}
\section{Composite Hypotheses}
Previously we have considered $H_0$ and $H_1$ simple hypotheses, with error probabilities:
\begin{equation*}
    \alpha \in \P_{H_0}(\vec{X} \in C),\qquad \beta = \P_{H_1}(\vec{X} \notin C)
\end{equation*}
We will now consider $\vec{X}$ having probability density $f_{\vec{X}}(\vec{x}, \theta)$ and:
\begin{align*}
    H_0:& \theta \in \Theta_0 \subseteq \Theta \\
    H_1:& \theta \in \Theta_1 \subseteq \Theta \\
\end{align*}
\begin{definition}{Power function}
    The \underline{power function} is defined to be $W(\theta) = \P_{\theta}(\vec{X} \in C)$.
\end{definition}
\begin{definition}{Size (composite hypotheses)}
    The \underline{size} of a test with composite hypotheses as above is the worst-case type I error:
    \begin{equation*}
        \alpha = \sup_{\theta \in \Theta_0} W(\theta)
    \end{equation*}
\end{definition}
\subsection{Uniformly Most Powerful Tests}
\begin{definition}{Uniformly most powerful tests}
    A test of size $\alpha$ of $H_0$ against $H_1$ is \underline{uniformly most powerful} (UMP) if:
    \begin{enumerate}
        \item $\sup_{\theta \in \Theta_0} W(\theta) \leq \alpha$
        \item For any other test of size $\alpha$, with power function $W^*$, we have $W(\theta) \geq W^*(\theta)$ for all $\theta \in \Theta_1$
    \end{enumerate}
\end{definition}
\begin{remark}
    Such a test may not exist. A good candidate, however, is an LRT.
\end{remark}
\begin{example}[One-sided test for normal location]
    Suppose $X_1, \cdots, X_n \sum N(\mu, \sigma_0^2)$ with $\sigma_0^2$ known. Consider the hypotheses:
    \begin{align*}
        H_0:&\mu \leq \mu_0 \\
        H_1:&\mu > \mu_0
    \end{align*}
    Recall that for the simple hypotheses, $H_0' : \mu = \mu_0$ and $H_1' : \mu = \mu_1$, the LRT had critical region:
    \begin{equation*}
        C = \subsetselect{\vec{x}}{z = \frac{\sqrt{n}(\overline{X} - \mu_0)}{\sigma_0}}
    \end{equation*}
    We will show that the same test is UMP for our composite hypotheses. Consider the power function:
    \begin{align*}
        W(\mu) &= \P_{\mu}(\text{reject $H_0$}) \\
        &= \P_\mu\left(\frac{\sqrt{n}(\overline{X} - \mu_0)}{\sigma_0} > z_\alpha\right) \\
        &= \P_\mu\left(\frac{\sqrt{n}(\overline{X} - \mu)}{\sigma_0} > z_\alpha + \frac{\sqrt{n}(\mu_0 - \mu)}{\sigma_0}\right) \\
        &= 1 - \Phi\left(z_\alpha + \frac{\sqrt{n}(\mu_0 - \mu)}{\sigma_0}\right)
    \end{align*}
    Then this is monotonically increasing, and it is $\alpha$ at $\mu = \mu_0$. Therefore, $\sum_{\mu \in \Theta_0} W(\mu) = \alpha$ and the test has size $\alpha$.

    Next, consider any other test with size $\leq \alpha$ with power function $W^*$. This must have size $\leq \alpha$ for $H_0'$ against $H_1$. This is because:
    \begin{equation*}
        W^*(\mu_0) \leq \sup_{\mu \in \Theta} \leq \alpha.
    \end{equation*}
    Thus, by Neyman-Pearson, $W(\mu_1) \geq W^*(\mu_1)$. Then since this argument works for any $\mu_1 > \mu_0$, we find:
    \begin{equation*}
        W(\mu) \geq W^*(\mu) ~\forall \mu \in \Theta_1
    \end{equation*}
\end{example}
\subsection{Generalised Likelihood Ratio Tests}
The generalised likelihood ratio statistic is:
\begin{equation*}
    \Lambda_{\vec{X}}(H_0, H_1) = \frac{\sup_{\theta \in \Theta_1}f_{\vec{X}}(\vec{x}, \theta)}{\sup_{\theta \in \Theta_0}f_{\vec{X}}(\vec{x}, \theta)}
\end{equation*}
and the generalised likelihood ratio test (GLRT) rejects $H_0$ when $\Lambda_{\vec{X}}$ is large.
\begin{example}[Two-sided test for normal location]
    Consdier $X_1, \cdots, X_n \sim N(\mu, \sigma_0^2)$ where $\sigma_0^2$ is known. We would like to test:
    \begin{align*}
        H_0:&\mu = \mu_0 \\
        H_1:&\mu \neq \mu_0
    \end{align*}
    Then $\Theta_0 = \{\mu_0\}$ and $\Theta_1 = \R \backslash \{\mu_0\}$. Next, the generalised likelihood ratio statistic is:
    \begin{align*}
        \Lambda_{\vec{X}}(H_0, H_1) &= \frac{(2\pi \sigma_0^2)^{n / 2} \exp\left(\frac{1}{2\sigma_0^2} \sum_{i=1}^{n} (X_i - \overline{X})^2\right)}{(2\pi \sigma_0^2)^{n / 2} \exp\left(\frac{1}{2\sigma_0^2} \sum_{i=1}^{n} (X_i - \mu_0)^2\right)} \\
        2\log(\Lambda_{\vec{X}}(H_0, H_1)) &= \frac{1}{\sigma_0^2} \left(\sum_{i=1}^{n} (X_i - \mu_0)^2 - \sum_{i=1}^{n} (X_i - \overline{X})^2\right) \\
        &= \frac{n}{\sigma_0^2} (\overline(X) - \mu_0)^2
    \end{align*}
    Then because $\log$ is an increasing function, the GLRT rejects $H_0$ if $\frac{\sqrt{n}|\overline(X) - \mu_0|}{\sigma_0}$ is large. Under $H_0$, this has distribution $N(0, 1)$. Therefore the test that rejects $H_0$ if the given ratio is greater than $\Phi^{-1}(\alpha / 2)$ has size $\alpha$.

    Note that $2\log(\Lambda_{\vec{X}}(H_0, H_1)) \sim \chi_1^2$. Therefore, the critical region can also be written:
    \begin{equation*}
        \left\{x~\left|~\frac{n(\overline{X} - \mu_0^2)}{\sigma_0^2} > \chi_1^2(\alpha)\right.\right\}
    \end{equation*}

    In fact, a more general result says that $2\log(\Lambda_{\vec{X}}(H_0, H_1))$ is approximately a $\chi^2$ distribution for large $n$.
\end{example}

\end{document}