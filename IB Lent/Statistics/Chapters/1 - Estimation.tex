\documentclass[../Main.tex]{subfiles}

\begin{document}
\section{Probability Review}
Let $X : \Omega \mapsto \R$ be a random variable defined on the probability space $(\Omega, \F, \P)$. Here $\Omega$ is the sample space, $\F$ is the set of events and $\P$ is the probability measure, $\P : \F \mapsto [0, 1]$.
\subsection{Distribution Functions}
Let the random variable $X$ be as above defined.
\begin{definition}{Cumulative distribution function}
    The \underline{cumulative distribution function} (cdf) of $X$ is $F_X(x) = \P(X \leq x)$.
\end{definition}
\begin{definition}{Discrete random variable}
    A \underline{discrete random variable} takes values in a countable set $\samplespace$ and has \underline{probability mass function} (pmf) $p_X(x) = \P(X = x)$.
\end{definition}
\begin{definition}{Continuous random variable}
    A \underline{continuous random variable} has a \underline{probability density function} $f_X$ satisfying:
    \begin{equation*}
        \P(X \in A) = \int_A f_X(x) dx
    \end{equation*}
    for measurable sets $A$.
\end{definition}
\begin{definition}{Independence}
    A sequence of random variables $X_1, \cdots, X_n$ are \underline{independent} if:
    \begin{equation*}
        \P(X_1 \leq x_1, \cdots, X_n \leq x_n) = \P(X_1, \leq x_1) \cdot \cdots \cdot \P(X_n \leq x_n)
    \end{equation*}
    for all vectors $(x_1, \cdots, x_n)$.

    If each $X_i$ has pdf/pmf $F_{X_i}(x)$ then this is equivalent to:
    \begin{equation*}
        f_{\vec{X}}(\vec{x}) = \prod_{i=1}^{n} f_{X_i}(x_i)
    \end{equation*}
\end{definition}
\subsection{Moments}
\begin{definition}{Expectation}
    The \underline{expectation} of $X$ is:
    \begin{equation*}
        \E[X] =
        \begin{cases}
            \sum_{x \in \samplespace} x p_X(x) & \text{if $X$ is discrete} \\
            \int_{-\infty}^{\infty} xf_X(x) dx  & \text{if $X$ is continuous}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{definition}{Variance}
    The \underline{variance} of $X$ is $\Var(X) = \E\left[(X - E[X])^2\right]$
\end{definition}
\begin{definition}{Moment generating function}
    The \underline{moment generating function} (mgf) of $X$ is $M(t) = \E\left[e^{tX}\right]$, and satisfies:
    \begin{equation*}
        \E[X^n] = \left.\frac{d^{n}}{dx^{n}} M(t) \right|_{t = 0}
    \end{equation*}
    Further, under mild conditions, equality of mgf is sufficient for random variables to have the same distribution.
\end{definition}

Recall the following properties of expectation and variance:
\begin{gather*}
    \E\left[a_1 X_1 + \cdots + a_n X_n\right] = a_1 \E[X_1] + \cdots + a_n \E[X_n] \\
    \Var(a_1 X_1 + \cdots + a_n X_n) = \sum_{i, j = 1}^n a_i a_j \Cov(X_i X_j)
\end{gather*}
or, in vector notation:
\begin{gather*}
    \E\left[\vec{a}^T \vec{X}\right] = \vec{a}^T \E[\vec{X}] \\
    \Var(\vec{a}^T \vec{X}) = \vec{a}^T \Var(\vec{X}) \vec{a}
\end{gather*}
\subsection{Conditional Probability}
\begin{definition}{Joint pmf/pdf}
    The \underline{joint pmf} of two discrete random variables is the function $p_{X, Y}(x, y) = \P(X = x, Y = y)$.

    The \underline{joint pdf} of two discrete random variables is the function that satisfies:
    \begin{equation*}
        \P(X \leq x, Y \leq y) = \int_{-\infty}^{x} \int_{-\infty}^{y} f_{X, Y}(u, v) dv~du 
    \end{equation*}
\end{definition}
The \underline{marginal pdf} of $Y$ is:
\begin{equation*}
    f_Y(y) = \int_{-\infty}^{\infty} f_{X, Y}(x, y) dx 
\end{equation*}
\begin{definition}{Conditional probability mass function}
    If $X$ and $Y$ are discrete random variables with joint pmf $P_{X, Y}(x, y) = \P(X = x, Y = y)$, and marginal pmf $p_Y(y) = \P(Y = y) = \sum_{x \in \samplespace} p_{X, Y}(x, y)$, then the \underline{conditional pmf} is:
    \begin{equation*}
        P_{X | Y}(x, y) = \P(X = x | Y = y) = \frac{p_{X, Y}(x, y)}{p_Y(y)}
    \end{equation*}
\end{definition}
The conditional pdf looks similar (for the continuous case), replacing $p$ with $f$.
\begin{definition}{Conditional expectation}
    The \underline{conditional expectation} of $X$ given $Y$ is:
    \begin{equation*}
        E[X | Y] =
        \begin{cases}
            \sum_{x \in \samplespace} x p_{X | Y}(x | Y) & \text{discrete case} \\
            \int_{-\infty}^{\infty} x f(x | Y) dx & \text{continuous case}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{definition}{Law of Total Expectation}
    The \underline{Law of Total Expectation} is:
    \begin{equation}
        \E[X] = \E[\E[X | Y]]
        \label{eqnTotalExpectation}
    \end{equation}
\end{definition}
This is a consequence of the law of total probability.
\begin{proposition}[Law of Total Variance]
    For $X$ and $Y$ random variables,
    \begin{equation*}
        \Var(X) = E[\Var(X | Y)] + \Var(E[X | Y])
    \end{equation*}
    \label{propTotalVar}
\end{proposition}
\begin{proof}
    \begin{align*}
        \Var(X) &= E[X^2] - (E[X])^2 \\
        &= E[E[X^2 | Y]] - E[E[X | Y]] \\
        &= E[E[X^2|Y] - (E[X^2 | Y])^2] + E[(E[X | Y])^2] - (E[E[X | Y]])^2 \\
        &= \Var(X | Y) + \Var(E[X | Y])
    \end{align*}
\end{proof}
\begin{proposition}[Change of Variables Formula]
    Given a differentiable bijection:
    \begin{align*}
        \phi : \R^2 &\mapsto \R^2\\
        (x, y) &\mapsto (u, v)
    \end{align*}
    For random variables $X, Y$, the probability density function for $(U, V) = \phi(X, Y)$ is given by:
    \begin{equation*}
        f_{U, V}(u, v) = f_{X, Y}(x(u, v), y(u, v)) |\det(J)|
    \end{equation*}
    where $J$ is the Jacobian:
    \begin{equation*}
        J = 
        \begin{pmatrix}
            \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
            \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
        \end{pmatrix}
    \end{equation*}
    \label{propChangeVars}
\end{proposition}
\subsection{Limit Theorems}
\begin{theorem}[Law of Large Numbers]
    Suppose that $(X_i)_{i = 1}^n$ are independent and identically distributed random variables with expectation $\mu$ and variance $\sigma^2$.

    Define the sum and sample mean:
    \begin{align*}
        S_n &= \sum_{i=1}^{n} X_i & \overline{X_n} &= \frac{S_n}{n}
    \end{align*}
    Then we have the following:
    \begin{itemize}
        \item \textbf{Weak LLN:} $\overline{X_n} \xrightarrow{p} \mu$, meaning $\P(|\overline{X_n} - \mu| > \epsilon) \to 0$ as $n \to \infty$ for any $\epsilon > 0$.
        \item \textbf{Strong LLN:} $\overline{X_n} \xrightarrow{a.s.} \mu$, meaning $\P(\lim_{n \to \infty} \overline{X_n} = \mu) = 1$.
    \end{itemize}
    \label{thmLLN}
\end{theorem}
\begin{theorem}[Central Limit Theorem]
    Using the above definitions, define:
    \begin{equation*}
        Z_n = \frac{S_n - n \mu}{\sigma \sqrt{n}}.
    \end{equation*}
    Then this is approximately $N(0, 1)$ for large $n$. We write $S_n \approx N(n \mu, n \sigma^2)$.

    Precisely,
    \begin{equation*}
        \P(Z_n \leq z) \to \Phi(z)~\forall z\in\R
    \end{equation*}
    where $\Phi$ is the cdf of the standard normal $N(0, 1)$.
    \label{thmCentralLim}
\end{theorem}
\section{Parametric Estimation}
\subsection{Estimators and Bias}
Consider observing multiples samples $X_1, \cdots, X_n$, which are independent and identically distributed. Let these exist in the sample space $\samplespace$. Let $\vec{X}$ be the vector of the $X_i$.

\begin{definition}{Statistical model}
    A \underline{statistical model} is a pair $(\Omega, \mathcal{P})$ where $\Omega$ is the sample space and $\mathcal{P}$ is a set of possible probability distributions. These are parameterised by the vector $\vec{\theta}$, $\mathcal{P} = \subsetselect{f(x;\vec{\theta})}{\vec{\theta} \in \Theta}$.
\end{definition}

Assume that $X_1$ belongs to a \underline{statistical model} $\subsetselect{p(x;\theta)}{\theta \in \Theta}$.
\begin{examples}{}
    \item Suppose $X_1 \sim \Po(\lambda)$, then $\theta = \lambda \in \Theta = (0, \infty)$.
    \item Suppose $X_1 \sim N(\mu, \sigma^2)$, then $\vec{\theta} = (\mu, \sigma^2) \in \Theta = \R \times (0, \infty)$.
\end{examples}
We then want to be able to:
\begin{itemize}
    \item Given an estimate $\hat{\theta} : \samplespace^n \mapsto \Theta$ of the true value of $\theta$.
    \item Give an interval estimator $(\hat{\theta}_1(X), \hat{\theta}_2(X))$ of $\theta$.
    \item Test some hypothesis about $\theta$.
\end{itemize}
\begin{definition}{Statistic}
    A \underline{statistic} is a function $T : \samplespace \mapsto \R$. It is written $T(\vec{x})$ for the statistic applied to observed data, and written $T(\vec{X})$ when applied to a vector of random variables $\vec{X}$. $T(\vec{X})$ is a random variable and its distribution is the \underline{sampling distribution}.
\end{definition}
\begin{definition}{Estimator}
    An \underline{estimator} is a statistic where we care about how close it is to the real parameter $\theta$. It is usually a random variable, written $\hat{\theta}$. An observed value from $\hat{\theta}$ is the \underline{estimate} for $\theta$.
\end{definition}
\begin{example}
    Suppose $X_i$ are normally distributed, $N(\mu, 1)$. Let $\hat{\mu} = \frac1n \sum_{i=1}^n X_i$, simply the average of the output of the random variables. Then the sampling distribution of $\hat{\mu}$ is:
    \begin{equation*}
        \hat{\mu} \sim N\left(\mu, \frac1n\right)
    \end{equation*}
    \label{expNormalEstimator}
\end{example}
\begin{definition}{Estimator bias}
    The \underline{bias} of an estimator $\hat{\theta}$ is:
    \begin{equation*}
        \bias(\hat{\theta}) = \E_\theta[\hat{\theta}] -\theta
    \end{equation*}
    where the expectation is taken over the $X_i$.
    
    We say that $\thhat$ is \underline{unbiased} if $\bias(\hat{\theta}) = 0$.
\end{definition}
\begin{warning}
    In general, $\bias(\hat{\theta})$ can be a function of $\theta$, which is not explicit in notation.
\end{warning}
\begin{example}[Continuation of example~\ref{expNormalEstimator}]
    We note that the estimator $\hat{\mu}$ is unbiased for $\mu$:
    \begin{equation*}
        E_\mu[\hat{\mu}] = \frac1n \sum_{i=1}^{n} \E_\mu[X_i] = \mu
    \end{equation*}
\end{example}
\subsection{Bias-Variance Decomposition}
\begin{definition}{Mean-squared error}
    The \underline{mean-squared error} (MSE) of an estimator $\hat{\theta}$ is:
    \begin{equation*}
        \mse{\hat{\theta}} = \E_\theta[(\hat{\theta} - \theta)^2]
    \end{equation*}
\end{definition}
\begin{proposition}[Bias-Variance Decomposition]
    For an estimator $\thhat$ like above,
    \begin{equation*}
        \mse{\thhat} = (\bias(\thhat))^2 + \Var_\theta (\thhat)
    \end{equation*}
    \label{propBiasVarDecomp}
\end{proposition}
\begin{proof}
    \begin{align*}
        \mse(\thhat) &= \E_\theta[(\thhat - \theta)^2] \\
        &= \E_\theta[(\thhat - E_\theta[\thhat] + E_\theta[\thhat] - \theta)^2] \\
        &= \E_\theta[(\thhat - E_\theta[\thhat])^2 + 2(\thhat - E_\theta[\thhat])(E_\theta[\thhat] - \theta) + (E_\theta[\thhat] - \theta)^2] \\
        &= \E_\theta[(\thhat - \E_\theta(\thhat))^2] + (E_\theta[\thhat] - \theta)^2 + 2(\E_{\theta}[\thhat] - \theta) \E_\theta[\thhat - \E_\theta[\thhat]] \\
        &= \Var_\theta(\thhat) + (\bias(\thhat))^2 + 2(E_\theta[\thhat] - \theta) (\E_\theta[\thhat] - \E_\theta[\thhat]) \\
        &= \Var_\theta(\thhat) + (\bias(\thhat))^2 
    \end{align*}
\end{proof}
\begin{example}
    Suppose $X \sim \text{Bin}(n, \theta)$ where $n$ is known, $\theta \in [0, 1]$.

    We consider an estimator $T_u = \frac{X}{n}$. The mean of this estimator is:
    \begin{equation*}
        \E_\theta[T_u] = \frac{E[X]}{n} = \theta
    \end{equation*}
    Therefore, $T_u$ is unbiased. Therefore, the mean-squared error is:
    \begin{align*}
        \mse(T_u) &= \Var(T_u) + \left(\bias{T_u}\right)^2 \\
        &= \frac{1}{n^2} \Var_\theta(X) \\
        &= \frac{\theta(1-\theta)}{n}
    \end{align*}
    
    Now consider a biased estimator $T_b = \frac{X + 1}{n + 2} = \omega \frac{X}{n} + (1 - \omega) \frac12$ where $\omega = \frac{n}{n+2}$.

    \begin{align*}
        \bias(T_b) &= E_\theta[T_b] - \theta \\
        &= \frac{E[X] + 1}{n + 2} - \theta \\
        &= (1 - \omega)\left(\frac12 - \theta\right) \\
        \Var(T_b) &= \frac{\Var_\theta(X)}{(n + 2)^2} \\
        &= \frac{\omega^2 \theta(1-\theta)}{n}
    \end{align*}
    Then the mean-squared error is:
    \begin{equation*}
        \mse(T_b) = \omega^2\underbrace{\frac{\theta(1-\theta)}{n}}_{\mse(T_u)} + (1 - \omega)^2 \left(\frac12 - \theta\right)^2 \\
    \end{equation*}
    These errors are plotted in figure~\ref{figEstimatorErrorPlot}. We see that, further from $\theta = \frac12$, the biased estimator has the greater error. However, since we have biased it towards $\frac12$, it is more accurate than the unbiased estimator closer to $\theta = \frac12$.
\end{example}
\begin{figure}
    \centering
    \begin{tikzpicture}[mark size=0, domain=0:1]
        \begin{axis}[
                axis lines=middle,
                xmin=-0.2,xmax=1.2,
                xlabel={$x$},
                ylabel={$\mse$},
                legend entries={$T_u$, $T_b$},
                legend pos=outer north east
            ]
            \pgfmathsetmacro{\n}{10}
            \pgfmathsetmacro{\w}{\n / (\n + 2)}
            \addplot {x * (1-x) / \n};
            \addplot {\w * \w * x * (1-x) / \n + (1 - \w) * (1 - \w) * (0.5 - x) * (0.5-x)};
        \end{axis}
    \end{tikzpicture}
    \caption{Comparison of mse with biased and unbiased estimators}
    \label{figEstimatorErrorPlot}
\end{figure}
\section{Sufficient Statistics}
Suppose $X_1, \cdots, X_n$ are independent, identically distributed random variables taking values in $\samplespace$, with pdf (or pmf) $f_X(\cdot, \theta)$. Consider $\theta$ fixed. Let $\vec{X}$ be the vector of the $X_i$.
\begin{definition}{Sufficient statistic}
    A statistic $T$ is \underline{sufficient} for $\theta$ if the conditional distribution $\vec{X} | T(\vec{X})$ does not depend on $\theta$.
\end{definition}
\begin{remark}
    The parameter $\theta$ and the sufficient statistic may be vectors, $\vec{\theta}$, and $\vec{T}(\vec{X})$
\end{remark}
\begin{example}
    Suppose $X_i \sim \text{Ber}(\theta)$ for $\theta \in [0, 1]$. Then:
    \begin{align*}
        f_{\vec{X}}(\vec{x}, \theta) &= \prod_{i=1}^{n} \theta^{x_i}(1 - x_i) \\
        &= \theta^{\sum_{i=1}^n X_i} (1 - \theta)^{n - \sum_{i=1}^n X_i}
    \end{align*}
    Then we see that only the sum of the $X_i$ is needed. Therefore, consider a statistic $T(\vec{X}) = \sum_{i=1}^n X_i$.
    \begin{align*}
        f_{\vec{X} | T = t}(\vec{x} | T = t) &= \frac{\P_\theta(\vec{X} = \vec{x}, T(\vec{X}) = t)}{\P_\theta(T(\vec{X}) = t)} \\
        &= \frac{\P_\theta(\vec{X} = \vec{x})}{\P_\theta(T(\vec{X}) = t)} \\
        &= \frac{\theta^{\sum_{i=1}^n X_i} (1-\theta)^{n-\sum_{i=1}^n X_i}}{\binom{n}{t} \theta^t (1 - \theta)^t} \\
        &= \left[\binom{n}{t}\right]^{-1}
    \end{align*}
    \label{expBerSuffStat}
\end{example}
\subsection{Characterising Sufficiency}
\begin{theorem}[Factorisation Criterion]
    A statistic $T$ is sufficient for $\theta$ if and only if:
    \begin{equation*}
        f_{\vec{X}}(\vec{x}, \theta) = g(T(\vec{x}), \theta)h(\vec{x})
    \end{equation*}
    for suitable functions $g$ and $h$.
    \label{thmFactorisationCriterion}
\end{theorem}
\begin{proof}
    \begin{proofdirection}{$\Rightarrow$}{Let the pmf factorise}
        Consider first the case that $X_i$ are all discrete.

        Compute the function $f_{\vec{X} | T = t}$:
        \begin{align*}
            f_{\vec{X}|T = t}(\vec{x} | T = t) &= \frac{\P_\theta(\vec{X} = \vec{x}, T(\vec{x}) = t)}{\P_\theta(T(\vec{x}) = t)} \\
            &= \frac{g(T(\vec{x}), \theta) h(\vec{x})}{\sum_{\vec{x}' \in \samplespace} g(t, \theta)h(x')} \\
            &= \frac{h(\vec{x})}{\sum_{\vec{x}' \in \samplespace} h(\vec{x}')} \\
        \end{align*}
        and this does not depend on $\theta$, so $T(\vec{X})$ is sufficient.

        In the continuous case, we would have an integral instead of the sum in the denominator.
    \end{proofdirection}
    \begin{proofdirection}{$\Leftarrow$}{Suppose that $T(\vec{X})$ is sufficient.}
        We can write:
        \begin{align*}
            \P_\theta(\vec{X} = \vec{x}) &= \P_\theta(\vec{X} = \vec{x}, T(\vec{X}) = T(\vec{x})) \\
            &= \underbrace{\P_\theta(\vec{X} = \vec{x} | T(\vec{X}) = t(\vec{x}))}_{h(\vec{x})} \underbrace{\P_\theta(T(\vec{X}) = t(\vec{x}))}_{g(T(\vec{x}, \theta))}
        \end{align*}
        And by assumption the term identified with $h(\vec{x})$ is independent of $\theta$.
    \end{proofdirection}
\end{proof}
\begin{example}[Example~\ref{expBerSuffStat} continued]
    Then $T(x) = \sum_{i=1}^n X_i$, and we find that $f_{\vec{X}}$ is only a function of $t$ and $\theta$, so we write $g = f_{\vec{X}}$ and $h = 1$.
\end{example}
\begin{example}
    Let $X_i \sim \text{Unif}[0, \theta]$ for $i = 1$ to $n$. $\theta \in (0, \infty)$. The density function is:
    \begin{align*}
        f_{\vec{X}}(\vec{x}, \theta) &= \prod_{i=1}^{n} \frac1\theta \ind{X_i \in [0, \theta]} \\
        &= \underbrace{\frac{1}{\theta^n} \ind{\max_i X_i \leq \theta}}_{g(T(\vec{x}), \theta)} \underbrace{\ind{\min_i X_i \geq 0}}_{h(\vec{x})}
    \end{align*}
    so by \thmref{thmFactorisationCriterion}, $T = \max_i X_i$ is sufficient.
    \label{expUnifSuffStat}
\end{example}
\subsection{Minimal Sufficiency}
\begin{definition}{Minimal sufficient statistic}
    A sufficient statistic $T(\vec{X})$ is \underline{minimal sufficient} if it is a function of every other sufficient statistic. That is, if $R(\vec{X})$ is also sufficient,
    \begin{equation*}
        R(\vec{x}) = R(\vec{y}) \implies T(\vec{x}) = T(\vec{y})~\forall \vec{x}, \vec{y} \in \samplespace.
    \end{equation*}
\end{definition}
\begin{remark}
    Minimal sufficient statistics are unique up to bijection.
\end{remark}
\begin{theorem}[Minimality Criterion]
    Suppose $T(\vec{X})$ is a statistic such that:
    \begin{equation*}
        \frac{f_{\vec{X}}(\vec{x}, \theta)}{f_{\vec{X}}(\vec{y}, \theta)} \text{ is constant (in $\theta$)} \iff T(\vec{x}) = T(\vec{y})
    \end{equation*}
    Then $T$ is a minimal sufficient statistic.
    \label{thmMinCriterion}
\end{theorem}
\begin{proof}
    We need to show that such a statistic is sufficient and minimal.
    \begin{subproof}{$T$ is a sufficient statistic}
        Let $\vec{y}$ be an element of $\samplespace$ with $T(\vec{x}) = T(\vec{y})$. Choose $\vec{y}$ independently of $\vec{x}$ in the set $\subsetselect{\vec{z}}{T(\vec{z}) = T(\vec{x})}$.
        \begin{equation*}
            f_{\vec{X}}(\vec{x}, \theta) = \underbrace{f_{\vec{X}}(\vec{y}, \theta)}_{g(T(\vec{x}), \theta)} \underbrace{\frac{f_{\vec{X}}(\vec{x}, \theta)}{f_{\vec{X}}(\vec{y}, \theta)}}_{h(\vec{x})}
        \end{equation*}
        Then this meets the criterion in \thmref{thmFactorisationCriterion} because $\vec{y}$ was chosen from $\subsetselect{\vec{z}}{T(\vec{z}) = T(\vec{x})}$ independently of $\vec{x}$ (so $g$ depends only on $T(\vec{x})$), and $h$ is independent of $\theta$ by hypothesis.
    \end{subproof}
    \begin{subproof}{$T$ is minimal}
        Suppose that $S$ is another sufficient statistic. Then by \thmref{thmFactorisationCriterion}, there exist $g_S$ and $h_S$ such that:
        \begin{equation*}
            f_{\vec{X}}(\vec{x}, \theta) = g_S(T(\vec{x}), \theta) h_S(\vec{x})
        \end{equation*}
        Then suppose $S(\vec{x}) = S(\vec{y})$. Then:
        \begin{align*}
            \frac{f_{\vec{X}}(\vec{x}, \theta)}{f_{\vec{X}}(\vec{y}, \theta)}&= \frac{g_S(S(\vec{x}), \theta) h_S(\vec{x})}{g_S(S(\vec{y}), \theta) h_S(\vec{y})} \\
            &= \frac{h_S(\vec{x})}{h_S(\vec{y})}
        \end{align*}
        Then this is independent of $\theta$, and so by hypothesis, this means $T(\vec{x}) = T(\vec{y})$.

        Since $S$ was arbitrary, any sufficient statistic $S$ gives:
        \begin{equation*}
            S(\vec{x}) = S(\vec{y}) \implies T(\vec{x}) = T(\vec{y})
        \end{equation*}
        And so $T$ is minimal.
    \end{subproof}
\end{proof}
\begin{example}
    Suppose $X_i \sim N(\mu, \sigma^2)$ for $i = 1$ to $n$. Now our parameter is a vector, $\vec{\theta} = (\mu, \sigma^2)^T$.

    \begin{align*}
        \frac{f_{\vec{X}}(\vec{x}, \mu, \sigma^2)}{f_{\vec{X}}(\vec{y}, \mu, \sigma^2)}&= \frac{(2\pi\sigma^2)^{-n / 2} \exp\left[\frac{-1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2\right]}{(2\pi\sigma^2)^{-n / 2} \exp\left[\frac{-1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2\right]} \\
        &= \exp\left[-\frac{1}{2\sigma^2} \left(\sum_{i=1}^n X_i^2 - \sum_{i=1}^n Y_i^2\right) + \frac{\mu}{\sigma^2} \left(\sum_{i=1}^n X_i - \sum_{i=1}^{n} Y_i\right)\right]
    \end{align*}
    Then this is constant in $(\mu, \sigma^2)$ if and only if:
    \begin{equation*}
        \sum_{i=1}^n X_i = \sum_{i=1}^n Y_i \text{ and } \sum_{i=1}^n X_i^2 = \sum_{i=1}^n Y_i^2
    \end{equation*}
    So a minimal sufficient statistic is:
    \begin{equation*}
        T(X) = \left(\sum_{i=1}^n X_i, \sum_{i=1}^n X_i^2\right)
    \end{equation*}
    Because we can perform a bijection without losing minimal sufficiency, consider instead the statistic:
    \begin{align*}
        S(\vec{X}) &= (\overline{\vec{X}}, S_{XX}),\\
        \text{Where }&\overline{\vec{X}} = \frac1n \sum_{i=1}^{n} X_i,\quad S_{XX} = \sum_{i=1}^{n} (X_i - \overline{X})^2
    \end{align*}
\end{example}
\begin{theorem}[Rao-Blackwell Theorem]
    Let $T$ be a sufficient statistic for $\theta$, and let $\tilde{\theta}$ be an estimator for $\theta$ with $\E_\theta[(\tilde{\theta})^2]$ finite for all $\theta$.

    Define a new estimator $\hat{\theta} = \E_\theta[\tilde{\theta} | T(\vec{X})]$. Then for all $\theta$,
    \begin{equation}
        \mse(\hat{\theta}) \leq \mse(\tilde{\theta})
        \label{eqnRaoBlackwellIneq}
    \end{equation}
    and the inequality is strict unless $\tilde{\theta}$ is a function of $T(\vec{X})$.
    \label{thmRaoBlackwell}
\end{theorem}
\begin{remark}
    We have a problem if in fact any of the estimators depend on $\theta$, since then we cannot use our estimator. We can resolve this, but in the notation above this is not clear.

    In the continuous case,
    \begin{equation*}
        \hat{\theta} = \int \tilde{\theta}(\vec{x}) f_{\vec{X} | T(\vec{x})}(\vec{x}, T) d^n \vec{x}
    \end{equation*}
    Then by sufficiency of $T$, the conditional pmf does not depend on $\theta$, and so $\hat{\theta}$ does not depend on $\theta$.
\end{remark}
\begin{proof}
    In this proof we will drop the subscript $\theta$ for expectations.
    \begin{align*}
        \E[\hat{\theta}] &= \E[\E[\hat{\theta} | T]] = \E[\tilde{\theta}] \\
        \therefore \bias(\hat{\theta}) &= \bias(\tilde{\theta})
    \end{align*}
    Then by the conditional variance formula,
    \begin{align*}
        \Var(\hat{\theta}) &= \E[\Var(\tilde{\theta} | T)] + \Var(\E[\tilde{\theta} | T]) \\
        &= \E[\Var(\tilde{\theta} | T)] + \Var(\hat{\theta}) \\
        &\geq \Var(\hat{\theta})
    \end{align*}
    Therefore, by \thmref{propBiasVarDecomp}, 
    \begin{equation}
        \mse(\tilde{\theta}) \geq \mse(\hat{\theta})
        \tag{\ref{eqnRaoBlackwellIneq}}
    \end{equation}

    Note that the above inequality is strict unless $\E[\Var(\tilde{\theta} | T)] = 0$, which is exactly when the distribution $\tilde{\theta} | T$ is constant, so exactly when $\tilde{\theta}$ is a function of $T$.
\end{proof}
\begin{example}
    Suppose $X_i$ are independent and identically distributed Poisson random variables (with parameter $\lambda$). Suppose we have $\theta = \P(X_i = 0) = e^{-\lambda}$.
    \begin{align*}
        f_{\vec{X}} &= \frac{e^{-n \lambda} \lambda^{\sum_{i=1}^n X_i}}{\prod_{i=1}^{n} (X_i)!} \\
        &= \frac{\theta^n (-\log(\theta))^{\sum_{i=1}^n X_i}}{\prod_{i=1}^{n} (X_i)!}
    \end{align*}
    Then we see, by factorising, that $T = \sum_{i=1}^{n} X_i$ is a sufficient statistic. Note that by the property of a Poisson distribution, $T \sim \operatorname{Po}(n\lambda)$. In order to use \thmref{thmRaoBlackwell}, start with a simple estimator $\tilde{\theta} = \ind{X_1 = 0}$.

    \begin{align*}
        \hat{\theta} &= \E[\tilde{\theta} | T = t] \\
        &= \P(X_1 = 0 | \sum_{i=1}^{n} X_i = t) \\
        &= \frac{\P\left(X_1 = 0, \sum_{i=2}^{n} X_i = t\right)}{\P\left(\sum_{i=1}^{n} X_i = t\right)} \\
        &= \frac{\P\left(X_1 = 0\right)\P\left(\sum_{i=2}^{n} X_i = t\right)}{\P\left(\sum_{i=1}^{n} X_i = t\right)} \\
        &= \frac{e^{-\lambda} e^{-(n-1)\lambda} \frac{((n-1) \lambda)^t}{t!}}{e^{-n\lambda} \frac{(n\lambda)^t}{t!}} \\
        &= \left(\frac{n-1}{n}\right)^t
    \end{align*}
    And therefore $\hat{\theta} = \left(1 - \frac1n\right)^{\sum_{i=1}^n X_i}$.
\end{example}
\begin{example}
    Let $X_i$ be independent and identically distributed uniform random variables over $[0, \theta]$. Suppose we want to estimate $\theta$. In example~\ref{expUnifSuffStat}, we saw $T = \max_i X_i$ is sufficient for $\theta$. Let $\tilde{\theta} = 2X_1$, an unbiased estimator that only takes into account the first value.
    \begin{align*}
        \hat{\theta} &= \E[\tilde{\theta} | T] \\
        &= 2E[X_1 | \max_i X_i = t] \\
        &= 2E[X_1 | \max_i X_i = t, X_1 = \max_i X_i] \\
        &\qquad \times \P(X_1 = \max_i X_i | \max_i X_i = t) \\
        &\quad + 2E[X_1 | \max_i X_i = t, X_1 \neq \max_i X_i] \\
        &\qquad \times \P(X_1 \neq \max_i X_i | \max_i X_i = t) \\
        &= 2t \times \frac1n + 2\E[X_1 | X_1 < t, \max_{i>1} = t] \left(\frac{n-1}{n}\right) \\
        &= 2t \times \frac1n + 2\left(\frac{t}{2}\right) \left(\frac{n-1}{n}\right) \\
        &= \frac{n+1}{n} t
    \end{align*}
    Therefore we have a new estimator $\hat{\theta} = \frac{n+1}{n} \max_i X_i$.
\end{example}
\subsection{Maximum Likelihood Estimators}
Let $\vec{X} = (X_1, \cdots, X_n)^T$ have joint pdf (or pmf) $f_{\vec{X}} (\vec{x}, \theta)$.
\begin{definition}{Likelihood}
    The \underline{likelihood} of $\theta$ is the function:
    \begin{equation*}
        L : \theta \mapsto f_{\vec{X}} (\vec{x}, \theta)
    \end{equation*}
\end{definition}
\begin{definition}{Maximum likelihood estimator}
    The \underline{maximum likelihood estimator} is the value of $\theta$ maximising $L$.
\end{definition}
For $n$ independent and identically distributed random variables, the likelihood is:
\begin{equation*}
    L(\theta) = \prod_{i=1}^{n} f_{X}(x_i, \theta)
\end{equation*}
Where $f_X$ is the distribution of all the $X_i$.
\begin{definition}{Log likelihood}
    The \underline{log likelihood} is the function:
    \begin{equation*}
        l(\theta) = \log(L(\theta)).
    \end{equation*}
\end{definition}
The log likelihood is often easier to use, because the product that we would normally have when dealing with independent and identically distributed random variables is now a sum.
\begin{example}
    Let $X_i \sim \text{Ber}(p)$ for $i = 1$ to $n$.
    \begin{align*}
        l(p) &= \left(\sum_{i=1}^{n} X_i\right) \log(p) + \left(n - \sum_{i=1}^{n} X_i\right) \log(1-p) \\
        \frac{dl}{dp} &= \frac{\sum_{i=1}^{n} X_i}{p} - \frac{n - \sum_{i=1}^{n} X_i}{1-p} 
    \end{align*}
    Then requiring $\frac{dl}{dp} = 0$ gives:
    \begin{equation*}
        p = \frac{\sum_{i=1}^{n} X_i}{n}
    \end{equation*}
    and this is the MLE. We note that it has expectation $p$, so it is unbiased.
\end{example}
\begin{example}
    Now consider $n$ independent and identically distributed normal random variables.
    \begin{align*}
        l(\mu, \sigma^2) &= -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} \left(X_i - \mu\right)^2 \\
        \frac{\partial l}{\partial \mu} &= -\frac{1}{\sigma^2} \left(X_i - \mu\right) \\
        \frac{\partial l}{\partial \sigma^2} &= -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^{n} (X_i - \mu)^2
    \end{align*}
    Then setting the derivatives to zero:
    \begin{align*}
        \mu &= \frac{1}{n} \sum_{i=1}^{n} X_i \\
        \sigma^2 &= \frac1n \sum_{i=1}^{n} (X_i - \mu)^2
    \end{align*}
    Then the MLE $\hat{\mu}$ is unbiased, but $\hat{\sigma^2}$ is biased. We see this has the $\chi^2$ distribution, $\hat{\sigma^2} = \frac{\sigma^2}{n} \chi_{n-1}^2$. This has expectation:
    \begin{equation*}
        \E[\hat{\sigma^2}] = \frac{n-1}{n} \sigma^2
    \end{equation*}
    Then this is \underline{biased}, but \underline{asymptotically unbiased}.
\end{example}
\begin{example}
    Now suppose $X_i \sim U[0, \theta]$.
    \begin{equation*}
        l(\theta) = \frac{1}{\theta^n} \ind{\max_i X_i \leq \theta}
    \end{equation*}
    Then the MLE is $\hat{\theta} = \max_i X_i$.

    Using \ref{TODO}, %Find the ref for uniform Rao-Blackwell application
    we know that $\E\left[\frac{n+1}{n} \max_i X_i\right]$ is unbiased. Therefore,
    \begin{equation*}
        \E[\hat{\theta}] = \frac{n+1}{n} \theta
    \end{equation*}
    Then again this is unbiased but asymptotically unbiased.
\end{example}
\begin{propositions}[MLE Properties]{
        \label{propsMLEProperties}
    }
    \item If $T$ is a sufficient statistic, the MLE is a function of $T$ and we can factorise as follows:
        \begin{equation*}
            L(\theta) = g(T(\vec{x}), \theta) h(\vec{x})
        \end{equation*}
    \item If $\phi = h(\theta)$, for a bijection $h$, the MLE of $\phi$ is $\hat{\phi} = h(\hat{\theta})$.
    \item Suppose that $\sqrt{n} \left(\hat{\theta} - \theta\right)$ is approximately normal for large $n$, with mean $0$. Then the covariance matrix is the ``smallest attainable'' (for more detail, see II Principles of Statistics).
\end{propositions}
\subsection{Stein's Paradox}
Consider $n$ independent normal random variables, $X_i \sim N(\mu_i, 1)$. We want to estimate the vector $\vec{\mu}$.
\begin{example}
    Suppose:
    \begin{align*}
        \mu_1 &= \text{mean IQ of a Cambridge student}\\
        \mu_2 &= \text{mean diameter of craters on the moon}\\
        \mu_3 &= \text{Mean weight of New Zealand sheep}
    \end{align*}
    Our intuition tells us that the best estimator should be simply the observations: $\hat{\vec{\mu}} = X$. However, there exist unintuitive estimators with smaller mean-squared error.
    
    Consider the \underline{James-Stein estimator}:
    \begin{equation*}
        \hat{\mu}_{JS} = \left(1 - \frac{n-2}{\lVert X \rVert}\right) X.
    \end{equation*}
    Then this has lower MSE than our intuitive estimator! We can see this by considering a simplified estimator $\hat{\vec{\mu}}_{\lambda}$. Then we find the MSE:
    \begin{equation}
        \mse(\hat{\vec{\mu}}_{\lambda}) = \left(\bias(\hat{\vec{\mu}}_{\lambda})\right)^2 + \Var(\hat{\vec{\mu}}_{\lambda}) = (\lambda - 1)^2 + \lVert \mu \rVert_2^2 + \lambda^2 n
    \end{equation}
    As a function of $\lambda$, we see that for larger $n$ it is advantageous to choose smaller $\lambda$ to trade off bias and variance. The optimal $\lambda$ depends on $\lVert \mu \rVert^2$, but since we do not know this we use $\lVert X \rVert$ instead.
\end{example}
This is Stein's Paradox. In order to estimate $\mu_i$, the best estimator will make use of all the $X_i$, however this goes against our intuition.
\section{Confidence Intervals}
\begin{definition}{Confidence interval}
    A $(1 - \gamma)$ \underline{confidence interval} for a parameter $\theta$ is a random interval $(A(\vec{X}), B(\vec{X}))$ such that:
    \begin{equation*}
        \P(A(\vec{X}) \leq \theta \leq B(\vec{X})) = \gamma
    \end{equation*}
    for some $\gamma \in (0, 1)$ and all values of the true parameter $\theta$.
\end{definition}
\begin{warning}
    The following is an incorrect interpretation: having observed $\vec{X} = \vec{x}$, there is a $1 - \gamma$ probability that $\theta$ is in $(A(\vec{x}), B(\vec{x}))$.
\end{warning}
\begin{example}
    Consider $n$ independent random variables with $X_i = N(\theta, 1)$.
    We want to find a $95\%$ confidence interval for $\theta$. Consider:
    \begin{equation*}
        \overline{X} = \frac1n \sum_{i=1}^{n} X_i \sim N\left(\theta, \frac1n\right).
    \end{equation*}
    If we define $Z = \sqrt{n} \left(\overline{X} - \theta\right)$, then $Z \sim N(0, 1)$.

    Let $z_1, z_2$ be numbers such that $\Phi(z_2) - \Phi(z_1) = 0.95$ (where $\Phi$ is the cumulative distribution function for the standard normal).
    \begin{align*}
        0.95 &= \P(z_1 \leq \sqrt{n} (\overline{X} - \theta) \leq z_2) \\
        &= \P\left(\overline{X} - \frac{z_2}{\sqrt{n}} \leq \theta \leq \overline{X} - \frac{z_1}{\sqrt{n}}\right)
    \end{align*}
    And so our $95\%$ confidence interval is:
    \begin{equation*}
        \left(\overline{X} - \frac{z_2}{\sqrt{n}}, \overline{X} - \frac{z_1}{\sqrt{n}}\right)
    \end{equation*}
    And we can take $z_2 \approx 1.96$, $z_1 \approx -1.96$, so then our confidence interval is:
    \begin{equation*}
        \left(\overline{X} - \frac{1.96}{\sqrt{n}}, \overline{X} + \frac{1.96}{\sqrt{n}}\right)
    \end{equation*}
\end{example}
From this example, we have a process for finding a confidence interval:
\begin{enumerate}
    \item Find a quantity $R(\vec{X}, \theta)$ such that the $\P_\theta$ distribution of $R$ does not depend on $\theta$. This is called a pivot.
    \item Write down the statement:
        \begin{equation*}
            \P(c_1 \leq R(\vec{X}, \theta) \leq c_2) = \gamma
        \end{equation*}
        where $c_1, c_2$ are quantities of the distribution.
    \item Rearrange the above to get the form required for a confidence interval.
\end{enumerate}
\begin{remark}
    When $\theta$ is a vector, we consider confidence sets rather than confidence intervals.
\end{remark}
\end{document}